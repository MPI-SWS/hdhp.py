1.02e+03 pattern=  9 task=  0 (u=300)  {'docs': u'Corrected versions of the numerically invariant expressions for the affineand Euclidean signature of a planar curve proposed by E.Calabi et. al arepresented. The new formulas are valid for fine but otherwise arbitrarypartitions of the curve. We also give numerically invariant expressions for thefour differential invariants parametrizing the three dimensional version of theEuclidean signature curve, namely the curvature, the torsion and theirderivatives with respect to arc length.'}
1.17e+03 pattern=  4 task=  0 (u=508)  {'docs': u'We use circle-packing methods to generate quadrilateral meshes for polygonaldomains, with guaranteed bounds both on the quality and the number of elements.We show that these methods can generate meshes of several types: (1) theelements form the cells of a Voronoi diagram, (2) all elements have twoopposite right angles, (3) all elements are kites, or (4) all angles are atmost 120 degrees. In each case the total number of elements is O(n), where n isthe number of input vertices.'}
1.19e+03 pattern= 17 task=  1 (u=508)  {'docs': u'Here we present the results of the NSF-funded Workshop on ComputationalTopology, which met on June 11 and 12 in Miami Beach, Florida. This reportidentifies important problems involving both computation and topology.'}
1.3e+03 pattern= 26 task=  2 (u=508)  {'docs': u'The regression depth of a hyperplane with respect to a set of n points in R^dis the minimum number of points the hyperplane must pass through in a rotationto vertical. We generalize hyperplane regression depth to k-flats for any kbetween 0 and d-1. The k=0 case gives the classical notion of center points. Weprove that for any k and d, deep k-flats exist, that is, for any set of npoints there always exists a k-flat with depth at least a constant fraction ofn. As a consequence, we derive a linear-time (1+epsilon)-approximationalgorithm for the deepest flat.'}
  845 pattern=  5 task=  0 (u=509)  {'docs': u'We show that any polyhedron forming a topological ball with an even number ofquadrilateral sides can be partitioned into O(n) topological cubes, meetingface to face. The result generalizes to non-simply-connected polyhedrasatisfying an additional bipartiteness condition. The same techniques can alsobe used to reduce the geometric version of the hexahedral mesh generationproblem to a finite case analysis amenable to machine solution.'}
1.12e+03 pattern=  4 task=  1 (u=509)  {'docs': u'We introduce a class of "inverse parametric optimization" problems, in whichone is given both a parametric optimization problem and a desired optimalsolution; the task is to determine parameter values that lead to the givensolution. We describe algorithms for solving such problems for minimum spanningtrees, shortest paths, and other "optimal subgraph" problems, and discussapplications in multicast routing, vehicle path planning, resource allocation,and board game programming.'}
1.14e+03 pattern= 16 task=  2 (u=509)  {'docs': u'We describe simple linear time algorithms for coloring the squares ofbalanced and unbalanced quadtrees so that no two adjacent squares are given thesame color. If squares sharing sides are defined as adjacent, we color balancedquadtrees with three colors, and unbalanced quadtrees with four colors; theseresults are both tight, as some quadtrees require this many colors. If squaressharing corners are defined as adjacent, we color balanced or unbalancedquadtrees with six colors; for some quadtrees, at least five colors arerequired.'}
1.14e+03 pattern= 19 task=  3 (u=509)  {'docs': u'A fractal construction shows that, for any beta>0, the beta-skeleton of apoint set can have arbitrarily large dilation. In particular this applies tothe Gabriel graph.'}
1.25e+03 pattern=  5 task=  4 (u=509)  {'docs': u'We solve the subgraph isomorphism problem in planar graphs in linear time,for any pattern of constant size. Our results are based on a technique ofpartitioning the planar graph into pieces of small tree-width, and applyingdynamic programming within each piece. The same methods can be used to solveother planar graph problems including connectivity, diameter, girth, inducedsubgraph isomorphism, and shortest paths.'}
  843 pattern=  4 task=  0 (u=511)  {'docs': u'We study the problem of moving a vertex in an unstructured mesh oftriangular, quadrilateral, or tetrahedral elements to optimize the shapes ofadjacent elements. We show that many such problems can be solved in linear timeusing generalized linear programming. We also give efficient algorithms forsome mesh smoothing problems that do not fit into the generalized linearprogramming paradigm.'}
  984 pattern=  8 task=  0 (u=1382)  {'docs': u'A mean field feedback artificial neural network algorithm is developed andexplored for the set covering problem. A convenient encoding of the inequalityconstraints is achieved by means of a multilinear penalty function. Anapproximate energy minimum is obtained by iterating a set of mean fieldequations, in combination with annealing. The approach is numerically testedagainst a set of publicly available test problems with sizes ranging up to5x10^3 rows and 10^6 columns. When comparing the performance with exact resultsfor sizes where these are available, the approach yields results within a fewpercent from the optimal solutions. Comparisons with other approximate methodsalso come out well, in particular given the very low CPU consumption required-- typically a few seconds. Arbitrary problems can be processed using thealgorithm via a public domain server.'}
1.15e+03 pattern= 14 task=  0 (u=1559)  {'docs': u'In this note, we study the easy certificate classes introduced byHemaspaandra, Rothe, and Wechsung, with regard to the question of whether ornot surjective one-way functions exist. This is an important open question incryptology. We show that the existence of partial one-way permutations can becharacterized by separating P from the class of UP sets that, for allunambiguous polynomial-time Turing machines accepting them, always have easy(i.e., polynomial-time computable) certificates. This extends results ofGrollmann and Selman. By Gr\\"adel\'s recent results about one-way functions,this also links statements about easy certificates of NP sets with statementsin finite model theory. Similarly, there exist surjective poly-one one-wayfunctions if and only if there is a set L in P such that not all FewP machinesaccepting L always have easy certificates. We also establish a conditionnecessary and sufficient for the existence of (total) one-way permutations.'}
1.19e+03 pattern= 14 task=  0 (u=1853)  {'docs': u'Two planar sets are circularly separable if there exists a circle enclosingone of the sets and whose open interior disk does not intersect the other set.  This paper studies two problems related to circular separability. Alinear-time algorithm is proposed to decide if two polygons are circularlyseparable. The algorithm outputs the smallest separating circle. The secondproblem asks for the largest circle included in a preprocessed, convex polygon,under some point and/or line constraints. The resulting circle must contain thequery points and it must lie in the halfplanes delimited by the query lines.'}
1.19e+03 pattern= 14 task=  0 (u=1853)  {'docs': u'A circle $C$ separates two planar sets if it encloses one of the sets and itsopen interior disk does not meet the other set. A separating circle is alargest one if it cannot be locally increased while still separating the twogiven sets. An Theta(n log n) optimal algorithm is proposed to find all largestcircles separating two given sets of line segments when line segments areallowed to meet only at their endpoints. In the general case, when linesegments may intersect $\\Omega(n^2)$ times, our algorithm can be adapted towork in O(n alpha(n) log n) time and O(n \\alpha(n)) space, where alpha(n)represents the extremely slowly growing inverse of the Ackermann function.'}
1.19e+03 pattern=  9 task=  1 (u=1853)  {'docs': u'We consider the motion planning problem for a point constrained to move alonga smooth closed convex path of bounded curvature. The workspace of the movingpoint is bounded by a convex polygon with m vertices, containing an obstacle ina form of a simple polygon with $n$ vertices. We present an O(m+n) timealgorithm finding the path, going around the obstacle, whose curvature is thesmallest possible.'}
1.14e+03 pattern= 18 task=  0 (u=2168)  {'docs': u'The aim of the Alma project is the design of a strongly typed constraintprogramming language that combines the advantages of logic and imperativeprogramming. The first stage of the project was the design and implementationof Alma-0, a small programming language that provides a support for declarativeprogramming within the imperative programming framework. It is obtained byextending a subset of Modula-2 by a small number of features inspired by thelogic programming paradigm. In this paper we discuss the rationale for thedesign of Alma-0, the benefits of the resulting hybrid programming framework,and the current work on adding constraint processing capabilities to thelanguage. In particular, we discuss the role of the logical and customaryvariables, the interaction between the constraint store and the program, andthe need for lists.'}
1.39e+03 pattern= 21 task=  1 (u=2168)  {'docs': u"We study here a natural situation when constraint programming can be entirelyreduced to rule-based programming. To this end we explain first how one cancompute on constraint satisfaction problems using rules represented by simplefirst-order formulas. Then we consider constraint satisfaction problems thatare based on predefined, explicitly given constraints. To solve them we firstderive rules from these explicitly given constraints and limit the computationprocess to a repeated application of these rules, combined with labeling.Weconsider here two types of rules. The first type, that we call equality rules,leads to a new notion of local consistency, called {\\em rule consistency} thatturns out to be weaker than arc consistency for constraints of arbitrary arity(called hyper-arc consistency in \\cite{MS98b}). For Boolean constraints ruleconsistency coincides with the closure under the well-known propagation rulesfor Boolean constraints. The second type of rules, that we call membershiprules, yields a rule-based characterization of arc consistency. To showfeasibility of this rule-based approach to constraint programming we show howboth types of rules can be automatically generated, as {\\tt CHR} rules of\\cite{fruhwirth-constraint-95}. This yields an implementation of this approachto programming by means of constraint logic programming. We illustrate theusefulness of this approach to constraint programming by discussing variousexamples, including Boolean constraints, two typical examples of many valuedlogics, constraints dealing with Waltz's language for describing polyhedralscenes, and Allen's qualitative approach to temporal logic."}
1.03e+03 pattern=  4 task=  0 (u=2991)  {'docs': u'We wish to tile a rectangle or a torus with only vertical and horizontal barsof a given length, such that the number of bars in every column and row equalsgiven numbers. We present results for particular instances and for a moregeneral problem, while leaving open the initial problem.'}
1.11e+03 pattern= 14 task=  1 (u=2991)  {'docs': u"We address a discrete tomography problem that arises in the study of theatomic structure of crystal lattices. A polyatomic structure T can be definedas an integer lattice in dimension D>=2, whose points may be occupied by $c$distinct types of atoms. To ``analyze'' T, we conduct ell measurements that wecall_discrete X-rays_. A discrete X-ray in direction xi determines the numberof atoms of each type on each line parallel to xi. Given ell such non-parallelX-rays, we wish to reconstruct T.  The complexity of the problem for c=1 (one atom type) has been completelydetermined by Gardner, Gritzmann and Prangenberg, who proved that the problemis NP-complete for any dimension D>=2 and ell>=3 non-parallel X-rays, and thatit can be solved in polynomial time otherwise.  The NP-completeness result above clearly extends to any c>=2, and thereforewhen studying the polyatomic case we can assume that ell=2. As shown in anotherarticle by the same authors, this problem is also NP-complete for c>=6 atoms,even for dimension D=2 and axis-parallel X-rays. They conjecture that theproblem remains NP-complete for c=3,4,5, although, as they point out, the proofidea does not seem to extend to c<=5.  We resolve the conjecture by proving that the problem is indeed NP-completefor c>=3 in 2D, even for axis-parallel X-rays. Our construction relies heavilyon some structure results for the realizations of 0-1 matrices with given rowand column sums."}
1.11e+03 pattern=  4 task=  2 (u=2991)  {'docs': u'Tomography is the area of reconstructing objects from projections. Here wewish to reconstruct a set of cells in a two dimensional grid, given the numberof cells in every row and column. The set is required to be an hv-convexpolyomino, that is all its cells must be connected and the cells in every rowand column must be consecutive. A simple, polynomial algorithm forreconstructing hv-convex polyominoes is provided, which is several orders ofmagnitudes faster than the best previously known algorithm from Barcucci et al.In addition, the problem of reconstructing a special class of centeredhv-convex polyominoes is addressed. (An object is centered if it contains a rowwhose length equals the total width of the object). It is shown that in thiscase the reconstruction problem can be solved in linear time.'}
1.15e+03 pattern=  4 task=  0 (u=3075)  {'docs': u"A decade ago, a beautiful paper by Wagner developed a ``toolkit'' that incertain cases allows one to prove problems hard for parallel access to NP.However, the problems his toolkit applies to most directly are not overlynatural. During the past year, problems that previously were known only to beNP-hard or coNP-hard have been shown to be hard even for the class of setssolvable via parallel access to NP. Many of these problems are longstanding andextremely natural, such as the Minimum Equivalent Expression problem (which wasthe original motivation for creating the polynomial hierarchy), the problem ofdetermining the winner in the election system introduced by Lewis Carroll in1876, and the problem of determining on which inputs heuristic algorithmsperform well. In the present article, we survey this recent progress in raisinglower bounds."}
1.22e+03 pattern=  4 task=  1 (u=3075)  {'docs': u'During the past decade, nine papers have obtained increasingly strongconsequences from the assumption that boolean or bounded-query hierarchiescollapse. The final four papers of this nine-paper progression actually achievedownward collapse---that is, they show that high-level collapses inducecollapses at (what beforehand were thought to be) lower complexity levels. Forexample, for each $k\\geq 2$ it is now known that if $\\psigkone=\\psigktwo$ then$\\ph=\\sigmak$. This article surveys the history, the results, and thetechnique---the so-called easy-hard method---of these nine papers.'}
1.22e+03 pattern= 14 task=  2 (u=3075)  {'docs': u'Hemaspaandra, Hempel, and Wechsung [cs.CC/9909020] raised the followingquestions: If one is allowed one question to each of two different informationsources, does the order in which one asks the questions affect the class ofproblems that one can solve with the given access? If so, which order yieldsthe greater computational power?  The answers to these questions have been learned-inasfar as they can belearned without resolving whether or not the polynomial hierarchy collapses-forboth the polynomial hierarchy and the boolean hierarchy. In the polynomialhierarchy, query order never matters. In the boolean hierarchy, query ordersometimes does not matter and, unless the polynomial hierarchy collapses,sometimes does matter. Furthermore, the study of query order has yieldeddividends in seemingly unrelated areas, such as bottleneck computations anddownward translation of equality.  In this article, we present some of the central results on query order. Thearticle is written in such a way as to encourage the reader to try his or herown hand at proving some of these results. We also give literature pointers tothe quickly growing set of related results and applications.'}
1.22e+03 pattern= 22 task=  3 (u=3075)  {'docs': u"Downward translation of equality refers to cases where a collapse of somepair of complexity classes would induce a collapse of some other pair ofcomplexity classes that (a priori) one expects are smaller. Recently, the firstdownward translation of equality was obtained that applied to the polynomialhierarchy-in particular, to bounded access to its levels [cs.CC/9910007]. Inthis paper, we provide a much broader downward translation that extends notonly that downward translation but also that translation's elegant enhancementby Buhrman and Fortnow. Our work also sheds light on previous research on thestructure of refined polynomial hierarchies, and strengthens the connectionbetween the collapse of bounded query hierarchies and the collapse of thepolynomial hierarchy."}
  981 pattern=  3 task=  0 (u=3379)  {'docs': u'We present here a generalization of the work done by Rabin and Ben-Or. Wegive a protocol for multiparty computation which tolerates any Q^2 activeadversary structure based on the existence of a broadcast channel, securecommunication between each pair of participants, and a monotone span programwith multiplication tolerating the structure. The secrecy achieved isunconditional although we allow an exponentially small probability of error.This is possible due to a protocol for computing the product of two valuesalready shared by means of a homomorphic commitment scheme which appearedoriginally in a paper of Chaum, Evertse and van de Graaf.'}
  957 pattern=  7 task=  0 (u=4222)  {'docs': u'The universal object oriented languages made programming more simple andefficient. In the article is considered possibilities of using similar methodsin computer algebra. A clear and powerful universal language is useful ifparticular problem was not implemented in standard software packages likeREDUCE, MATHEMATICA, etc. and if the using of internal programming languages ofthe packages looks not very efficient.  Functional languages like LISP had some advantages and traditions foralgebraic and symbolic manipulations. Functional and object orientedprogramming are not incompatible ones. An extension of the model of an objectfor manipulation with pure functions and algebraic expressions is considered.'}
  443 pattern=  3 task=  0 (u=4682)  {'docs': u'This thesis presents two similarity-based approaches to sparse data problems.The first approach is to build soft, hierarchical clusters: soft, because eachevent belongs to each cluster with some probability; hierarchical, becausecluster centroids are iteratively split to model finer distinctions. Our secondapproach is a nearest-neighbor approach: instead of calculating a centroid foreach class, as in the hierarchical clustering approach, we in essence build acluster around each word. We compare several such nearest-neighbor approacheson a word sense disambiguation task and find that as a whole, their performanceis far superior to that of standard methods. In another set of experiments, weshow that using estimation techniques based on the nearest-neighbor modelenables us to achieve perplexity reductions of more than 20 percent overstandard techniques in the prediction of low-frequency events, andstatistically significant speech recognition error-rate reduction.'}
1.46e+03 pattern= 14 task=  0 (u=5068)  {'docs': u'Phase transitions in combinatorial problems have recently been shown to beuseful in locating "hard" instances of combinatorial problems. The connectionbetween computational complexity and the existence of phase transitions hasbeen addressed in Statistical Mechanics and Artificial Intelligence, but notstudied rigorously.  We take a step in this direction by investigating the existence of sharpthresholds for the class of generalized satisfiability problems defined bySchaefer. In the case when all constraints are clauses we give a completecharacterization of such problems that have a sharp threshold.  While NP-completeness does not imply (even in this restricted case) theexistence of a sharp threshold, it "almost implies" this, since clausalgeneralized satisfiability problems that lack a sharp threshold are either  1. polynomial time solvable, or  2. predicted, with success probability lower bounded by some positiveconstant by across all the probability range, by a single, trivial procedure.'}
1.15e+03 pattern= 20 task=  0 (u=5377)  {'docs': u"Rice's Theorem states that every nontrivial language property of therecursively enumerable sets is undecidable. Borchert and Stephan initiated thesearch for complexity-theoretic analogs of Rice's Theorem. In particular, theyproved that every nontrivial counting property of circuits is UP-hard, and thata number of closely related problems are SPP-hard.  The present paper studies whether their UP-hardness result itself can beimproved to SPP-hardness. We show that their UP-hardness result cannot bestrengthened to SPP-hardness unless unlikely complexity class containmentshold. Nonetheless, we prove that every P-constructibly bi-infinite countingproperty of circuits is SPP-hard. We also raise their general lower bound fromunambiguous nondeterminism to constant-ambiguity nondeterminism."}
1.15e+03 pattern=  4 task=  1 (u=5377)  {'docs': u"We prove that the join of two sets may actually fall into a lower level ofthe extended low hierarchy than either of the sets. In particular, there existsets that are not in the second level of the extended low hierarchy, EL_2, yettheir join is in EL_2. That is, in terms of extended lowness, the join operatorcan lower complexity. Since in a strong intuitive sense the join does not lowercomplexity, our result suggests that the extended low hierarchy is unnatural asa complexity measure. We also study the closure properties of EL_ and provethat EL_2 is not closed under certain Boolean operations. To this end, weestablish the first known (and optimal) EL_2 lower bounds for certain notionsgeneralizing Selman's P-selectivity, which may be regarded as an interestingresult in its own right."}
1.33e+03 pattern= 21 task=  2 (u=5377)  {'docs': u'We discuss the use of projects in first-year graduate complexity theorycourses.'}
1.05e+03 pattern=  3 task=  0 (u=5489)  {'docs': u'This paper provides a proof of the proposed Internet standard Transport LevelSecurity protocol using the Gong-Needham-Yahalom logic. It is intended as ateaching aid and hopes to show to students: the potency of a formal method forprotocol design; some of the subtleties of authenticating parties on a networkwhere all messages can be intercepted; the design of what should be a widelyaccepted standard.'}
1.07e+03 pattern=  3 task=  1 (u=5489)  {'docs': u"This paper presents some fundamental collective choice theory for informationsystem designers, particularly those working in the field of computer-supportedcooperative work. This paper is focused on a presentation of Arrow'sPossibility and Impossibility theorems which form the fundamental boundary onthe efficacy of collective choice: voting and selection procedures. It restatesthe conditions that Arrow placed on collective choice functions in morerigorous second-order logic, which could be used as a set of test conditionsfor implementations, and a useful probabilistic result for analyzing votes onissue pairs. It also describes some simple collective choice functions. Thereis also some discussion of how enterprises should approach putting theirresources under collective control: giving an outline of a superstructure ofperformative agents to carry out this function and what distributing processingtechnology would be needed."}
1.47e+03 pattern= 31 task=  0 (u=5546)  {'docs': u'An elastic ideal 2D propagation medium, i.e., a membrane, can be simulated bymodels discretizing the wave equation on the time-space grid (finite differencemethods), or locally discretizing the solution of the wave equation (waveguidemeshes). The two approaches provide equivalent computational structures, andintroduce numerical dispersion that induces a misalignment of the modes fromtheir theoretical positions. Prior literature shows that dispersion can bearbitrarily reduced by oversizing and oversampling the mesh, or by adptingoffline warping techniques. In this paper we propose to reduce numericaldispersion by embedding warping elements, i.e., properly tuned allpass filters,in the structure. The resulting model exhibits a significant reduction indispersion, and requires less computational resources than a regular meshstructure having comparable accuracy.'}
1.5e+03 pattern=  4 task=  0 (u=5622)  {'docs': u'We study the quantum complexity of the static set membership problem: given asubset S (|S| \\leq n) of a universe of size m (m \\gg n), store it as a table ofbits so that queries of the form `Is x \\in S?\' can be answered. The goal is touse a small table and yet answer queries using few bitprobes. This problem wasconsidered recently by Buhrman, Miltersen, Radhakrishnan and Venkatesh, wherelower and upper bounds were shown for this problem in the classicaldeterministic and randomized models. In this paper, we formulate this problemin the "quantum bitprobe model" and show tradeoff results between space andtime.In this model, the storage scheme is classical but the query scheme isquantum.We show, roughly speaking, that similar lower bounds hold in thequantum model as in the classical model, which imply that the classical upperbounds are more or less tight even in the quantum case. Our lower bounds areproved using linear algebraic techniques.'}
  813 pattern=  3 task=  0 (u=5871)  {'docs': u'In this document we study the application of weighted proportional fairnessto data flows in the Internet. We let the users set the weights of theirconnections in order to maximise the utility they get from the network. Whencombined with a pricing scheme where connections are billed by weight and time,such a system is known to maximise the total utility of the network. Our studycase is a national Web cache server connected to long distance links. Wepropose two ways of weighting TCP connections by manipulating some parametersof the protocol and present results from simulations and prototypes. We finallydiscuss how proportional fairness could be used to implement an Internet withdifferentiated services.'}
  683 pattern=  4 task=  0 (u=6045)  {'docs': u'We consider the possibility of encoding m classical bits into much fewer nquantum bits so that an arbitrary bit from the original m bits can be recoveredwith a good probability, and we show that non-trivial quantum encodings existthat have no classical counterparts. On the other hand, we show that quantumencodings cannot be much more succint as compared to classical encodings, andwe provide a lower bound on such quantum encodings. Finally, using this lowerbound, we prove an exponential lower bound on the size of 1-way quantum finiteautomata for a family of languages accepted by linear sized deterministicfinite automata.'}
1.46e+03 pattern= 12 task=  0 (u=6232)  {'docs': u"The ability of a robot to detect and respond to changes in its environment ispotentially very useful, as it draws attention to new and potentially importantfeatures. We describe an algorithm for learning to filter out previouslyexperienced stimuli to allow further concentration on novel features. Thealgorithm uses a model of habituation, a biological process which causes adecrement in response with repeated presentation. Experiments with a mobilerobot are presented in which the robot detects the most novel stimulus andturns towards it (`neotaxis')."}
1.48e+03 pattern= 27 task=  0 (u=7190)  {'docs': u'The vertex-cover problem is studied for random graphs $G_{N,cN}$ having $N$vertices and $cN$ edges. Exact numerical results are obtained by abranch-and-bound algorithm. It is found that a transition in the coverabilityat a $c$-dependent threshold $x=x_c(c)$ appears, where $xN$ is the cardinalityof the vertex cover. This transition coincides with a sharp peak of the typicalnumerical effort, which is needed to decide whether there exists a cover with$xN$ vertices or not. For small edge concentrations $c\\ll 0.5$, a clusterexpansion is performed, giving very accurate results in this regime. Theseresults are extended using methods developed in statistical physics. The socalled annealed approximation reproduces a rigorous bound on $x_c(c)$ which wasknown previously. The main part of the paper contains an application of thereplica method. Within the replica symmetric ansatz the threshold $x_c(c)$ andthe critical backbone size $b_c(c)$ can be calculated. For $c<e/2$ the resultsshow an excellent agreement with the numerical findings. At average vertexdegree $2c=e$, an instability of the simple replica symmetric solution occurs.'}
1.37e+03 pattern= 23 task=  0 (u=8008)  {'docs': u'The scalability of massively parallel algorithms is a fundamental question incomputer science. We study the scalability and the efficiency of a conservativemassively parallel algorithm for discrete-event simulations where the discreteevents are Poisson arrivals. The parallel algorithm is applicable to a widerange of problems, including dynamic Monte Carlo simulations for largeasynchronous systems with short-range interactions. The evolution of thesimulated time horizon is analogous to a growing and fluctuating surface, andthe efficiency of the algorithm corresponds to the density of local minima ofthis surface. In one dimension we find that the steady state of the macroscopiclandscape is governed by the Edwards-Wilkinson Hamiltonian, which implies thatthe algorithm is scalable. Preliminary results for higher-dimensional logicaltopologies are discussed.'}
  919 pattern=  4 task=  0 (u=8043)  {'docs': u"We significantly improve known time bounds for solving the minimum cutproblem on undirected graphs. We use a ``semi-duality'' between minimum cutsand maximum spanning tree packings combined with our previously developedrandom sampling techniques. We give a randomized algorithm that finds a minimumcut in an m-edge, n-vertex graph with high probability in O(m log^3 n) time. Wealso give a simpler randomized algorithm that finds all minimum cuts with highprobability in O(n^2 log n) time. This variant has an optimal RNCparallelization. Both variants improve on the previous best time bound of O(n^2log^3 n). Other applications of the tree-packing approach are new, nearly tightbounds on the number of near minimum cuts a graph may have and a new datastructure for representing them in a space-efficient manner."}
1.34e+03 pattern= 14 task=  0 (u=9201)  {'docs': u'In this paper, we focus on the problem of existence and computing of smalland large stable models. We show that for every fixed integer k, there is alinear-time algorithm to decide the problem LSM (large stable models problem):does a logic program P have a stable model of size at least |P|-k. In contrast,we show that the problem SSM (small stable models problem) to decide whether alogic program P has a stable model of size at most k is much harder. We presenttwo algorithms for this problem but their running time is given by polynomialsof order depending on k. We show that the problem SSM is fixed-parameterintractable by demonstrating that it is W[2]-hard. This result implies that itis unlikely, an algorithm exists to compute stable models of size at most kthat would run in time O(n^c), where c is a constant independent of k. We alsoprovide an upper bound on the fixed-parameter complexity of the problem SSM byshowing that it belongs to the class W[3].'}
  918 pattern=  4 task=  0 (u=9217)  {'docs': u'We consider the problem of coloring k-colorable graphs with the fewestpossible colors. We present a randomized polynomial time algorithm that colorsa 3-colorable graph on $n$ vertices with min O(Delta^{1/3} log^{1/2} Delta logn), O(n^{1/4} log^{1/2} n) colors where Delta is the maximum degree of anyvertex. Besides giving the best known approximation ratio in terms of n, thismarks the first non-trivial approximation result as a function of the maximumdegree Delta. This result can be generalized to k-colorable graphs to obtain acoloring using min O(Delta^{1-2/k} log^{1/2} Delta log n), O(n^{1-3/(k+1)}log^{1/2} n) colors. Our results are inspired by the recent work of Goemans andWilliamson who used an algorithm for semidefinite optimization problems, whichgeneralize linear programs, to obtain improved approximations for the MAX CUTand MAX 2-SAT problems. An intriguing outcome of our work is a dualityrelationship established between the value of the optimum solution to oursemidefinite program and the Lovasz theta-function. We show lower bounds on thegap between the optimum solution of our semidefinite program and the actualchromatic number; by duality this also demonstrates interesting new facts aboutthe theta-function.'}
  969 pattern=  4 task=  0 (u=9391)  {'docs': u'In this paper we consider quantum interactive proof systems, i.e.,interactive proof systems in which the prover and verifier may perform quantumcomputations and exchange quantum messages. It is proved that every language inPSPACE has a quantum interactive proof system that requires only two rounds ofcommunication between the prover and verifier, while having exponentially small(one-sided) probability of error. It follows that quantum interactive proofsystems are strictly more powerful than classical interactive proof systems inthe constant-round case unless the polynomial time hierarchy collapses to thesecond level.'}
 58.9 pattern=  1 task=  0 (u=10803)  {'docs': u'A machine translation system is said to be *complete* if all expressions thatare correct according to the source-language grammar can be translated into thetarget language. This paper addresses the completeness issue for compositionalmachine translation in general, and for compositional machine translation ofcontext-free grammars in particular. Conditions that guarantee translationcompleteness of context-free grammars are presented.'}
 57.7 pattern=  1 task=  0 (u=10807)  {'docs': u'This paper describes the development and use of a lexical semantic databasefor the Verbmobil speech-to-speech machine translation system. The motivationis to provide a common information source for the distributed development ofthe semantics, transfer and semantic evaluation modules and to store lexicalsemantic information application-independently.  The database is organized around a set of abstract semantic classes and hasbeen used to define the semantic contributions of the lemmata in the vocabularyof the system, to automatically create semantic lexica and to check thecorrectness of the semantic representations built up. The semantic classes aremodelled using an inheritance hierarchy. The database is implemented using thelexicon formalism LeX4 developed during the project.'}
1.12e+03 pattern=  9 task=  0 (u=11776)  {'docs': u'The notion of quantum Turing machines is a basis of quantum complexitytheory. We discuss a general model of multi-tape, multi-head Quantum Turingmachines with multi final states that also allow tape heads to stay still.'}
  332 pattern=  0 task=  0 (u=11855)  {'docs': u'Although sloppy interpretation is usually accounted for by theories ofellipsis, it often arises in non-elliptical contexts. In this paper, a theoryof sloppy interpretation is provided which captures this fact. The underlyingidea is that sloppy interpretation results from a semantic constraint onparallel structures and the theory is shown to predict sloppy readings fordeaccented and paycheck sentences as well as relational-, event-, andone-anaphora. It is further shown to capture the interaction of sloppy/strictambiguity with quantification and binding.'}
  834 pattern=  4 task=  0 (u=12106)  {'docs': u'Consider a system F of n polynomial equations in n unknowns, over analgebraically closed field of arbitrary characteristic. We present a fastmethod to find a point in every irreducible component of the zero set Z of F.Our techniques allow us to sharpen and lower prior complexity bounds for thisproblem by fully taking into account the monomial term structure. As acorollary of our development we also obtain new explicit formulae for the exactnumber of isolated roots of F and the intersection multiplicity of thepositive-dimensional part of Z. Finally, we present a combinatorialconstruction of non-degenerate polynomial systems, with specified monomial termstructure and maximally many isolated roots, which may be of independentinterest.'}
1.32e+03 pattern= 12 task=  0 (u=13475)  {'docs': u"We describe a framework and equations used to model and predict the behaviorof multi-agent systems (MASs) with learning agents. A difference equation isused for calculating the progression of an agent's error in its decisionfunction, thereby telling us how the agent is expected to fare in the MAS. Theequation relies on parameters which capture the agent's learning abilities,such as its change rate, learning rate and retention rate, as well as relevantaspects of the MAS such as the impact that agents have on each other. Wevalidate the framework with experimental results using reinforcement learningagents in a market system, as well as with other experimental results gatheredfrom the AI literature. Finally, we use PAC-theory to show how to calculatebounds on the values of the learning parameters."}
1.42e+03 pattern=  4 task=  0 (u=14389)  {'docs': u'The traditional split-up into a low level language and a high level languagein the design of computer algebra systems may become obsolete with the adventof more versatile computer languages. We describe GiNaC, a special-purposesystem that deliberately denies the need for such a distinction. It is entirelywritten in C++ and the user can interact with it directly in that language. Itwas designed to provide efficient handling of multivariate polynomials,algebras and special functions that are needed for loop calculations intheoretical quantum field theory. It also bears some potential to become a moregeneral purpose symbolic package.'}
1.49e+03 pattern=  4 task=  0 (u=16227)  {'docs': u"Many problems in robust control and motion planning can be reduced to eitherfind a sound approximation of the solution space determined by a set ofnonlinear inequalities, or to the ``guaranteed tuning problem'' as defined byJaulin and Walter, which amounts to finding a value for some tuning parametersuch that a set of inequalities be verified for all the possible values of someperturbation vector. A classical approach to solve these problems, whichsatisfies the strong soundness requirement, involves some quantifierelimination procedure such as Collins' Cylindrical Algebraic Decompositionsymbolic method. Sound numerical methods using interval arithmetic and localconsistency enforcement to prune the search space are presented in this paperas much faster alternatives for both soundly solving systems of nonlinearinequalities, and addressing the guaranteed tuning problem whenever theperturbation vector has dimension one. The use of these methods in cameracontrol is investigated, and experiments with the prototype of a declarativemodeller to express camera motion using a cinematic language are reported andcommented."}
1.46e+03 pattern= 30 task=  0 (u=16629)  {'docs': u"This paper is a structured introduction to Light Affine Logic, and to itsintuitionistic fragment. Light Affine Logic has a polynomially costing cutelimination (P-Time correctness), and encodes all P-Time Turing machines(P-Time completeness). P-Time correctness is proved by introducing the Proofnets for Intuitionistic Light Affine Logic. P-Time completeness is demonstratedin full details thanks to a very compact program notation. On one side, theproof of P-Time correctness describes how the complexity of cut elimination iscontrolled, thanks to a suitable cut elimination strategy that exploitsstructural properties of the Proof nets. This allows to have a good catch onthe meaning of the ``paragraph'' modality, which is a peculiarity of lightlogics. On the other side, the proof of P-Time completeness, together with alot of programming examples, gives a flavor of the non trivial task ofprogramming with resource limitations, using Intuitionistic Light Affine Logicderivations as programs."}
1.14e+03 pattern=  5 task=  0 (u=17239)  {'docs': u'We propose a new data structure to compute the Delaunay triangulation of aset of points in the plane. It combines good worst case complexity, fastbehavior on real data, and small memory occupation.  The location structure is organized into several levels. The lowest leveljust consists of the triangulation, then each level contains the triangulationof a small sample of the levels below. Point location is done by marching in atriangulation to determine the nearest neighbor of the query at that level,then the march restarts from that neighbor at the level below. Using a smallsample (3%) allows a small memory occupation; the march and the use of thenearest neighbor to change levels quickly locate the query.'}
1.14e+03 pattern=  4 task=  1 (u=17239)  {'docs': u"This paper presents how the space of spheres and shelling may be used todelete a point from a $d$-dimensional triangulation efficiently. In dimensiontwo, if k is the degree of the deleted vertex, the complexity is O(k log k),but we notice that this number only applies to low cost operations, while timeconsuming computations are only done a linear number of times.  This algorithm may be viewed as a variation of Heller's algorithm, which ispopular in the geographic information system community. Unfortunately, Helleralgorithm is false, as explained in this paper."}
1.14e+03 pattern= 17 task=  2 (u=17239)  {'docs': u'The assumption of real-number arithmetic, which is at the basis ofconventional geometric algorithms, has been seriously challenged in recentyears, since digital computers do not exhibit such capability.  A geometric predicate usually consists of evaluating the sign of somealgebraic expression. In most cases, rounded computations yield a reliableresult, but sometimes rounded arithmetic introduces errors which may invalidatethe algorithms. The rounded arithmetic may produce an incorrect result only ifthe exact absolute value of the algebraic expression is smaller than some(small) varepsilon, which represents the largest error that may arise in theevaluation of the expression. The threshold varepsilon depends on the structureof the expression and on the adopted computer arithmetic, assuming that theinput operands are error-free.  A pair (arithmetic engine,threshold) is an "arithmetic filter". In this paperwe develop a general technique for assessing the efficacy of an arithmeticfilter. The analysis consists of evaluating both the threshold and theprobability of failure of the filter.  To exemplify the approach, under the assumption that the input points bechosen randomly in a unit ball or unit cube with uniform density, we analyzethe two important predicates "which-side" and "insphere". We show that theprobability that the absolute values of the corresponding determinants be nolarger than some positive value V, with emphasis on small V, is Theta(V) forthe which-side predicate, while for the insphere predicate it is Theta(V^(2/3))in dimension 1, O(sqrt(V)) in dimension 2, and O(sqrt(V) ln(1/V)) in higherdimensions. Constants are small, and are given in the paper.'}
1.14e+03 pattern= 17 task=  2 (u=17239)  {'docs': u'An efficient technique to solve precision problems consists in using exactcomputations. For geometric predicates, using systematically expensive exactcomputations can be avoided by the use of filters. The predicate is firstevaluated using rounding computations, and an error estimation gives acertificate of the validity of the result. In this note, we studies thestatistical efficiency of filters for cosphericity predicate with an assumptionof regular distribution of the points. We prove that the expected value of thepolynomial corresponding to the in sphere test is greater than epsilon withprobability O(epsilon log 1/epsilon) improving the results of a previous paperby the same authors.'}
1.21e+03 pattern= 14 task=  3 (u=17239)  {'docs': u'Given a finite set of non-collinear points in the plane, there exists a linethat passes through exactly two points. Such a line is called an ordinary line.An efficient algorithm for computing such a line was proposed by Mukhopadhyayet al. In this note we extend this result in two directions. We first show howto use this algorithm to compute an ordinary conic, that is, a conic passingthrough exactly five points, assuming that all the points do not lie on thesame conic. Both our proofs of existence and the consequent algorithms aresimpler than previous ones. We next show how to compute an ordinary hyperplanein three and higher dimensions.'}
1.14e+03 pattern= 11 task=  0 (u=17268)  {'docs': u'Natural language generation systems (NLG) map non-linguistic representationsinto strings of words through a number of steps using intermediaterepresentations of various levels of abstraction. Template based systems, bycontrast, tend to use only one representation level, i.e. fixed strings, whichare combined, possibly in a sophisticated way, to generate the final text.  In some circumstances, it may be profitable to combine NLG and template basedtechniques. The issue of combining generation techniques can be seen in moreabstract terms as the issue of mixing levels of representation of differentdegrees of linguistic abstraction. This paper aims at defining a referencearchitecture for systems using mixed representations. We argue that mixedrepresentations can be used without abandoning a linguistically groundedapproach to language generation.'}
1.14e+03 pattern= 14 task=  0 (u=17271)  {'docs': u'We provide a lower bound construction showing that the union of unit balls inthree-dimensional space has quadratic complexity, even if they all contain theorigin. This settles a conjecture of Sharir.'}
1.3e+03 pattern=  4 task=  0 (u=17989)  {'docs': u'An approach to the solution of NP-complete problems based on quantumcomputing and chaotic dynamics is proposed. We consider the satisfiabilityproblem and argue that the problem, in principle, can be solved in polynomialtime if we combine the quantum computer with the chaotic dynamics amplifierbased on the logistic map. We discuss a possible implementation of such achaotic quantum computation by using the atomic quantum computer with quantumgates described by the Hartree-Fock equations. In this case, in principle, onecan build not only standard linear quantum gates but also nonlinear gates andmoreover they obey to Fermi statistics. This new type of entaglement relatedwith Fermi statistics can be interesting also for quantum communication theory.'}
1.26e+03 pattern= 25 task=  0 (u=19509)  {'docs': u'Recent developments in theoretical linguistics have lead to a widespreadacceptance of constraint-based analyses of prosodic morphology phenomena suchas truncation, infixation, floating morphemes and reduplication. Of these,reduplication is particularly challenging for state-of-the-art computationalmorphology, since it involves copying of some part of a phonological string. Inthis paper I argue for certain extensions to the one-level model of phonologyand morphology (Bird & Ellison 1994) to cover the computational aspects ofprosodic morphology using finite-state methods. In a nutshell, enriched lexicalrepresentations provide additional automaton arcs to repeat or skip sounds andalso to allow insertion of additional material. A kind of resourceconsciousness is introduced to control this additional freedom, distinguishingbetween producer and consumer arcs. The non-finite-state copying aspect ofreduplication is mapped to automata intersection, itself a non-finite-stateoperation. Bounded local optimization prunes certain automaton arcs that failto contribute to linguistic optimisation criteria. The paper then presentsimplemented case studies of Ulwa construct state infixation, Germanhypocoristic truncation and Tagalog over-applying reduplication that illustratethe expressive power of this approach, before its merits and limitations arediscussed and possible extensions are sketched. I conclude that the one-levelapproach to prosodic morphology presents an attractive way of extendingfinite-state techniques to difficult phenomena that hitherto resisted elegantcomputational analyses.'}
1.45e+03 pattern= 29 task=  0 (u=19529)  {'docs': u'A one-time pad (OTP) based cipher to insure both data protection andintegrity when mobile code arrives to a remote host is presented. Dataprotection is required when a mobile agent could retrieve confidentialinformation that would be encrypted in untrusted nodes of the network; in thiscase, information management could not rely on carrying an encryption key. Dataintegrity is a prerequisite because mobile code must be protected againstmalicious hosts that, by counterfeiting or removing collected data, could coverinformation to the server that has sent the agent. The algorithm described inthis article seems to be simple enough, so as to be easily implemented. Thisscheme is based on a non-interactive protocol and allows a remote host tochange its own data on-the-fly and, at the same time, protecting informationagainst handling by other hosts.'}
1.44e+03 pattern= 10 task=  0 (u=19539)  {'docs': u'When reasoning in description, modal or temporal logics it is often useful toconsider axioms representing universal truths in the domain of discourse.Reasoning with respect to an arbitrary set of axioms is hard, even forrelatively inexpressive logics, and it is essential to deal with such axioms inan efficient manner if implemented systems are to be effective in realapplications. This is particularly relevant to Description Logics, wheresubsumption reasoning with respect to a terminology is a fundamental problem.Two optimisation techniques that have proved to be particularly effective indealing with terminologies are lazy unfolding and absorption. In this paper weseek to improve our theoretical understanding of these important techniques. Wedefine a formal framework that allows the techniques to be precisely described,establish conditions under which they can be safely applied, and prove that,provided these conditions are respected, subsumption testing algorithms willstill function correctly. These results are used to show that the proceduresused in the FaCT system are correct and, moreover, to show how efficiency canbe significantly improved, while still retaining the guarantee of correctness,by relaxing the safety conditions for absorption.'}
1.01e+03 pattern=  3 task=  0 (u=20283)  {'docs': u'This paper, following (Dymetman:1998), presents an approach to grammardescription and processing based on the geometry of cancellation diagrams, aconcept which plays a central role in combinatorial group theory(Lyndon-Schuppe:1977). The focus here is on the geometric intuitions and onrelating group-theoretical diagrams to the traditional charts associated withcontext-free grammars and type-0 rewriting systems. The paper is structured asfollows. We begin in Section 1 by analyzing charts in terms of constructscalled cells, which are a geometrical counterpart to rules. Then we move inSection 2 to a presentation of cancellation diagrams and show how they can beused computationally. In Section 3 we give a formal algebraic presentation ofthe concept of group computation structure, which is based on the standardnotions of free group and conjugacy. We then relate in Section 4 the geometricand the algebraic views of computation by using the fundamental theorem ofcombinatorial group theory (Rotman:1994). In Section 5 we study in more detailthe relationship between the two views on the basis of a simple grammar statedas a group computation structure. In section 6 we extend this grammar to handlenon-local constructs such as relative pronouns and quantifiers. We conclude inSection 7 with some brief notes on the differences between normal submonoidsand normal subgroups, group computation versus rewriting systems, and the useof group morphisms to study the computational complexity of parsing andgeneration.'}
1.01e+03 pattern=  3 task=  0 (u=20285)  {'docs': u"Recent technological advances have made it possible to build real-time,interactive spoken dialogue systems for a wide variety of applications.However, when users do not respect the limitations of such systems, performancetypically degrades. Although users differ with respect to their knowledge ofsystem limitations, and although different dialogue strategies make systemlimitations more apparent to users, most current systems do not try to improveperformance by adapting dialogue behavior to individual users. This paperpresents an empirical evaluation of TOOT, an adaptable spoken dialogue systemfor retrieving train schedules on the web. We conduct an experiment in which 20users carry out 4 tasks with both adaptable and non-adaptable versions of TOOT,resulting in a corpus of 80 dialogues. The values for a wide range ofevaluation measures are then extracted from this corpus. Our results show thatadaptable TOOT generally outperforms non-adaptable TOOT, and that the utilityof adaptation depends on TOOT's initial dialogue strategies."}
1.06e+03 pattern=  4 task=  0 (u=20541)  {'docs': u"Consider the finite regular language L_n = {w0 : w \\in {0,1}^*, |w| \\le n}.It was shown by Ambainis, Nayak, Ta-Shma and Vazirani that while this languageis accepted by a deterministic finite automaton of size O(n), any one-wayquantum finite automaton (QFA) for it has size 2^{Omega(n/log n)}. This wasbased on the fact that the evolution of a QFA is required to be reversible.When arbitrary intermediate measurements are allowed, this intuition breaksdown. Nonetheless, we show a 2^{Omega(n)} lower bound for such QFA for L_n,thus also improving the previous bound. The improved bound is obtained bysimple entropy arguments based on Holevo's theorem. This method also allows usto obtain an asymptotically optimal (1-H(p))n bound for the dense quantum codes(random access codes) introduced by Ambainis et al. We then turn to Holevo'stheorem, and show that in typical situations, it may be replaced by a tighterand more transparent in-probability bound."}
1.45e+03 pattern=  4 task=  1 (u=20541)  {'docs': u"One of the most intriguing facts about communication using quantum states isthat these states cannot be used to transmit more classical bits than thenumber of qubits used, yet there are ways of conveying information withexponentially fewer qubits than possible classically. Moreover, these methodshave a very simple structure---they involve little interaction between thecommunicating parties. We look more closely at the ways in which informationencoded in quantum states may be manipulated, and consider the question as towhether every classical protocol may be transformed to a ``simpler'' quantumprotocol of similar efficiency. By a simpler protocol, we mean a protocol thatuses fewer message exchanges. We show that for any constant k, there is aproblem such that its k+1 message classical communication complexity isexponentially smaller than its k message quantum communication complexity, thusanswering the above question in the negative. Our result builds on twoprimitives, local transitions in bi-partite states (based on previous work) andaverage encoding which may be of significance in other applications as well."}
1.49e+03 pattern= 32 task=  0 (u=21264)  {'docs': u"While Kolmogorov complexity is the accepted absolute measure of informationcontent of an individual finite object, a similarly absolute notion is neededfor the relation between an individual data sample and an individual modelsummarizing the information in the data, for example, a finite set (orprobability distribution) where the data sample typically came from. Thestatistical theory based on such relations between individual objects can becalled algorithmic statistics, in contrast to classical statistical theory thatdeals with relations between probabilistic ensembles. We develop thealgorithmic theory of statistic, sufficient statistic, and minimal sufficientstatistic. This theory is based on two-part codes consisting of the code forthe statistic (the model summarizing the regularity, the meaningfulinformation, in the data) and the model-to-data code. In contrast to thesituation in probabilistic statistical theory, the algorithmic relation of(minimal) sufficiency is an absolute relation between the individual model andthe individual data sample. We distinguish implicit and explicit descriptionsof the models. We give characterizations of algorithmic (Kolmogorov) minimalsufficient statistic for all data samples for both description modes--in theexplicit mode under some constraints. We also strengthen and elaborate earlierresults on the ``Kolmogorov structure function'' and ``absolutelynon-stochastic objects''--those rare objects for which the simplest models thatsummarize their relevant information (minimal sufficient statistics) are atleast as complex as the objects themselves. We demonstrate a close relationbetween the probabilistic notions and the algorithmic ones."}
1.24e+03 pattern= 23 task=  0 (u=21474)  {'docs': u"We propose a hybrid image-space/object-space solution to the classical hiddensurface removal problem: Given n disjoint triangles in Real^3 and p samplepoints (``pixels'') in the xy-plane, determine the first triangle directlybehind each pixel. Our algorithm constructs the sampled visibility map of thetriangles with respect to the pixels, which is the subset of the trapezoids ina trapezoidal decomposition of the analytic visibility map that contain atleast one pixel. The sampled visibility map adapts to local changes in imagecomplexity, and its complexity is bounded both by the number of pixels and bythe complexity of the analytic visibility map. Our algorithm runs in timeO(n^{1+e} + n^{2/3+e}t^{2/3} + p), where t is the output size and e is anypositive constant. This is nearly optimal in the worst case and comparesfavorably with the best output-sensitive algorithms for both ray casting andanalytic hidden surface removal. In the special case where the pixels form aregular grid, a sweepline variant of our algorithm runs in time O(n^{1+e} +n^{2/3+e}t^{2/3} + t log p), which is usually sublinear in the number ofpixels."}
1.23e+03 pattern= 17 task=  0 (u=22966)  {'docs': u"Suppose A is a finite set equipped with a probability measure P and let M bea ``mass'' function on A. We give a probabilistic characterization of the mostefficient way in which A^n can be almost-covered using spheres of a fixedradius. An almost-covering is a subset C_n of A^n, such that the union of thespheres centered at the points of C_n has probability close to one with respectto the product measure P^n. An efficient covering is one with small massM^n(C_n); n is typically large. With different choices for M and the geometryon A our results give various corollaries as special cases, including Shannon'sdata compression theorem, a version of Stein's lemma (in hypothesis testing),and a new converse to some measure concentration inequalities on discretespaces. Under mild conditions, we generalize our results to abstract spaces andnon-product measures."}
1.22e+03 pattern= 21 task=  0 (u=24938)  {'docs': u"In this article, we study parameterized complexity theory from theperspective of logic, or more specifically, descriptive complexity theory.  We propose to consider parameterized model-checking problems for variousfragments of first-order logic as generic parameterized problems and show howthis approach can be useful in studying both fixed-parameter tractability andintractability. For example, we establish the equivalence between themodel-checking for existential first-order logic, the homomorphism problem forrelational structures, and the substructure isomorphism problem. Our maintractability result shows that model-checking for first-order formulas isfixed-parameter tractable when restricted to a class of input structures withan excluded minor. On the intractability side, for every t >= 0 we prove anequivalence between model-checking for first-order formulas with t quantifieralternations and the parameterized halting problem for alternating Turingmachines with t alternations. We discuss the close connection between thisalternation hierarchy and Downey and Fellows' W-hierarchy.  On a more abstract level, we consider two forms of definability, called Fagindefinability and slicewise definability, that are appropriate for describingparameterized problems. We give a characterization of the class FPT of allfixed-parameter tractable problems in terms of slicewise definability in finitevariable least fixed-point logic, which is reminiscent of the Immerman-VardiTheorem characterizing the class PTIME in terms of definability in leastfixed-point logic."}
1.49e+03 pattern= 33 task=  0 (u=25554)  {'docs': u"With the advent of wide security platforms able to express simultaneously allthe policies comprising an organization's global security policy, the problemof inconsistencies within security policies become harder and more relevant.  We have defined a tool based on the CHR language which is able to detectseveral types of inconsistencies within and between security policies and otherspecifications, namely workflow specifications.  Although the problem of security conflicts has been addressed by severalauthors, to our knowledge none has addressed the general problem of securityinconsistencies, on its several definitions and target specifications."}
1.49e+03 pattern=  4 task=  0 (u=25558)  {'docs': u'We consider worst case time bounds for NP-complete problems including 3-SAT,3-coloring, 3-edge-coloring, and 3-list-coloring. Our algorithms are based on aconstraint satisfaction (CSP) formulation of these problems. 3-SAT isequivalent to (2,3)-CSP while the other problems above are special cases of(3,2)-CSP; there is also a natural duality transformation from (a,b)-CSP to(b,a)-CSP. We give a fast algorithm for (3,2)-CSP and use it to improve thetime bounds for solving the other problems listed above. Our techniques involvea mixture of Davis-Putnam-style backtracking with more sophisticated matchingand network flow based ideas.'}
1.12e+03 pattern=  1 task=  0 (u=27091)  {'docs': u'This paper describes a case study conducted in collaboration with Nortel todemonstrate the feasibility of applying formal modeling techniques totelecommunication systems. A formal description language, SDL, was chosen byour qualitative CASE tool evaluation to model a multimedia-messaging systemdescribed by an 80-page natural language specification. Our model was used toidentify errors in the software requirements document and to derive testsuites, shadowing the existing development process and keeping track of avariety of productivity data.'}
1.44e+03 pattern= 12 task=  0 (u=27803)  {'docs': u'Existing procedures for model validation have been deemed inadequate for manyengineering systems. The reason of this inadequacy is due to the high degree ofcomplexity of the mechanisms that govern these systems. It is proposed in thispaper to shift the attention from modeling the engineering system itself tomodeling the uncertainty that underlies its behavior. A mathematical frameworkfor modeling the uncertainty in complex engineering systems is developed. Thisframework uses the results of computational learning theory. It is based on thepremise that a system model is a learning machine.'}
1.39e+03 pattern= 28 task=  0 (u=30960)  {'docs': u"The possibility of translating logic programs into functional ones has longbeen a subject of investigation. Common to the many approaches is that theoriginal logic program, in order to be translated, needs to be well-moded andthis has led to the common understanding that these programs can be consideredto be the ``functional part'' of logic programs. As a consequence of this ithas become widely accepted that ``complex'' logical variables, the possibilityof a dynamic selection rule, and general properties of non-well-moded programsare exclusive features of logic programs. This is not quite true, as some ofthese features are naturally found in lazy functional languages. We readdressthe old question of what features are exclusive to the logic programmingparadigm by defining a simple translation applicable to a wider range of logicprograms, and demonstrate that the current circumscription is unreasonablyrestrictive."}
1.11e+03 pattern= 15 task=  0 (u=33899)  {'docs': u'Previous work in the context of natural language querying of temporaldatabases has established a method to map automatically from a large subset ofEnglish time-related questions to suitable expressions of a temporal logic-likelanguage, called TOP. An algorithm to translate from TOP to the TSQL2 temporaldatabase language has also been defined. This paper shows how TOP expressionscould be translated into a simpler logic-like language, called BOT. BOT is veryclose to traditional first-order predicate logic (FOPL), and hence existingmethods to manipulate FOPL expressions can be exploited to interface totime-sensitive applications other than TSQL2 databases, maintaining theexisting English-to-TOP mapping.'}
1.12e+03 pattern=  3 task=  0 (u=33902)  {'docs': u'The article surveys a little of the history of the technology, sets out themain current theoretical approaches in brief, and discusses the on-goingopposition between theoretical and empirical approaches. It illustrates thesituation with some discussion of CONVERSE, a system that won the Loebner prizein 1997 and which displays features of both approaches.'}
1.49e+03 pattern=  4 task=  0 (u=35104)  {'docs': u'The allocation of scarce spectral resources to support as many userapplications as possible while maintaining reasonable quality of service is afundamental problem in wireless communication. We argue that the problem isbest formulated in terms of decision theory. We propose a scheme that takesdecision-theoretic concerns (like preferences) into account and discuss thedifficulties and subtleties involved in applying standard techniques from thetheory of Markov Decision Processes (MDPs) in constructing an algorithm that isdecision-theoretically optimal. As an example of the proposed framework, weconstruct such an algorithm under some simplifying assumptions. Additionally,we present analysis and simulation results that show that our algorithm meetsits design goals. Finally, we investigate how far from optimal one well-knownheuristic is. The main contribution of our results is in providing insight andguidance for the design of near-optimal admission-control policies.'}
1.21e+03 pattern= 14 task=  0 (u=39114)  {'docs': u"We show that, contrary to common belief, Dijkstra's self-stabilizing mutualexclusion algorithm on a ring [Dij74,Dij82] also stabilizes when the number ofstates per node is one less than the number of nodes on the ring."}
1.43e+03 pattern= 14 task=  0 (u=43393)  {'docs': u'The paper has established and verified the theory prevailing widely amongimage and pattern recognition specialists that the bottom-up indirect regionalmatching process is the more stable and the more robust than the globalmatching process against concentrated types of noise represented by clutter,outlier or occlusion in the imagery. We have demonstrated this by analyzing theeffect of concentrated noise on a typical decision making process of asimplified two candidate voting model where our theorem establishes the lowerbounds to a critical breakdown point of election (or decision) result by thebottom-up matching process are greater than the exact bound of the globalmatching process implying that the former regional process is capable ofaccommodating a higher level of noise than the latter global process before theresult of decision overturns. We present a convincing experimental verificationsupporting not only the theory by a white-black flag recognition problem in thepresence of localized noise but also the validity of the conjecture by a facialrecognition problem that the theorem remains valid for other decision makingprocesses involving an important dimension-reducing transform such as principalcomponent analysis or a Gabor transform.'}
  248 pattern=  2 task=  0 (u=44645)  {'docs': u'This thesis addresses automatic lexical error recovery and tokenization ofcorrupt text input. We propose a technique that can automatically correctmisspellings, segmentation errors and real-word errors in a unified frameworkthat uses both a model of language production and a model of the typingbehavior, and which makes tokenization part of the recovery process.  The typing process is modeled as a noisy channel where Hidden Markov Modelsare used to model the channel characteristics. Weak statistical language modelsare used to predict what sentences are likely to be transmitted through thechannel. These components are held together in the Token Passing frameworkwhich provides the desired tight coupling between orthographic pattern matchingand linguistic expectation.  The system, CTR (Connected Text Recognition), has been tested on two corporaderived from two different applications, a natural language dialogue system anda transcription typing scenario. Experiments show that CTR can automaticallycorrect a considerable portion of the errors in the test sets withoutintroducing too much noise. The segmentation error correction rate is virtuallyfaultless.'}
1.12e+03 pattern= 10 task=  0 (u=44711)  {'docs': u"For over a decade, researchers in formal methods tried to create formalismsthat permit natural specification of systems and allow mathematical reasoningabout their correctness. The availability of fully-automated reasoning toolsenables more non-specialists to use formal methods effectively --- theirresponsibility reduces to just specifying the model and expressing the desiredproperties. Thus, it is essential that these properties be represented in alanguage that is easy to use and sufficiently expressive.  Linear-time temporal logic is a formalism that has been extensively used byresearchers for specifying properties of systems. When such properties areclosed under stuttering, i.e. their interpretation is not modified bytransitions that leave the system in the same state, verification tools canutilize a partial-order reduction technique to reduce the size of the model andthus analyze larger systems. If LTL formulas do not contain the ``next''operator, the formulas are closed under stuttering, but the resulting languageis not expressive enough to capture many important properties, e.g., propertiesinvolving events. Determining if an arbitrary LTL formula is closed understuttering is hard --- it has been proven to be PSPACE-complete.  In this paper we relax the restriction on LTL that guarantees closure understuttering, introduce the notion of edges in the context of LTL, and providetheorems that enable syntactic reasoning about closure under stuttering of LTLformulas."}
1.24e+03 pattern= 12 task=  0 (u=46358)  {'docs': u"Agents are small programs that autonomously take actions based on changes intheir environment or ``state.'' Over the last few years, there have been anincreasing number of efforts to build agents that can interact and/orcollaborate with other agents. In one of these efforts, Eiter, Subrahmanian amdPick (AIJ, 108(1-2), pages 179-255) have shown how agents may be built on topof legacy code. However, their framework assumes that agent states arecompletely determined, and there is no uncertainty in an agent's state. Thus,their framework allows an agent developer to specify how his agents will reactwhen the agent is 100% sure about what is true/false in the world state. Inthis paper, we propose the concept of a \\emph{probabilistic agent program} andshow how, given an arbitrary program written in any imperative language, we maybuild a declarative ``probabilistic'' agent program on top of it which supportsdecision making in the presence of uncertainty. We provide two alternativesemantics for probabilistic agent programs. We show that the second semantics,though more epistemically appealing, is more complex to compute. We providesound and complete algorithms to compute the semantics of \\emph{positive} agentprograms."}
1.25e+03 pattern= 12 task=  0 (u=46577)  {'docs': u'The influence of time-dependent fitnesses on the infinite population dynamicsof simple genetic algorithms (without crossover) is analyzed. Based on generalarguments, a schematic phase diagram is constructed that allows one tocharacterize the asymptotic states in dependence on the mutation rate and thetime scale of changes. Furthermore, the notion of regular changes is raised forwhich the population can be shown to converge towards a generalizedquasispecies. Based on this, error thresholds and an optimal mutation rate areapproximately calculated for a generational genetic algorithm with a movingneedle-in-the-haystack landscape. The so found phase diagram is fullyconsistent with our general considerations.'}
1.23e+03 pattern= 11 task=  0 (u=46924)  {'docs': u"The logic of equality with uninterpreted functions (EUF) provides a means ofabstracting the manipulation of data by a processor when verifying thecorrectness of its control logic. By reducing formulas in this logic topropositional formulas, we can apply Boolean methods such as Ordered BinaryDecision Diagrams (BDDs) and Boolean satisfiability checkers to perform theverification.  We can exploit characteristics of the formulas describing the verificationconditions to greatly simplify the propositional formulas generated. Inparticular, we exploit the property that many equations appear only in positiveform. We can therefore reduce the set of interpretations of the functionsymbols that must be considered to prove that a formula is universally valid tothose that are ``maximally diverse.''  We present experimental results demonstrating the efficiency of this approachwhen verifying pipelined processors using the method proposed by Burch andDill."}
1.08e+03 pattern= 11 task=  0 (u=47511)  {'docs': u'We recently presented a methodology for quantitatively reducing the risk andcost of executing electronic transactions in a bursty network environment suchas the Internet. In the language of portfolio theory, time to complete atransaction and its variance replace the expected return and risk associatedwith a security, whereas restart times replace combinations of securities.While such a strategy works well with single users, the question remains as toits usefulness when used by many. By using mean field arguments and agent-basedsimulations, we determine that a restart strategy remains advantageous even ifeverybody uses it.'}
1.1e+03 pattern= 13 task=  0 (u=48199)  {'docs': u'String transductions that are definable in monadic second-order (mso) logic(without the use of parameters) are exactly those realized by deterministictwo-way finite state transducers. Nondeterministic mso definable stringtransductions (i.e., those definable with the use of parameters) correspond tocompositions of two nondeterministic two-way finite state transducers that havethe finite visit property. Both families of mso definable string transductionsare characterized in terms of Hennie machines, i.e., two-way finite statetransducers with the finite visit property that are allowed to rewrite theirinput tape.'}
1.38e+03 pattern= 10 task=  0 (u=49281)  {'docs': u'Logical frameworks based on intuitionistic or linear logics with higher-typequantification have been successfully used to give high-level, modular, andformal specifications of many important judgments in the area of programminglanguages and inference systems. Given such specifications, it is natural toconsider proving properties about the specified systems in the framework: forexample, given the specification of evaluation for a functional programminglanguage, prove that the language is deterministic or that evaluation preservestypes. One challenge in developing a framework for such reasoning is thathigher-order abstract syntax (HOAS), an elegant and declarative treatment ofobject-level abstraction and substitution, is difficult to treat in proofsinvolving induction. In this paper, we present a meta-logic that can be used toreason about judgments coded using HOAS; this meta-logic is an extension of asimple intuitionistic logic that admits higher-order quantification over simplytyped lambda-terms (key ingredients for HOAS) as well as induction and a notionof definition. We explore the difficulties of formal meta-theoretic analysis ofHOAS encodings by considering encodings of intuitionistic and linear logics,and formally derive the admissibility of cut for important subsets of theselogics. We then propose an approach to avoid the apparent tradeoff between thebenefits of higher-order abstract syntax and the ability to analyze theresulting encodings. We illustrate this approach through examples involving thesimple functional and imperative programming languages PCF and PCF:=. Weformally derive such properties as unicity of typing, subject reduction,determinacy of evaluation, and the equivalence of transition semantics andnatural semantics presentations of evaluation.'}
 1.63 pattern=  0 task=  0 (u=50007)  {'docs': u'In this paper, a hierarchical context definition is added to an existingclustering algorithm in order to increase its robustness. The resultingalgorithm, which clusters contexts and events separately, is used to experimentwith different ways of defining the context a language model takes intoaccount. The contexts range from standard bigram and trigram contexts to partof speech five-grams. Although none of the models can compete directly with abackoff trigram, they give up to 9\\% improvement in perplexity wheninterpolated with a trigram. Moreover, the modified version of the algorithmleads to a performance increase over the original version of up to 12\\%.'}
1.5e+03 pattern= 18 task=  0 (u=50045)  {'docs': u"In this paper Schapire and Singer's AdaBoost.MH boosting algorithm is appliedto the Word Sense Disambiguation (WSD) problem. Initial experiments on a set of15 selected polysemous words show that the boosting approach surpasses NaiveBayes and Exemplar-based approaches, which represent state-of-the-art accuracyon supervised WSD. In order to make boosting practical for a real learningdomain of thousands of words, several ways of accelerating the algorithm byreducing the feature space are studied. The best variant, which we callLazyBoosting, is tested on the largest sense-tagged corpus available containing192,800 examples of the 191 most frequent and ambiguous English words. Again,boosting compares favourably to the other benchmark algorithms."}
1.5e+03 pattern= 21 task=  0 (u=54429)  {'docs': u'Constraint Handling Rules (CHR) have provided a realistic solution to anover-arching problem in many fields that deal with constraint logicprogramming: how to combine recursive functions or relations with constraintswhile avoiding non-termination problems. This paper focuses on some otherbenefits that CHR, specifically their implementation in SICStus Prolog, haveprovided to computational linguists working on grammar design tools. CHR rulesare applied by means of a subsumption check and this check is made only whentheir variables are instantiated or bound. The former functionality is at bestdifficult to simulate using more primitive coroutining statements such asSICStus when/2, and the latter simply did not exist in any form before CHR.  For the sake of providing a case study in how these can be applied to grammardevelopment, we consider the Attribute Logic Engine (ALE), a Prologpreprocessor for logic programming with typed feature structures, and itsextension to a complete grammar development system for Head-driven PhraseStructure Grammar (HPSG), a popular constraint-based linguistic theory thatuses typed feature structures. In this context, CHR can be used not only toextend the constraint language of feature structure descriptions to includerelations in a declarative way, but also to provide support for constraintswith complex antecedents and constraints on the co-occurrence of feature valuesthat are necessary to interpret the type system of HPSG properly.'}
1.26e+03 pattern= 24 task=  0 (u=56192)  {'docs': u'We survey recent developments in the study of (worst-case) one-way functionshaving strong algebraic and security properties. According to [RS93], this lineof research was initiated in 1984 by Rivest and Sherman who designed two-partysecret-key agreement protocols that use strongly noninvertible, total,associative one-way functions as their key building blocks. If commutativity isadded as an ingredient, these protocols can be used by more than two parties,as noted by Rabi and Sherman [RS93] who also developed digital signatureprotocols that are based on such enhanced one-way functions.  Until recently, it was an open question whether one-way functions having thealgebraic and security properties that these protocols require could be createdfrom any given one-way function. Recently, Hemaspaandra and Rothe [HR99]resolved this open issue in the affirmative, by showing that one-way functionsexist if and only if strong, total, commutative, associative one-way functionsexist.  We discuss this result, and the work of Rabi, Rivest, and Sherman, and recentwork of Homan [Hom99] that makes progress on related issues.'}
1.03e+03 pattern=  5 task=  0 (u=56677)  {'docs': u"We present a new approach to the simulation and analysis of immune systembehavior. The simulations that can be done with our software package calledSIMMUNE are based on immunological data that describe the behavior of immunesystem agents (cells, molecules) on a microscopial (i.e. agent-agentinteraction) scale by defining cellular stimulus-response mechanisms. Since thebehavior of the agents in SIMMUNE can be very flexibly configured, itsapplication is not limited to immune system simulations. We outline theprinciples of SIMMUNE's multiscale analysis of emergent structure within thesimulated immune system that allow the identification of immunological contextsusing minimal a priori assumptions about the higher level organization of theimmune system."}
1.02e+03 pattern= 10 task=  0 (u=56679)  {'docs': u'We present an open architecture for just-in-time code generation and dynamiccode optimization that is flexible, customizable, and extensible. Whileprevious research has primarily investigated functional aspects of such asystem, architectural aspects have so far remained unexplored. In this paper,we argue that these properties are important to generate optimal code for avariety of hardware architectures and different processor generations withinprocessor families. These properties are also important to make system-levelcode generation useful in practice.'}
1.31e+03 pattern= 27 task=  0 (u=57160)  {'docs': u'The multiplicative Newton-like method developed by the author et al. isextended to the situation where the dynamics is restricted to the orthogonalgroup. A general framework is constructed without specifying the cost function.Though the restriction to the orthogonal groups makes the problem somewhatcomplicated, an explicit expression for the amount of individual jumps isobtained. This algorithm is exactly second-order-convergent. The globalinstability inherent in the Newton method is remedied by aLevenberg-Marquardt-type variation. The method thus constructed can readily beapplied to the independent component analysis. Its remarkable performance isillustrated by a numerical simulation.'}
  862 pattern=  6 task=  0 (u=57910)  {'docs': u"The Abstract Syntax Description Language (ASDL) is a language for specifyingthe tree data structures often found in compiler intermediate representations.The ASDL generator reads an ASDL specification and generates code to construct,read, and write instances of the trees specified. Using ASDL permits a compilerto be decomposed into semi-independent components that communicate by readingand writing trees. Each component can be written in a different language,because the ASDL generator can emit code in several languages, and the fileswritten by ASDL-generated code are machine- and language-independent. ASDL ispart of the National Compiler Infrastructure project, which seeks to reducedramatically the overhead of computer systems research by making it much easierto build high-quality compilers. This paper describes dividing lcc, a widelyused retargetable C compiler, into two components that communicate via treesdefined in ASDL. As the first use of ASDL in a `real' compiler, this experiencereveals much about the effort required to retrofit an existing compiler to useASDL, the overheads involved, and the strengths and weaknesses of ASDL itselfand, secondarily, of lcc."}
1.15e+03 pattern= 14 task=  0 (u=61504)  {'docs': u'One way of suggesting that an NP problem may not be NP-complete is to showthat it is in the class UP. We suggest an analogous new approach---weaker instrength of evidence but more broadly applicable---to suggesting thatconcrete~NP problems are not NP-complete. In particular we introduce the classEP, the subclass of NP consisting of those languages accepted by NP machinesthat when they accept always have a number of accepting paths that is a powerof two. Since if any NP-complete set is in EP then all NP sets are in EP, itfollows---with whatever degree of strength one believes that EP differs fromNP---that membership in EP can be viewed as evidence that a problem is notNP-complete.  We show that the negation equivalence problem for OBDDs (ordered binarydecision diagrams) and the interchange equivalence problem for 2-dags are inEP. We also show that for boolean negation the equivalence problem is inEP^{NP}, thus tightening the existing NP^{NP} upper bound. We show that FewP,bounded ambiguity polynomial time, is contained in EP, a result that is notknown to follow from the previous SPP upper bound. For the three problems andclasses just mentioned with regard to EP, no proof of membership/containment inUP is known, and for the problem just mentioned with regard to EP^{NP}, noproof of membership in UP^{NP} is known. Thus, EP is indeed a tool that givesevidence against NP-completeness in natural cases where UP cannot currently beapplied.'}
1.09e+03 pattern= 12 task=  0 (u=63393)  {'docs': u'Bestvina and Handel have found an effective algorithm that determines whethera given homeomorphism of an orientable, possibly punctured surface ispseudo-Anosov. We present a software package in Java that realizes thisalgorithm for surfaces with one puncture. Moreover, the package allows the userto define homeomorphisms in terms of Dehn twists, and in the pseudo-Anosov caseit generates images of train tracks in the sense of Bestvina-Handel.'}