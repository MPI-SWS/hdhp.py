1.69e+03 pattern=  0 task=  0 (u=78)  {'docs': u'We define five increasingly comprehensive classes of infinite-state systems,called STS1--5, whose state spaces have finitary structure. For four of theseclasses, we provide examples from hybrid systems.'}
2.91e+03 pattern=  0 task=  0 (u=132)  {'docs': u'We describe a scheme for moving living code between a set of distributedprocesses coordinated with unification based Linda operations, and itsapplication to building a comprehensive Logic programming based Internetprogramming framework. Mobile threads are implemented by capturing first ordercontinuations in a compact data structure sent over the network. Code isfetched lazily from its original base turned into a server as the continuationexecutes at the remote site. Our code migration techniques, in combination witha dynamic recompilation scheme, ensure that heavily used code moves up smoothlyon a speed hierarchy while volatile dynamic code is kept in a quickly updatableform. Among the examples, we describe how to build programmable client andserver components (Web servers, in particular) and mobile agents.'}
3.23e+03 pattern=  0 task=  0 (u=146)  {'docs': u'In this paper, the average coset weight distribution (ACWD) of structuredensembles of LDPC (Low-density Parity-Check) matrix, which is called combinedensembles, is discussed. A combined ensemble is composed of a set of simplerensembles such as a regular bipartite ensemble. Two classes of combinedensembles have prime importance; a stacked ensemble and a concatenatedensemble, which consists of set of stacked matrices and concatenated matrices,respectively. The ACWD formulas of these ensembles is shown in this paper. Suchformulas are key tools to evaluate the ACWD of a complex combined ensemble.  From the ACWD of an ensemble, we can obtain some detailed properties of acode (e.g., weight of coset leaders) which is not available from an averageweight distribution. Moreover, it is shown that the analysis based on the ACWDis indispensable to evaluate the average weight distribution of some classes ofcombined ensembles.'}
2.75e+03 pattern=  0 task=  0 (u=184)  {'docs': u"We propose a framework for modeling uncertainty where both belief and doubtcan be given independent, first-class status. We adopt probability theory asthe mathematical formalism for manipulating uncertainty. An agent can expressthe uncertainty in her knowledge about a piece of information in the form of aconfidence level, consisting of a pair of intervals of probability, one foreach of her belief and doubt. The space of confidence levels naturally leads tothe notion of a trilattice, similar in spirit to Fitting's bilattices.Intuitively, thep oints in such a trilattice can be ordered according to truth,information, or precision. We develop a framework for probabilistic deductivedatabases by associating confidence levels with the facts and rules of aclassical deductive database. While the trilattice structure offers a varietyof choices for defining the semantics of probabilistic deductive databases, ourchoice of semantics is based on the truth-ordering, which we find to be closestto the classical framework for deductive databases. In addition to proposing adeclarative semantics based on valuations and an equivalent semantics based onfixpoint theory, we also propose a proof procedure and prove it sound andcomplete. We show that while classical Datalog query programs have a polynomialtime data complexity, certain query programs in the probabilistic deductivedatabase framework do not even terminate on some input databases. We identify alarge natural class of query programs of practical interest in our framework,and show that programs in this class possess polynomial time data complexity,i.e., not only do they terminate on every input database, they are guaranteedto do so in a number of steps polynomial in the input database size."}
2.1e+03 pattern=  0 task=  0 (u=186)  {'docs': u'Prioritized default reasoning has illustrated its rich expressiveness andflexibility in knowledge representation and reasoning. However, many importantaspects of prioritized default reasoning have yet to be thoroughly explored. Inthis paper, we investigate two properties of prioritized logic programs in thecontext of answer set semantics. Specifically, we reveal a close relationshipbetween mutual defeasibility and uniqueness of the answer set for a prioritizedlogic program. We then explore how the splitting technique for extended logicprograms can be extended to prioritized logic programs. We prove splittingtheorems that can be used to simplify the evaluation of a prioritized logicprogram under certain conditions.'}
2.19e+03 pattern=  0 task=  1 (u=186)  {'docs': u'Representing defeasibility is an important issue in common sense reasoning.In reasoning about action and change, this issue becomes more difficult becausedomain and action related defeasible information may conflict with generalinertia rules. Furthermore, different types of defeasible information may alsointerfere with each other during the reasoning. In this paper, we develop aprioritized logic programming approach to handle defeasibilities in reasoningabout action. In particular, we propose three action languages {\\cal AT}^{0},{\\cal AT}^{1} and {\\cal AT}^{2} which handle three types of defeasibilities inaction domains named defeasible constraints, defeasible observations andactions with defeasible and abnormal effects respectively. Each language with ahigher superscript can be viewed as an extension of the language with a lowersuperscript. These action languages inherit the simple syntax of {\\cal A}language but their semantics is developed in terms of transition systems wheretransition functions are defined based on prioritized logic programs. Byillustrating various examples, we show that our approach eventually provides apowerful mechanism to handle various defeasibilities in temporal prediction andpostdiction. We also investigate semantic properties of these three actionlanguages and characterize classes of action domains that present moredesirable solutions in reasoning about action within the underlying actionlanguages.'}
2.27e+03 pattern=  0 task=  0 (u=205)  {'docs': u'We study the graph coloring problem over random graphs of finite averageconnectivity $c$. Given a number $q$ of available colors, we find that graphswith low connectivity admit almost always a proper coloring whereas graphs withhigh connectivity are uncolorable. Depending on $q$, we find the precise valueof the critical average connectivity $c_q$. Moreover, we show that below $c_q$there exist a clustering phase $c\\in [c_d,c_q]$ in which ground statesspontaneously divide into an exponential number of clusters and where theproliferation of metastable states is responsible for the onset of complexityin local search algorithms.'}
3e+03 pattern=  0 task=  0 (u=217)  {'docs': u"Boltzmann selection is an important selection mechanism in evolutionaryalgorithms as it has theoretical properties which help in theoretical analysis.However, Boltzmann selection is not used in practice because a good annealingschedule for the `inverse temperature' parameter is lacking. In this paper wepropose a Cauchy annealing schedule for Boltzmann selection scheme based on ahypothesis that selection-strength should increase as evolutionary process goeson and distance between two selection strengths should decrease for the processto converge. To formalize these aspects, we develop formalism for selectionmechanisms using fitness distributions and give an appropriate measure forselection-strength. In this paper, we prove an important result, by which wederive an annealing schedule called Cauchy annealing schedule. We demonstratethe novelty of proposed annealing schedule using simulations in the frameworkof genetic algorithms."}
1.02e+03 pattern=  3 task=  0 (u=307)  {'docs': u'Corrected versions of the numerically invariant expressions for the affineand Euclidean signature of a planar curve proposed by E.Calabi et. al arepresented. The new formulas are valid for fine but otherwise arbitrarypartitions of the curve. We also give numerically invariant expressions for thefour differential invariants parametrizing the three dimensional version of theEuclidean signature curve, namely the curvature, the torsion and theirderivatives with respect to arc length.'}
2.7e+03 pattern=  0 task=  0 (u=330)  {'docs': u'We present a method that allows for the discovery of communities withingraphs of arbitrary size in times that scale linearly with their size. Thismethod avoids edge cutting and is based on notions of voltage drops acrossnetworks that are both intuitive and easy to solve regardless of the complexityof the graph involved. We additionally show how this algorithm allows for theswift discovery of the community surrounding a given node without having toextract all the communities out of a graph.'}
2.86e+03 pattern=  0 task=  0 (u=353)  {'docs': u'We consider the problem of coloring Erdos-Renyi and regular random graphs offinite connectivity using q colors. It has been studied so far using the cavityapproach within the so-called one-step replica symmetry breaking (1RSB) ansatz.We derive a general criterion for the validity of this ansatz and, applying itto the ground state, we provide evidence that the 1RSB solution gives exactthreshold values c_q for the q-COL/UNCOL phase transition. We also study theasymptotic thresholds for q >> 1 finding c_q = 2qlog(q)-log(q)-1+o(1) inperfect agreement with rigorous mathematical bounds, as well as the nature ofexcited states, and give a global phase diagram of the problem.'}
1.85e+03 pattern=  0 task=  0 (u=376)  {'docs': u'We present the first explicit, and currently simplest, randomized algorithmfor 2-process wait-free test-and-set. It is implemented with two 4-valuedsingle writer single reader atomic variables. A test-and-set takes at most 11expected elementary steps, while a reset takes exactly 1 elementary step. Basedon a finite-state analysis, the proofs of correctness and expected length arecompressed into one table.'}
3.18e+03 pattern=  0 task=  1 (u=376)  {'docs': u"Rush Hour Logic was introduced in [Flake&Baum99] as a model of computationinspired by the ``Rush Hour'' toy puzzle, in which cars can move horizontallyor vertically within a parking lot. The authors show how the model supportspolynomial space computation, using certain car configurations as buildingblocks to construct boolean circuits for a cpu and memory. They consider theuse of cars of length 3 crucial to their construction, and conjecture that carsof size 2 only, which we'll call `Size 2 Rush Hour', do not support polynomialspace computation. We settle this conjecture by showing that the requiredbuilding blocks are constructible in Size 2 Rush Hour. Furthermore, we considerUnit Rush Hour, which was hitherto believed to be trivial, show its relation tomaze puzzles, and provide empirical support for its hardness."}
2e+03 pattern=  0 task=  0 (u=377)  {'docs': u"The information in an individual finite object (like a binary string) iscommonly measured by its Kolmogorov complexity. One can divide that informationinto two parts: the information accounting for the useful regularity present inthe object and the information accounting for the remaining accidentalinformation. There can be several ways (model classes) in which the regularityis expressed. Kolmogorov has proposed the model class of finite sets,generalized later to computable probability mass functions. The resultingtheory, known as Algorithmic Statistics, analyzes the algorithmic sufficientstatistic when the statistic is restricted to the given model class. However,the most general way to proceed is perhaps to express the useful information asa recursive function. The resulting measure has been called the``sophistication'' of the object. We develop the theory of recursive functionsstatistic, the maximum and minimum value, the existence of absolutelynonstochastic objects (that have maximal sophistication--all the information inthem is meaningful and there is no residual randomness), determine its relationwith the more restricted model classes of finite sets, and computableprobability distributions, in particular with respect to the algorithmic(Kolmogorov) minimal sufficient statistic, the relation to the halting problemand further algorithmic properties."}
1.85e+03 pattern=  0 task=  0 (u=383)  {'docs': u'Termination of logic programs depends critically on the selection rule, i.e.the rule that determines which atom is selected in each resolution step. Inthis article, we classify programs (and queries) according to the selectionrules for which they terminate. This is a survey and unified view on differentapproaches in the literature. For each class, we present a sufficient, for mostclasses even necessary, criterion for determining that a program is in thatclass. We study six classes: a program strongly terminates if it terminates forall selection rules; a program input terminates if it terminates for selectionrules which only select atoms that are sufficiently instantiated in their inputpositions, so that these arguments do not get instantiated any further by theunification; a program local delay terminates if it terminates for localselection rules which only select atoms that are bounded w.r.t. an appropriatelevel mapping; a program left-terminates if it terminates for the usualleft-to-right selection rule; a program exists-terminates if there exists aselection rule for which it terminates; finally, a program has boundednondeterminism if it only has finitely many refutations. We propose asemantics-preserving transformation from programs with bounded nondeterminisminto strongly terminating programs. Moreover, by unifying different formalismsand making appropriate assumptions, we are able to establish a formal hierarchybetween the different classes.'}
1.8e+03 pattern=  0 task=  0 (u=385)  {'docs': u'Analysis of (partial) groundness is an important application of abstractinterpretation. There are several proposals for improving the precision of suchan analysis by exploiting type information, icluding our own work with Hill andKing, where we had shown how the information present in the type declarationsof a program can be used to characterise the degree of instantiation of a termin a precise and yet inherently finite way. This approach worked forpolymorphically typed programs as in Goedel or HAL. Here, we recast thisapproach following works by Codish, Lagoon and Stuckey. To formalise whichproperties of terms we want to characterise, we use labelling functions, whichare functions that extract subterms from a term along certain paths. Anabstract term collects the results of all labelling functions of a term. Forthe analysis, programs are executed on abstract terms instead of the concreteones, and usual unification is replaced by unification modulo an equalitytheory which includes the well-known ACI-theory. Thus we generalise the worksby Codish, Lagoon and Stuckey w.r.t. the type systems considered and relate theworks among each other.'}
1.85e+03 pattern=  0 task=  0 (u=386)  {'docs': u'We present a new approach to termination analysis of logic programs. Theessence of the approach is that we make use of general orderings (instead oflevel mappings), like it is done in transformational approaches to logicprogram termination analysis, but we apply these orderings directly to thelogic program and not to the term-rewrite system obtained through sometransformation. We define some variants of acceptability, based on generalorderings, and show how they are equivalent to LD-termination. We develop ademand driven, constraint-based approach to verify theseacceptability-variants.  The advantage of the approach over standard acceptability is that in somecases, where complex level mappings are needed, fairly simple orderings may beeasily generated. The advantage over transformational approaches is that itavoids the transformation step all together.  {\\bf Keywords:} termination analysis, acceptability, orderings.'}
1.63e+03 pattern=  0 task=  0 (u=387)  {'docs': u'We present a new approach to termination analysis of logic programs. Theessence of the approach is that we make use of general term-orderings (insteadof level mappings), like it is done in transformational approaches to logicprogram termination analysis, but that we apply these orderings directly to thelogic program and not to the term-rewrite system obtained through sometransformation. We define some variants of acceptability, based on generalterm-orderings, and show how they are equivalent to LD-termination. We developa demand driven, constraint-based approach to verify theseacceptability-variants.  The advantage of the approach over standard acceptability is that in somecases, where complex level mappings are needed, fairly simple term-orderingsmay be easily generated. The advantage over transformational approaches is thatit avoids the transformation step all together.'}
1.85e+03 pattern=  0 task=  1 (u=387)  {'docs': u'We present a new approach to termination analysis of numerical computationsin logic programs. Traditional approaches fail to analyse them due to nonwell-foundedness of the integers. We present a technique that allows toovercome these difficulties. Our approach is based on transforming a program inway that allows integrating and extending techniques originally developed foranalysis of numerical computations in the framework of query-mapping pairs withthe well-known framework of acceptability. Such an integration not onlycontributes to the understanding of termination behaviour of numericalcomputations, but also allows to perform a correct analysis of suchcomputations automatically, thus, extending previous work on aconstraints-based approach to termination. In the last section of the paper wediscuss possible extensions of the technique, including incorporating generalterm orderings.'}
1.96e+03 pattern=  0 task=  2 (u=387)  {'docs': u'We present a new approach to termination analysis of numerical computationsin logic programs. Traditional approaches fail to analyse them due to nonwell-foundedness of the integers. We present a technique that allows overcomingthese difficulties. Our approach is based on transforming a program in a waythat allows integrating and extending techniques originally developed foranalysis of numerical computations in the framework of query-mapping pairs withthe well-known framework of acceptability. Such an integration not onlycontributes to the understanding of termination behaviour of numericalcomputations, but also allows us to perform a correct analysis of suchcomputations automatically, by extending previous work on a constraint-basedapproach to termination. Finally, we discuss possible extensions of thetechnique, including incorporating general term orderings.'}
1.96e+03 pattern=  0 task=  3 (u=387)  {'docs': u'The term {\\em meta-programming} refers to the ability of writing programsthat have other programs as data and exploit their semantics.  The aim of this paper is presenting a methodology allowing us to perform acorrect termination analysis for a broad class of practical meta-interpreters,including negation and performing different tasks during the execution. It isbased on combining the power of general orderings, used in proving terminationof term-rewrite systems and programs, and on the well-known acceptabilitycondition, used in proving termination of logic programs.  The methodology establishes a relationship between the ordering needed toprove termination of the interpreted program and the ordering needed to provetermination of the meta-interpreter together with this interpreted program. Ifsuch a relationship is established, termination of one of those impliestermination of the other one, i.e., the meta-interpreter preserves termination.  Among the meta-interpreters that are analysed correctly are a proof treesconstructing meta-interpreter, different kinds of tracers and reasoners.  To appear without appendix in Theory and Practice of Logic Programming.'}
2.42e+03 pattern=  0 task=  0 (u=389)  {'docs': u'Much work has been done on extending the well-founded semantics to generaldisjunctive logic programs and various approaches have been proposed. However,these semantics are different from each other and no consensus is reached aboutwhich semantics is the most intended. In this paper we look at disjunctivewell-founded reasoning from different angles. We show that there is anintuitive form of the well-founded reasoning in disjunctive logic programmingwhich can be characterized by slightly modifying some exisitng approaches todefining disjunctive well-founded semantics, including program transformations,argumentation, unfounded sets (and resolution-like procedure). We also providea bottom-up procedure for this semantics. The significance of our work is notonly in clarifying the relationship among different approaches, but also shedsome light on what is an intended well-founded semantics for disjunctive logicprograms.'}
2.87e+03 pattern=  0 task=  0 (u=499)  {'docs': u'Numerical analysis has no satisfactory method for the more realisticoptimization models. However, with constraint programming one can compute acover for the solution set to arbitrarily close approximation. Because the useof constraint propagation for composite arithmetic expressions iscomputationally expensive, consistency is computed with interval arithmetic. Inthis paper we present theorems that support, selective initialization, a simplemodification of constraint propagation that allows composite arithmeticexpressions to be handled efficiently.'}
1.54e+03 pattern=  0 task=  0 (u=502)  {'docs': u"We show that, in John Conway's board game Phutball (or Philosopher'sFootball), it is NP-complete to determine whether the current player has a movethat immediately wins the game. In contrast, the similar problems ofdetermining whether there is an immediately winning move in checkers, or a movethat kings a man, are both solvable in polynomial time."}
3.18e+03 pattern=  0 task=  1 (u=502)  {'docs': u'In this paper we extend the theory of bidimensionality to two families ofgraphs that do not exclude fixed minors: map graphs and power graphs. In bothcases we prove a polynomial relation between the treewidth of a graph in thefamily and the size of the largest grid minor. These bounds improve the runningtimes of a broad class of fixed-parameter algorithms. Our novel technique ofusing approximate max-min relations between treewidth and size of grid minorsis powerful, and we show how it can also be used, e.g., to prove a linearrelation between the treewidth of a bounded-genus graph and the treewidth ofits dual.'}
1.95e+03 pattern=  0 task=  0 (u=509)  {'docs': u'The early promises of DNA computing to deliver a massively parallelarchitecture well-suited to computationally hard problems have so far beenlargely unkept. Indeed, it is probably fair to say that only toy problems havebeen addressed experimentally. Recent experimental development on algorithmicself-assembly using DNA tiles seem to offer the most promising path toward apotentially useful application of the DNA computing concept. In this paper, weexplore new geometries for algorithmic self-assembly, departing from thosepreviously described in the literature. This enables us to carry outmathematical operations like binary multiplication or cyclic convolutionproduct. We then show how to use the latter operation to implement an attackagainst the well-known public-key crypto system NTRU.'}
1.17e+03 pattern=  0 task=  0 (u=511)  {'docs': u'We use circle-packing methods to generate quadrilateral meshes for polygonaldomains, with guaranteed bounds both on the quality and the number of elements.We show that these methods can generate meshes of several types: (1) theelements form the cells of a Voronoi diagram, (2) all elements have twoopposite right angles, (3) all elements are kites, or (4) all angles are atmost 120 degrees. In each case the total number of elements is O(n), where n isthe number of input vertices.'}
1.19e+03 pattern=  0 task=  1 (u=511)  {'docs': u'Here we present the results of the NSF-funded Workshop on ComputationalTopology, which met on June 11 and 12 in Miami Beach, Florida. This reportidentifies important problems involving both computation and topology.'}
1.3e+03 pattern=  0 task=  2 (u=511)  {'docs': u'The regression depth of a hyperplane with respect to a set of n points in R^dis the minimum number of points the hyperplane must pass through in a rotationto vertical. We generalize hyperplane regression depth to k-flats for any kbetween 0 and d-1. The k=0 case gives the classical notion of center points. Weprove that for any k and d, deep k-flats exist, that is, for any set of npoints there always exists a k-flat with depth at least a constant fraction ofn. As a consequence, we derive a linear-time (1+epsilon)-approximationalgorithm for the deepest flat.'}
1.58e+03 pattern=  0 task=  3 (u=511)  {'docs': u'We give algorithms for computing the regression depth of a k-flat for a setof n points in R^d. The running time is O(n^(d-2) + n log n) when 0 < k < d-1,faster than the best time bound for hyperplane regression or for data depth.'}
1.68e+03 pattern=  0 task=  4 (u=511)  {'docs': u'We give linear-time quasiconvex programming algorithms for finding a Moebiustransformation of a set of spheres in a unit ball or on the surface of a unitsphere that maximizes the minimum size of a transformed sphere. We can also usesimilar methods to maximize the minimum distance among a set of pairs of inputpoints. We apply these results to vertex separation and symmetry display inspherical graph drawing, viewpoint selection in hyperbolic browsing, elementsize control in conformal structured mesh generation, and brain flat mapping.'}
1.91e+03 pattern=  0 task=  5 (u=511)  {'docs': u'We define and examine flip operations for quadrilateral and hexahedralmeshes, similar to the flipping transformations previously used in triangularand tetrahedral mesh generation.'}
2.38e+03 pattern=  0 task=  6 (u=511)  {'docs': u'We consider the problem of finding a large color space that can be generatedby all units in multi-projector tiled display systems. Viewing the problemgeometrically as one of finding a large parallelepiped within the intersectionof multiple parallelepipeds, and using colorimetric principles to define avolume-based objective function for comparing feasible solutions, we develop analgorithm for finding the optimal gamut in time O(n^3), where n denotes thenumber of projectors in the system. We also discuss more efficient quasiconvexprogramming algorithms for alternative objective functions based on maximizingthe quality of the color space extrema.'}
  846 pattern=  0 task=  0 (u=512)  {'docs': u'We show that any polyhedron forming a topological ball with an even number ofquadrilateral sides can be partitioned into O(n) topological cubes, meetingface to face. The result generalizes to non-simply-connected polyhedrasatisfying an additional bipartiteness condition. The same techniques can alsobe used to reduce the geometric version of the hexahedral mesh generationproblem to a finite case analysis amenable to machine solution.'}
1.12e+03 pattern=  0 task=  1 (u=512)  {'docs': u'We introduce a class of "inverse parametric optimization" problems, in whichone is given both a parametric optimization problem and a desired optimalsolution; the task is to determine parameter values that lead to the givensolution. We describe algorithms for solving such problems for minimum spanningtrees, shortest paths, and other "optimal subgraph" problems, and discussapplications in multicast routing, vehicle path planning, resource allocation,and board game programming.'}
1.14e+03 pattern=  0 task=  2 (u=512)  {'docs': u'We describe simple linear time algorithms for coloring the squares ofbalanced and unbalanced quadtrees so that no two adjacent squares are given thesame color. If squares sharing sides are defined as adjacent, we color balancedquadtrees with three colors, and unbalanced quadtrees with four colors; theseresults are both tight, as some quadtrees require this many colors. If squaressharing corners are defined as adjacent, we color balanced or unbalancedquadtrees with six colors; for some quadtrees, at least five colors arerequired.'}
1.14e+03 pattern=  0 task=  3 (u=512)  {'docs': u'A fractal construction shows that, for any beta>0, the beta-skeleton of apoint set can have arbitrarily large dilation. In particular this applies tothe Gabriel graph.'}
1.25e+03 pattern=  0 task=  4 (u=512)  {'docs': u'We solve the subgraph isomorphism problem in planar graphs in linear time,for any pattern of constant size. Our results are based on a technique ofpartitioning the planar graph into pieces of small tree-width, and applyingdynamic programming within each piece. The same methods can be used to solveother planar graph problems including connectivity, diameter, girth, inducedsubgraph isomorphism, and shortest paths.'}
1.56e+03 pattern=  0 task=  5 (u=512)  {'docs': u'Social studies researchers use graphs to model group activities in socialnetworks. An important property in this context is the centrality of a vertex:the inverse of the average distance to each other vertex. We describe arandomized approximation algorithm for centrality in weighted graphs. Forgraphs exhibiting the small world phenomenon, our method estimates thecentrality of all vertices with high probability within a (1+epsilon) factor innear-linear time.'}
1.56e+03 pattern=  0 task=  6 (u=512)  {'docs': u'We consider worst case time bounds for NP-complete problems including 3-SAT,3-coloring, 3-edge-coloring, and 3-list-coloring. Our algorithms are based on aconstraint satisfaction (CSP) formulation of these problems; 3-SAT isequivalent to (2,3)-CSP while the other problems above are special cases of(3,2)-CSP. We give a fast algorithm for (3,2)-CSP and use it to improve thetime bounds for solving the other problems listed above. Our techniques involvea mixture of Davis-Putnam-style backtracking with more sophisticated matchingand network flow based ideas.'}
1.62e+03 pattern=  0 task=  7 (u=512)  {'docs': u'We show that, for any n-vertex graph G and integer parameter k, there are atmost 3^{4k-n}4^{n-3k} maximal independent sets I \\subset G with |I| <= k, andthat all such sets can be listed in time O(3^{4k-n} 4^{n-3k}). These bounds aretight when n/4 <= k <= n/3. As a consequence, we show how to compute the exactchromatic number of a graph in time O((4/3 + 3^{4/3}/4)^n) ~= 2.4150^n,improving a previous O((1+3^{1/3})^n) ~= 2.4422^n algorithm of Lawler (1976).'}
1.84e+03 pattern=  0 task=  8 (u=512)  {'docs': u'Any two polygons of equal area can be partitioned into congruent sets ofpolygonal pieces, and in many cases one can connect the pieces by flexiblehinges while still allowing the connected set to form both polygons. However itis open whether such a hinged dissection always exists. We solve a special caseof this problem, by showing that any asymmetric polygon always has a hingeddissection to its mirror image. Our dissection forms a chain of kite-shapedpieces, found by a circle-packing algorithm for quadrilateral mesh generation.A hinged mirror dissection of a polygon with n sides can be formed with O(n)kites in O(n log n) time.'}
1.94e+03 pattern=  0 task=  9 (u=512)  {'docs': u'We show that geometric thickness and book thickness are not asymptoticallyequivalent: for every t, there exists a graph with geometric thickness two andbook thickness >= t.'}
1.95e+03 pattern=  0 task= 10 (u=512)  {'docs': u'We define the min-min expectation selection problem (resp. max-minexpectation selection problem) to be that of selecting k out of n givendiscrete probability distributions, to minimize (resp. maximize) theexpectation of the minimum value resulting when independent random variablesare drawn from the selected distributions. We assume each distribution hasfinitely many atoms. Let d be the number of distinct values in the support ofthe distributions. We show that if d is a constant greater than 2, the min-minexpectation problem is NP-complete but admits a fully polynomial timeapproximation scheme. For d an arbitrary integer, it is NP-hard to approximatethe min-min expectation problem with any constant approximation factor. Themax-min expectation problem is polynomially solvable for constant d; we leaveopen its complexity for variable d. We also show similar results for binaryselection problems in which we must choose one distribution from each of npairs of distributions.'}
2.13e+03 pattern=  0 task= 11 (u=512)  {'docs': u'Power law distribution seems to be an important characteristic of web graphs.Several existing web graph models generate power law graphs by adding newvertices and non-uniform edge connectivities to existing graphs. Researchershave conjectured that preferential connectivity and incremental growth are bothrequired for the power law distribution. In this paper, we propose a differentweb graph model with power law distribution that does not require incrementalgrowth. We also provide a comparison of our model with several others in theirability to predict web graph clustering behavior.'}
2.15e+03 pattern=  0 task= 12 (u=512)  {'docs': u'We show that graph-theoretic thickness and geometric thickness are notasymptotically equivalent: for every t, there exists a graph with thicknessthree and geometric thickness >= t.'}
2.21e+03 pattern=  0 task= 13 (u=512)  {'docs': u'Falmagne recently introduced the concept of a medium, a combinatorial objectencompassing hyperplane arrangements, topological orderings, acyclicorientations, and many other familiar structures. We find efficient solutionsfor several algorithmic problems on media: finding short reset sequences,shortest paths, testing whether a medium has a closed orientation, and listingthe states of a medium given a black-box description.'}
2.24e+03 pattern=  0 task= 14 (u=512)  {'docs': u'We provide a data structure for maintaining an embedding of a graph on asurface (represented combinatorially by a permutation of edges around eachvertex) and computing generators of the fundamental group of the surface, inamortized time O(log n + log g(log log g)^3) per update on a surface of genusg; we can also test orientability of the surface in the same time, and maintainthe minimum and maximum spanning tree of the graph in time O(log n + log^4 g)per update. Our data structure allows edge insertion and deletion as well asthe dual operations; these operations may implicitly change the genus of theembedding surface. We apply similar ideas to improve the constant factor in aseparator theorem for low-genus graphs, and to find in linear time atree-decomposition of low-genus low-diameter graphs.'}
2.45e+03 pattern=  0 task= 15 (u=512)  {'docs': u'We show how to find a Hamiltonian cycle in a graph of degree at most threewith n vertices, in time O(2^{n/3}) ~= 1.260^n and linear space. Our algorithmcan find the minimum weight Hamiltonian cycle (traveling salesman problem), inthe same time bound. We can also count or list all Hamiltonian cycles in adegree three graph in time O(2^{3n/8}) ~= 1.297^n. We also solve the travelingsalesman problem in graphs of degree at most four, by randomized anddeterministic algorithms with runtime O((27/4)^{n/3}) ~= 1.890^n andO((27/4+epsilon)^{n/3}) respectively. Our algorithms allow the input to specifya set of forced edges which must be part of any generated cycle. Our cyclelisting algorithm shows that every degree three graph has O(2^{3n/8})Hamiltonian cycles; we also exhibit a family of graphs with 2^{n/3} Hamiltoniancycles per graph.'}
2.5e+03 pattern=  0 task= 16 (u=512)  {'docs': u'We consider a class of multivariate recurrences frequently arising in theworst case analysis of Davis-Putnam-style exponential time backtrackingalgorithms for NP-hard problems. We describe a technique for proving asymptoticupper bounds on these recurrences, by using a suitable weight function toreduce the problem to that of solving univariate linear recurrences; show howto use quasiconvex programming to determine the weight function yielding thesmallest upper bound; and prove that the resulting upper bounds are within apolynomial factor of the true asymptotics of the recurrence. We develop andimplement a multiple-gradient descent algorithm for the resulting quasiconvexprograms, using a real-number arithmetic package for guaranteed accuracy of thecomputed worst case time bounds.'}
2.59e+03 pattern=  0 task= 17 (u=512)  {'docs': u'We show how to test the bipartiteness of an intersection graph of n linesegments or simple polygons in the plane, or of balls in R^d, in time O(n logn). More generally we find subquadratic algorithms for connectivity andbipartiteness testing of intersection graphs of a broad class of geometricobjects. For unit balls in R^d, connectivity testing has equivalent randomizedcomplexity to construction of Euclidean minimum spanning trees, and hence isunlikely to be solved as efficiently as bipartiteness testing. For linesegments or planar disks, testing k-colorability of intersection graphs for k>2is NP-complete.'}
2.81e+03 pattern=  0 task= 18 (u=512)  {'docs': u'We describe a polynomial time algorithm for, given an undirected graph G,finding the minimum dimension d such that G may be isometrically embedded intothe d-dimensional integer lattice Z^d.'}
2.93e+03 pattern=  0 task= 19 (u=512)  {'docs': u'We describe algorithms for drawing media, systems of states, tokens andactions that have state transition graphs in the form of partial cubes. Ouralgorithms are based on two principles: embedding the state transition graph ina low-dimensional integer lattice and projecting the lattice onto the plane, ordrawing the medium as a planar graph with centrally symmetric faces.'}
2.96e+03 pattern=  0 task= 20 (u=512)  {'docs': u"We describe algorithms, based on Avis and Fukuda's reverse search paradigm,for listing all maximal independent sets in a sparse graph in polynomial timeand delay per output. For bounded degree graphs, our algorithms take constanttime per set generated; for minor-closed graph families, the time is O(n) perset, and for more general sparse graph families we achieve subquadratic timeper set. We also describe new data structures for maintaining a dynamic vertexset S in a sparse or minor-closed graph family, and querying the number ofvertices not dominated by S; for minor-closed graph families the time perupdate is constant, while it is sublinear for any sparse graph family. We canalso maintain a dynamic vertex set in an arbitrary m-edge graph and test theindependence of the maintained set in time O(sqrt m) per update. We use thedomination data structures as part of our enumeration algorithms."}
3.11e+03 pattern=  0 task= 21 (u=512)  {'docs': u'We define quasiconvex programming, a form of generalized linear programmingin which one seeks the point minimizing the pointwise maximum of a collectionof quasiconvex functions. We survey algorithms for solving quasiconvex programseither numerically or via generalizations of the dual simplex method fromlinear programming, and describe varied applications of this geometricoptimization technique in meshing, scientific computation, informationvisualization, automated algorithm analysis, and robust statistics.'}
3.27e+03 pattern=  0 task= 22 (u=512)  {'docs': u'We study practically efficient methods for performing combinatorial grouptesting. We present efficient non-adaptive and two-stage combinatorial grouptesting algorithms, which identify the at most d items out of a given set of nitems that are defective, using fewer tests for all practical set sizes. Forexample, our two-stage algorithm matches the information theoretic lower boundfor the number of tests in a combinatorial group testing regimen.'}
  844 pattern=  0 task=  0 (u=514)  {'docs': u'We study the problem of moving a vertex in an unstructured mesh oftriangular, quadrilateral, or tetrahedral elements to optimize the shapes ofadjacent elements. We show that many such problems can be solved in linear timeusing generalized linear programming. We also give efficient algorithms forsome mesh smoothing problems that do not fit into the generalized linearprogramming paradigm.'}
1.95e+03 pattern=  0 task=  0 (u=531)  {'docs': u'After the first treatments of quantum finite state automata by Moore andCrutchfield and by Kondacs and Watrous, a number of papers study the power ofquantum finite state automata and their variants. This paper introduces a modelof two-way quantum one-counter automata (2Q1CAs), combining the model oftwo-way quantum finite state automata (2QFAs) by Kondacs and Watrous and themodel of one-way quantum one-counter automata (1Q1CAs) by Kravtsev. We give thedefinition of 2Q1CAs with well-formedness conditions. It is proved that 2Q1CAsare at least as powerful as classical two-way deterministic one-counterautomata (2D1CAs), that is, every language L recognizable by 2D1CAs isrecognized by 2Q1CAs with no error. It is also shown that severalnon-context-free languages including {a^n b^{n^2}} and {a^n b^{2^n}} arerecognizable by 2Q1CAs with bounded error.'}
1.72e+03 pattern=  0 task=  0 (u=532)  {'docs': u'This paper gives the first formal treatment of a quantum analogue ofmulti-prover interactive proof systems. It is proved that the class oflanguages having quantum multi-prover interactive proof systems is necessarilycontained in NEXP, under the assumption that provers are allowed to share atmost polynomially many prior-entangled qubits. This implies that, inparticular, if provers do not share any prior entanglement with each other, theclass of languages having quantum multi-prover interactive proof systems isequal to NEXP. Related to these, it is shown that, in the case a prover doesnot have his private qubits, the class of languages having quantumsingle-prover interactive proof systems is also equal to NEXP.'}
1.95e+03 pattern=  0 task=  1 (u=532)  {'docs': u"The class MA consists of languages that can be efficiently verified byclassical probabilistic verifiers using a single classical certificate, and theclass QMA consists of languages that can be efficiently verified by quantumverifiers using a single quantum certificate. Suppose that a verifier receivesnot only one but multiple certificates. In the classical setting, it is obviousthat a classical verifier with multiple classical certificates is essentiallythe same with the one with a single classical certificate. However, in thequantum setting where a quantum verifier is given a set of quantum certificatesin tensor product form (i.e. each quantum certificate is not entangled withothers), the situation is different, because the quantum verifier might utilizethe structure of the tensor product form. This suggests a possibility ofanother hierarchy of complexity classes, namely the QMA hierarchy. From thispoint of view, we extend the definition of QMA to QMA(k) for the case quantumverifiers use k quantum certificates, and analyze the properties of QMA(k).  To compare the power of QMA(2) with that of QMA(1) = QMA, we show oneinteresting property of ``quantum indistinguishability''. This gives a strongevidence that QMA(2) is more powerful than QMA(1). Furthermore, we show that,for any fixed positive integer $k \\geq 2$, if a language L has a one-sidedbounded error QMA(k) protocol with a quantum verifier using k quantumcertificates, L necessarily has a one-sided bounded error QMA(2) protocol witha quantum verifier using only two quantum certificates."}
1.95e+03 pattern=  0 task=  0 (u=534)  {'docs': u'In this paper we present the use of Constraint Programming for solvingbalanced academic curriculum problems. We discuss the important role thatheuristics play when solving a problem using a constraint-based approach. Wealso show how constraint solving techniques allow to very efficiently solvecombinatorial optimization problems that are too hard for integer programmingtechniques.'}
2.5e+03 pattern=  0 task=  0 (u=555)  {'docs': u"We prove lower bounds for the direct sum problem for two-party bounded errorrandomised multiple-round communication protocols. Our proofs use the notion ofinformation cost of a protocol, as defined by Chakrabarti, Shi, Wirth and Yaoand refined further by Bar-Yossef, Jayram, Kumar and Sivakumar. Our maintechnical result is a `compression' theorem saying that, for any probabilitydistribution $\\mu$ over the inputs, a $k$-round private coin bounded errorprotocol for a function $f$ with information cost $c$ can be converted into a$k$-round deterministic protocol for $f$ with bounded distributional error andcommunication cost $O(kc)$. We prove this result using a substate theorem aboutrelative entropy and a rejection sampling argument. Our direct sum resultfollows from this `compression' result via elementary information theoreticarguments.  We also consider the direct sum problem in quantum communication. Using aprobabilistic argument, we show that messages cannot be compressed in thismanner even if they carry small information. Hence, new techniques may benecessary to tackle the direct sum problem in quantum communication."}
2.61e+03 pattern=  0 task=  0 (u=660)  {'docs': u'A theory of systems with long-range correlations based on the considerationof binary N-step Markov chains is developed. In the model, the conditionalprobability that the i-th symbol in the chain equals zero (or unity) is alinear function of the number of unities among the preceding N symbols. Thecorrelation and distribution functions as well as the variance of number ofsymbols in the words of arbitrary length L are obtained analytically andnumerically. A self-similarity of the studied stochastic process is revealedand the similarity group transformation of the chain parameters is presented.The diffusion Fokker-Planck equation governing the distribution function of theL-words is explored. If the persistent correlations are not extremely strong,the distribution function is shown to be the Gaussian with the variance beingnonlinearly dependent on L. The applicability of the developed theory to thecoarse-grained written and DNA texts is discussed.'}
3.18e+03 pattern=  0 task=  0 (u=739)  {'docs': u'We relate the Burrows-Wheeler transformation with a result in combinatoricson words known as the Gessel-Reutenauer transformation.'}
2.71e+03 pattern=  0 task=  0 (u=882)  {'docs': u'In this paper we study the phase transitions of different types of RandomBoolean networks. These differ in their updating scheme: synchronous,semi-synchronous, or asynchronous, and deterministic or non-deterministic. Ithas been shown that the statistical properties of Random Boolean networkschange considerable according to the updating scheme. We study with computersimulations sensitivity to initial conditions as a measure of order/chaos. Wefind that independently of their updating scheme, all network types have verysimilar phase transitions, namely when the average number of connections ofnodes is between one and three. This critical value depends more on the size ofthe network than on the updating scheme.'}
2.86e+03 pattern=  0 task=  1 (u=882)  {'docs': u'We discuss which properties common-use artifacts should have to collaboratewithout human intervention. We conceive how devices, such as mobile phones,PDAs, and home appliances, could be seamlessly integrated to provide an"ambient intelligence" that responds to the user\'s desires without requiringexplicit programming or commands. While the hardware and software technology tobuild such systems already exists, as yet there is no standard protocol thatcan learn new meanings. We propose the first steps in the development of such aprotocol, which would need to be adaptive, extensible, and open to thecommunity, while promoting self-organization. We argue that devices,interacting through "game-like" moves, can learn to agree about how tocommunicate, with whom to cooperate, and how to delegate and coordinatespecialized tasks. Thus, they may evolve a distributed cognition or collectiveintelligence capable of tackling complex tasks.'}
3.26e+03 pattern=  0 task=  2 (u=882)  {'docs': u'Our technologies complexify our environments. Thus, new technologies need todeal with more and more complexity. Several efforts have been made to deal withthis complexity using the concept of self-organization. However, in order topromote its use and understanding, we must first have a pragmatic understandingof complexity and self-organization. This paper presents a conceptual frameworkfor speaking about self-organizing systems. The aim is to provide a methodologyuseful for designing and controlling systems developed to solve complexproblems. First, practical notions of complexity and self-organization aregiven. Then, starting from the agent metaphor, a conceptual framework ispresented. This provides formal ways of speaking about "satisfaction" ofelements and systems. The main premise of the methodology claims that reducingthe "friction" or "interference" of interactions between elements of a systemwill result in a higher "satisfaction" of the system, i.e. better performance.The methodology discusses different ways in which this can be achieved. A casestudy on self-organizing traffic lights illustrates the ideas presented in thepaper.'}
1.82e+03 pattern=  0 task=  0 (u=1043)  {'docs': u'This paper precisely analyzes the wire density and required area in standardlayout styles for the hypercube. The most natural, regular layout of ahypercube of N^2 nodes in the plane, in a N x N grid arrangement, usesfloor(2N/3)+1 horizontal wiring tracks for each row of nodes. (The number oftracks per row can be reduced by 1 with a less regular design.) This paper alsogives a simple formula for the wire density at any cut position and a fullcharacterization of all places where the wire density is maximized (which doesnot occur at the bisection).'}
3.13e+03 pattern=  0 task=  0 (u=1053)  {'docs': u"In ISNN'04, a novel symmetric cipher was proposed, by combining a chaoticsignal and a clipped neural network (CNN) for encryption. The present paperanalyzes the security of this chaotic cipher against chosen-plaintext attacks,and points out that this cipher can be broken by a chosen-plaintext attack.Experimental analyses are given to support the feasibility of the proposedattack."}
2.46e+03 pattern=  0 task=  0 (u=1056)  {'docs': u"We consider the problem of partitioning $n$ integers into two subsets ofgiven cardinalities such that the discrepancy, the absolute value of thedifference of their sums, is minimized. The integers are i.i.d. randomvariables chosen uniformly from the set $\\{1,...,M\\}$. We study how the typicalbehavior of the optimal partition depends on $n,M$ and the bias $s$, thedifference between the cardinalities of the two subsets in the partition. Inparticular, we rigorously establish this typical behavior as a function of thetwo parameters $\\kappa:=n^{-1}\\log_2M$ and $b:=|s|/n$ by proving the existenceof three distinct ``phases'' in the $\\kappa b$-plane, characterized by thevalue of the discrepancy and the number of optimal solutions: a ``perfectphase'' with exponentially many optimal solutions with discrepancy 0 or 1; a``hard phase'' with minimal discrepancy of order $Me^{-\\Theta(n)}$; and a``sorted phase'' with an unique optimal partition of order $Mn$, obtained byputting the $(s+n)/2$ smallest integers in one subset. Our phase diagram coversall but a relatively small region in the $\\kappa b$-plane. We also show thatthe three phases can be alternatively characterized by the number of basissolutions of the associated linear programming problem, and by the fraction ofthese basis solutions whose $\\pm 1$-valued components form optimal integerpartitions of the subproblem with the corresponding weights. We show inparticular that this fraction is one in the sorted phase, and exponentiallysmall in both the perfect and hard phases, and strictly exponentially smallerin the hard phase than in the perfect phase. Open problems are discussed, andnumerical experiments are presented."}
2.38e+03 pattern=  0 task=  0 (u=1057)  {'docs': u'We study the satisfiability of randomly generated formulas formed by $M$clauses of exactly $K$ literals over $N$ Boolean variables. For a given valueof $N$ the problem is known to be most difficult with $\\alpha=M/N$ close to theexperimental threshold $\\alpha_c$ separating the region where almost allformulas are SAT from the region where all formulas are UNSAT. Recent resultsfrom a statistical physics analysis suggest that the difficulty is related tothe existence of a clustering phenomenon of the solutions when $\\alpha$ isclose to (but smaller than) $\\alpha_c$. We introduce a new type of messagepassing algorithm which allows to find efficiently a satisfiable assignment ofthe variables in the difficult region. This algorithm is iterative and composedof two main parts. The first is a message-passing procedure which generalizesthe usual methods like Sum-Product or Belief Propagation: it passes messagesthat are surveys over clusters of the ordinary messages. The second part usesthe detailed probabilistic information obtained from the surveys in order tofix variables and simplify the problem. Eventually, the simplified problem thatremains is solved by a conventional heuristic.'}
2.39e+03 pattern=  0 task=  1 (u=1057)  {'docs': u'Survey Propagation is an algorithm designed for solving typical instances ofrandom constraint satisfiability problems. It has been successfully tested onrandom 3-SAT and random $G(n,\\frac{c}{n})$ graph 3-coloring, in the hard regionof the parameter space. Here we provide a generic formalism which applies to awide class of discrete Constraint Satisfaction Problems.'}
2.75e+03 pattern=  0 task=  2 (u=1057)  {'docs': u"It has been shown experimentally that a decimation algorithm based on SurveyPropagation (SP) equations allows to solve efficiently some combinatorialproblems over random graphs. We show that these equations can be derived assum-product equations for the computation of marginals in an extended spacewhere the variables are allowed to take an additional value -- $*$ -- when theyare not forced by the combinatorial constraints. An appropriate ``localequilibrium condition'' cost/energy function is introduced and its entropy isshown to coincide with the expected logarithm of the number of clusters ofsolutions as computed by SP. These results may help to clarify the geometricalnotion of clusters assumed by SP for the random K-SAT or random graph coloring(where it is conjectured to be exact) and helps to explain which kind ofclustering operation or approximation is enforced in general/small sized modelsin which it is known to be inexact."}
2.91e+03 pattern=  0 task=  0 (u=1133)  {'docs': u'Defeasible logic is a rule-based nonmonotonic logic, with both strict anddefeasible rules, and a priority relation on rules. We show that inference inthe propositional form of the logic can be performed in linear time. Thiscontrasts markedly with most other propositional nonmonotonic logics, in whichinference is intractable.'}
2.08e+03 pattern=  0 task=  0 (u=1138)  {'docs': u'Non-negative sparse coding is a method for decomposing multivariate data intonon-negative sparse components. In this paper we briefly describe themotivation behind this type of data representation and its relation to standardsparse coding and non-negative matrix factorization. We then give a simple yetefficient multiplicative algorithm for finding the optimal values of the hiddencomponents. In addition, we show how the basis vectors can be learned from theobserved data. Simulations demonstrate the effectiveness of the proposedmethod.'}
1.9e+03 pattern=  0 task=  0 (u=1174)  {'docs': u'The search for information on the web is faced with several problems, whicharise on the one hand from the vast number of available sources, and on theother hand from their heterogeneity. A promising approach is the use ofmulti-agent systems of information agents, which cooperatively solve advancedinformation-retrieval problems. This requires capabilities to address complextasks, such as search and assessment of sources, query planning, informationmerging and fusion, dealing with incomplete information, and handling ofinconsistency. In this paper, our interest is in the role which some methodsfrom the field of declarative logic programming can play in the realization ofreasoning capabilities for information agents. In particular, we are interestedin how they can be used and further developed for the specific needs of thisapplication domain. We review some existing systems and current projects, whichaddress information-integration problems. We then focus on declarativeknowledge-representation methods, and review and evaluate approaches from logicprogramming and nonmonotonic reasoning for information agents. We discussadvantages and drawbacks, and point out possible extensions and open issues.'}
1.92e+03 pattern=  0 task=  1 (u=1174)  {'docs': u'We consider an approach to update nonmonotonic knowledge bases represented asextended logic programs under answer set semantics. New information isincorporated into the current knowledge base subject to a causal rejectionprinciple enforcing that, in case of conflicts, more recent rules are preferredand older rules are overridden. Such a rejection principle is also exploited inother approaches to update logic programs, e.g., in dynamic logic programmingby Alferes et al. We give a thorough analysis of properties of our approach, toget a better understanding of the causal rejection principle. We reviewpostulates for update and revision operators from the area of theory change andnonmonotonic reasoning, and some new properties are considered as well. We thenconsider refinements of our semantics which incorporate a notion of minimalityof change. As well, we investigate the relationship to other approaches,showing that our approach is semantically equivalent to inheritance programs byBuccafurri et al. and that it coincides with certain classes of dynamic logicprograms, for which we provide characterizations in terms of graph conditions.Therefore, most of our results about properties of causal rejection principleapply to these approaches as well. Finally, we deal with computationalcomplexity of our approach, and outline how the update semantics and itsrefinements can be implemented on top of existing logic programming engines.'}
2.3e+03 pattern=  0 task=  2 (u=1174)  {'docs': u'Recently, several approaches to updating knowledge bases modeled as extendedlogic programs have been introduced, ranging from basic methods to incorporate(sequences of) sets of rules into a logic program, to more elaborate methodswhich use an update policy for specifying how updates must be incorporated. Inthis paper, we introduce a framework for reasoning about evolving knowledgebases, which are represented as extended logic programs and maintained by anupdate policy. We first describe a formal model which captures various updateapproaches, and we define a logical language for expressing properties ofevolving knowledge bases. We then investigate semantical and computationalproperties of our framework, where we focus on properties of knowledge stateswith respect to the canonical reasoning task of whether a given formula holdson a given evolving knowledge base. In particular, we present finitarycharacterizations of the evolution for certain classes of framework instances,which can be exploited for obtaining decidability results. In more detail, wecharacterize the complexity of reasoning for some meaningful classes ofevolving knowledge bases, ranging from polynomial to double exponential spacecomplexity.'}
1.92e+03 pattern=  0 task=  0 (u=1180)  {'docs': u'We propose an extension of the asynchronous pi-calculus with a notion ofrandom choice. We define an operational semantics which distinguishes betweenprobabilistic choice, made internally by the process, and nondeterministicchoice, made externally by an adversary scheduler. This distinction will allowus to reason about the probabilistic correctness of algorithms under certainschedulers. We show that in this language we can solve the electoral problem,which was proved not possible in the asynchronous $\\pi$-calculus. Finally, weshow an implementation of the probabilistic asynchronous pi-calculus in aJava-like language.'}
2.79e+03 pattern=  0 task=  0 (u=1364)  {'docs': u'Sharing, an abstract domain developed by D. Jacobs and A. Langen for theanalysis of logic programs, derives useful aliasing information. It iswell-known that a commonly used core of techniques, such as the integration ofSharing with freeness and linearity information, can significantly improve theprecision of the analysis. However, a number of other proposals for refineddomain combinations have been circulating for years. One feature that is commonto these proposals is that they do not seem to have undergone a thoroughexperimental evaluation even with respect to the expected precision gains. Inthis paper we experimentally evaluate: helping Sharing with the definitelyground variables found using Pos, the domain of positive Boolean formulas; theincorporation of explicit structural information; a full implementation of thereduced product of Sharing and Pos; the issue of reordering the bindings in thecomputation of the abstract mgu; an original proposal for the addition of a newmode recording the set of variables that are deemed to be ground or free; arefined way of using linearity to improve the analysis; the recovery of hiddeninformation in the combination of Sharing with freeness information. Finally,we discuss the issue of whether tracking compoundness allows the computation ofmore sharing information.'}
  984 pattern=  3 task=  0 (u=1389)  {'docs': u'A mean field feedback artificial neural network algorithm is developed andexplored for the set covering problem. A convenient encoding of the inequalityconstraints is achieved by means of a multilinear penalty function. Anapproximate energy minimum is obtained by iterating a set of mean fieldequations, in combination with annealing. The approach is numerically testedagainst a set of publicly available test problems with sizes ranging up to5x10^3 rows and 10^6 columns. When comparing the performance with exact resultsfor sizes where these are available, the approach yields results within a fewpercent from the optimal solutions. Comparisons with other approximate methodsalso come out well, in particular given the very low CPU consumption required-- typically a few seconds. Arbitrary problems can be processed using thealgorithm via a public domain server.'}
2.63e+03 pattern=  0 task=  0 (u=1500)  {'docs': u"The Thorup-Zwick (TZ) routing scheme is the first generic stretch-3 routingscheme delivering a nearly optimal local memory upper bound. Using both directanalysis and simulation, we calculate the stretch distribution of this routingscheme on random graphs with power-law node degree distributions, $P_k \\simk^{-\\gamma}$. We find that the average stretch is very low and virtuallyindependent of $\\gamma$. In particular, for the Internet interdomain graph,$\\gamma \\sim 2.1$, the average stretch is around 1.1, with up to 70% of pathsbeing shortest. As the network grows, the average stretch slowly decreases. Therouting table is very small, too. It is well below its upper bounds, and itssize is around 50 records for $10^4$-node networks. Furthermore, we find thatboth the average shortest path length (i.e. distance) $\\bar{d}$ and width ofthe distance distribution $\\sigma$ observed in the real Internet inter-AS graphhave values that are very close to the minimums of the average stretch in the$\\bar{d}$- and $\\sigma$-directions. This leads us to the discovery of a uniquecritical quasi-stationary point of the average TZ stretch as a function of$\\bar{d}$ and $\\sigma$. The Internet distance distribution is located in aclose neighborhood of this point. This observation suggests the analyticalstructure of the average stretch function may be an indirect indicator of somehidden optimization criteria influencing the Internet's interdomain topologyevolution."}
2.08e+03 pattern=  0 task=  0 (u=1552)  {'docs': u'In this paper, we give upper and lower bounds on the number of Steiner pointsrequired to construct a strictly convex quadrilateral mesh for a planar pointset. In particular, we show that $3{\\lfloor\\frac{n}{2}\\rfloor}$ internalSteiner points are always sufficient for a convex quadrilateral mesh of $n$points in the plane. Furthermore, for any given $n\\geq 4$, there are point setsfor which $\\lceil\\frac{n-3}{2}\\rceil-1$ Steiner points are necessary for aconvex quadrilateral mesh.'}
2.12e+03 pattern=  0 task=  1 (u=1552)  {'docs': u'In this note we consider the problem of manufacturing a convex polyhedralobject via casting. We consider a generalization of the sand casting processwhere the object is manufactured by gluing together two identical faces ofparts cast with a single piece mold. In this model we show that the class ofconvex polyhedra which can be enclosed between two concentric spheres of theratio of their radii less than 1.07 cannot be manufactured using only two castparts.'}
1.15e+03 pattern=  0 task=  0 (u=1569)  {'docs': u'In this note, we study the easy certificate classes introduced byHemaspaandra, Rothe, and Wechsung, with regard to the question of whether ornot surjective one-way functions exist. This is an important open question incryptology. We show that the existence of partial one-way permutations can becharacterized by separating P from the class of UP sets that, for allunambiguous polynomial-time Turing machines accepting them, always have easy(i.e., polynomial-time computable) certificates. This extends results ofGrollmann and Selman. By Gr\\"adel\'s recent results about one-way functions,this also links statements about easy certificates of NP sets with statementsin finite model theory. Similarly, there exist surjective poly-one one-wayfunctions if and only if there is a set L in P such that not all FewP machinesaccepting L always have easy certificates. We also establish a conditionnecessary and sufficient for the existence of (total) one-way permutations.'}
2.19e+03 pattern=  0 task=  0 (u=1676)  {'docs': u'We solve the following geometric problem, which arises in severalthree-dimensional applications in computational geometry: For whicharrangements of two lines and two spheres in R^3 are there infinitely manylines simultaneously transversal to the two lines and tangent to the twospheres?  We also treat a generalization of this problem to projective quadrics:Replacing the spheres in R^3 by quadrics in projective space P^3, and fixingthe lines and one general quadric, we give the following complete geometricdescription of the set of (second) quadrics for which the 2 lines and 2quadrics have infinitely many transversals and tangents: In thenine-dimensional projective space P^9 of quadrics, this is a curve of degree 24consisting of 12 plane conics, a remarkably reducible variety.'}
2.51e+03 pattern=  0 task=  1 (u=1676)  {'docs': u'We study the set of lines that meet a fixed line and are tangent to twospheres and classify the configurations consisting of a single line and threespheres for which there are infinitely many lines tangent to the three spheresthat also meet the given line. All such configurations are degenerate. The pathto this result involves the interplay of some beautiful and intricate geometryof real surfaces in 3-space, complex algebraic geometry, explicit computationand graphics.'}
2.37e+03 pattern=  0 task=  0 (u=1722)  {'docs': u'In this article, we characterize in terms of analytic tableaux the repairs ofinconsistent relational databases, that is databases that do not satisfy agiven set of integrity constraints. For this purpose we provide closing andopening criteria for branches in tableaux that are built for database instancesand their integrity constraints. We use the tableaux based characterization asa basis for consistent query answering, that is for retrieving from thedatabase answers to queries that are consistent wrt the integrity constraints.'}
1.19e+03 pattern=  0 task=  0 (u=1861)  {'docs': u'A circle $C$ separates two planar sets if it encloses one of the sets and itsopen interior disk does not meet the other set. A separating circle is alargest one if it cannot be locally increased while still separating the twogiven sets. An Theta(n log n) optimal algorithm is proposed to find all largestcircles separating two given sets of line segments when line segments areallowed to meet only at their endpoints. In the general case, when linesegments may intersect $\\Omega(n^2)$ times, our algorithm can be adapted towork in O(n alpha(n) log n) time and O(n \\alpha(n)) space, where alpha(n)represents the extremely slowly growing inverse of the Ackermann function.'}
1.19e+03 pattern=  0 task=  1 (u=1861)  {'docs': u'We consider the motion planning problem for a point constrained to move alonga smooth closed convex path of bounded curvature. The workspace of the movingpoint is bounded by a convex polygon with m vertices, containing an obstacle ina form of a simple polygon with $n$ vertices. We present an O(m+n) timealgorithm finding the path, going around the obstacle, whose curvature is thesmallest possible.'}
1.19e+03 pattern=  0 task=  2 (u=1861)  {'docs': u'Two planar sets are circularly separable if there exists a circle enclosingone of the sets and whose open interior disk does not intersect the other set.  This paper studies two problems related to circular separability. Alinear-time algorithm is proposed to decide if two polygons are circularlyseparable. The algorithm outputs the smallest separating circle. The secondproblem asks for the largest circle included in a preprocessed, convex polygon,under some point and/or line constraints. The resulting circle must contain thequery points and it must lie in the halfplanes delimited by the query lines.'}
2.23e+03 pattern=  0 task=  0 (u=1977)  {'docs': u'It is next to impossible to develop real-life applications in just pureProlog. With XPCE we realised a mechanism for integrating Prolog with anexternal object-oriented system that turns this OO system into a naturalextension to Prolog. We describe the design and how it can be applied to otherexternal OO systems.'}
3.21e+03 pattern=  0 task=  0 (u=1992)  {'docs': u'It is now well known that the performance of a linear code $C$ underiterative decoding on a binary erasure channel (and other channels) isdetermined by the size of the smallest stopping set in the Tanner graph for$C$. Several recent papers refer to this parameter as the \\emph{stoppingdistance} $s$ of $C$. This is somewhat of a misnomer since the size of thesmallest stopping set in the Tanner graph for $C$ depends on the correspondingchoice of a parity-check matrix. It is easy to see that $s \\le d$, where $d$ isthe minimum Hamming distance of $C$, and we show that it is always possible tochoose a parity-check matrix for $C$ (with sufficiently many dependent rows)such that $s = d$. We thus introduce a new parameter, termed the \\emph{stoppingredundancy} of $C$, defined as the minimum number of rows in a parity-checkmatrix $H$ for $C$ such that the corresponding stopping distance $s(H)$ attainsits largest possible value, namely $s(H) = d$.  We then derive general bounds on the stopping redundancy of linear codes. Wealso examine several simple ways of constructing codes from other codes, andstudy the effect of these constructions on the stopping redundancy.Specifically, for the family of binary Reed-Muller codes (of all orders), weprove that their stopping redundancy is at most a constant times theirconventional redundancy. We show that the stopping redundancies of the binaryand ternary extended Golay codes are at most 35 and 22, respectively. Finally,we provide upper and lower bounds on the stopping redundancy of MDS codes.'}
2.94e+03 pattern=  0 task=  0 (u=2057)  {'docs': u'This paper is concerned with online caching algorithms for the(n,k)-companion cache, defined by Brehob et. al. In this model the cache iscomposed of two components: a k-way set-associative cache and a companionfully-associative cache of size n. We show that the deterministic competitiveratio for this problem is (n+1)(k+1)-1, and the randomized competitive ratio isO(\\log n \\log k) and \\Omega(\\log n +\\log k).'}
2.01e+03 pattern=  0 task=  0 (u=2159)  {'docs': u'Randomized search algorithms for hard combinatorial problems exhibit a largevariability of performances. We study the different types of rare events whichoccur in such out-of-equilibrium stochastic processes and we show how theycooperate in determining the final distribution of running times. As abyproduct of our analysis we show how search algorithms are optimized by randomrestarts.'}
2.62e+03 pattern=  0 task=  1 (u=2159)  {'docs': u'We reconsider the one-step replica-symmetry-breaking (1RSB) solutions of tworandom combinatorial problems: k-XORSAT and k-SAT. We present a general methodfor establishing the stability of these solutions with respect to further stepsof replica-symmetry breaking. Our approach extends the ideas of [A.Montanariand F. Ricci-Tersenghi, Eur.Phys.J. B 33, 339 (2003)] to more generalcombinatorial problems.  It turns out that 1RSB is always unstable at sufficiently small clausesdensity alpha or high energy. In particular, the recent 1RSB solution to 3-SATis unstable at zero energy for alpha< alpha_m, with alpha_m\\approx 4.153. Onthe other hand, the SAT-UNSAT phase transition seems to be correctly describedwithin 1RSB.'}
2.97e+03 pattern=  0 task=  2 (u=2159)  {'docs': u"A new method for analyzing low density parity check (LDPC) codes and lowdensity generator matrix (LDGM) codes under bit maximum a posterioriprobability (MAP) decoding is introduced. The method is based on a rigorousapproach to spin glasses developed by Francesco Guerra. It allows to constructlower bounds on the entropy of the transmitted message conditional to thereceived one. Based on heuristic statistical mechanics calculations, weconjecture such bounds to be tight. The result holds for standard irregularensembles when used over binary input output symmetric channels. The method isfirst developed for Tanner graph ensembles with Poisson left degreedistribution. It is then generalized to `multi-Poisson' graphs, and, by acompletion procedure, to arbitrary degree distribution."}
1.14e+03 pattern=  0 task=  0 (u=2173)  {'docs': u'The aim of the Alma project is the design of a strongly typed constraintprogramming language that combines the advantages of logic and imperativeprogramming. The first stage of the project was the design and implementationof Alma-0, a small programming language that provides a support for declarativeprogramming within the imperative programming framework. It is obtained byextending a subset of Modula-2 by a small number of features inspired by thelogic programming paradigm. In this paper we discuss the rationale for thedesign of Alma-0, the benefits of the resulting hybrid programming framework,and the current work on adding constraint processing capabilities to thelanguage. In particular, we discuss the role of the logical and customaryvariables, the interaction between the constraint store and the program, andthe need for lists.'}
1.39e+03 pattern=  0 task=  1 (u=2173)  {'docs': u"We study here a natural situation when constraint programming can be entirelyreduced to rule-based programming. To this end we explain first how one cancompute on constraint satisfaction problems using rules represented by simplefirst-order formulas. Then we consider constraint satisfaction problems thatare based on predefined, explicitly given constraints. To solve them we firstderive rules from these explicitly given constraints and limit the computationprocess to a repeated application of these rules, combined with labeling.Weconsider here two types of rules. The first type, that we call equality rules,leads to a new notion of local consistency, called {\\em rule consistency} thatturns out to be weaker than arc consistency for constraints of arbitrary arity(called hyper-arc consistency in \\cite{MS98b}). For Boolean constraints ruleconsistency coincides with the closure under the well-known propagation rulesfor Boolean constraints. The second type of rules, that we call membershiprules, yields a rule-based characterization of arc consistency. To showfeasibility of this rule-based approach to constraint programming we show howboth types of rules can be automatically generated, as {\\tt CHR} rules of\\cite{fruhwirth-constraint-95}. This yields an implementation of this approachto programming by means of constraint logic programming. We illustrate theusefulness of this approach to constraint programming by discussing variousexamples, including Boolean constraints, two typical examples of many valuedlogics, constraints dealing with Waltz's language for describing polyhedralscenes, and Allen's qualitative approach to temporal logic."}
2.84e+03 pattern=  0 task=  2 (u=2173)  {'docs': u'We propose here a number of approaches to implement constraint propagationfor arithmetic constraints on integer intervals. To this end we introduceinteger interval arithmetic. Each approach is explained using appropriate proofrules that reduce the variable domains. We compare these approaches using a setof benchmarks.'}
2.84e+03 pattern=  0 task=  3 (u=2173)  {'docs': u"We provide elementary and uniform proofs of order independence for variousstrategy elimination procedures for finite strategic games, both for dominanceby pure and by mixed strategies. The proofs follow the same pattern and focuson the structural properties of the dominance relations. They rely on Newman'sLemma established in 1942 and related results on the abstract reductionsystems."}
1.7e+03 pattern=  7 task=  0 (u=2175)  {'docs': u'I outline the involvement of the Los Alamos e-print archive (arXiv) withinthe Open Archives Initiative (OAI) and describe the implementation of the dataprovider side of the OAI protocol v1.0. I highlight the ways in which we mapthe existing structure of arXiv onto elements of the protocol.'}
1.85e+03 pattern=  0 task=  1 (u=2175)  {'docs': u'In this article I outline the ideas behind the Open Archives Initiativemetadata harvesting protocol (OAIMH), and attempt to clarify some commonmisconceptions. I then consider how the OAIMH protocol can be used to exposeand harvest metadata. Perl code examples are given as practical illustration.'}
2.59e+03 pattern=  0 task=  2 (u=2175)  {'docs': u'The Open Archives Initiative (OAI) was created as a practical way to promoteinteroperability between eprint repositories. Although the scope of the OAI hasbeen broadened, eprint repositories still represent a significant fraction ofOAI data providers. In this article I present a brief survey of OAI eprintrepositories, and of services using metadata harvested from eprint repositoriesusing the OAI protocol for metadata harvesting (OAI-PMH). I then discussseveral situations where metadata harvesting may be used to further improve theutility of eprint archives as a component of the scholarly communicationinfrastructure.'}
2.6e+03 pattern=  0 task=  0 (u=2317)  {'docs': u'Common approach to radial distortion is by the means of polynomialapproximation, which introduces distortion-specific parameters into the cameramodel and requires estimation of these distortion parameters. The task ofestimating radial distortion is to find a radial distortion model that allowseasy undistortion as well as satisfactory accuracy. This paper presents a newradial distortion model with an easy analytical undistortion formula, whichalso belongs to the polynomial approximation category. Experimental results arepresented to show that with this radial distortion model, satisfactory accuracyis achieved.'}
2.6e+03 pattern=  0 task=  1 (u=2317)  {'docs': u'The common approach to radial distortion is by the means of polynomialapproximation, which introduces distortion-specific parameters into the cameramodel and requires estimation of these distortion parameters. The task ofestimating radial distortion is to find a radial distortion model that allowseasy undistortion as well as satisfactory accuracy. This paper presents a newclass of rational radial distortion models with easy analytical undistortionformulae. Experimental results are presented to show that with this class ofrational radial distortion models, satisfactory and comparable accuracy isachieved.'}
2.6e+03 pattern=  0 task=  2 (u=2317)  {'docs': u'The common approach to radial distortion is by the means of polynomialapproximation, which introduces distortion-specific parameters into the cameramodel and requires estimation of these distortion parameters. The task ofestimating radial distortion is to find a radial distortion model that allowseasy undistortion as well as satisfactory accuracy. This paper presents a newpiecewise radial distortion model with easy analytical undistortion formula.The motivation for seeking a piecewise radial distortion model is that, when acamera is resulted in a low quality during manufacturing, the nonlinear radialdistortion can be complex. Using low order polynomials to approximate theradial distortion might not be precise enough. On the other hand, higher orderpolynomials suffer from the inverse problem. With the new piecewise radialdistortion function, more flexibility is obtained and the radial undistortioncan be performed analytically. Experimental results are presented to show thatwith this new piecewise radial distortion model, better performance is achievedthan that using the single function. Furthermore, a comparable performance withthe conventional polynomial model using 2 coefficients can also beaccomplished.'}
2.62e+03 pattern=  0 task=  3 (u=2317)  {'docs': u'The commonly used radial distortion model for camera calibration is in factan assumption or a restriction. In practice, camera distortion could happen ina general geometrical manner that is not limited to the radial sense. Thispaper proposes a simplified geometrical distortion modeling method by using twodifferent radial distortion functions in the two image axes. A family ofsimplified geometric distortion models is proposed, which are either simplepolynomials or the rational functions of polynomials. Analytical geometricundistortion is possible using two of the distortion functions discussed inthis paper and their performance can be improved by applying a piecewisefitting idea. Our experimental results show that the geometrical distortionmodels always perform better than their radial distortion counterparts.Furthermore, the proposed geometric modeling method is more appropriate forcameras whose distortion is not perfectly radially symmetric around the centerof distortion.'}
2.91e+03 pattern=  0 task=  4 (u=2317)  {'docs': u'This paper presents a blind detection and compensation technique for cameralens geometric distortions. The lens distortion introduces higher-ordercorrelations in the frequency domain and in turn it can be detected usinghigher-order spectral analysis tools without assuming any specific calibrationtarget. The existing blind lens distortion removal method only considered asingle-coefficient radial distortion model. In this paper, two coefficients areconsidered to model approximately the geometric distortion. All the modelsconsidered have analytical closed-form inverse formulae.'}
2.73e+03 pattern=  0 task=  0 (u=2410)  {'docs': u'We present a method for hierarchical clustering of data called {\\it mutualinformation clustering} (MIC) algorithm. It uses mutual information (MI) as asimilarity measure and exploits its grouping property: The MI between threeobjects $X, Y,$ and $Z$ is equal to the sum of the MI between $X$ and $Y$, plusthe MI between $Z$ and the combined object $(XY)$. We use this both in theShannon (probabilistic) version of information theory and in the Kolmogorov(algorithmic) version. We apply our method to the construction of phylogenetictrees from mitochondrial DNA sequences and to the output of independentcomponents analysis (ICA) as illustrated with the ECG of a pregnant woman.'}
2.73e+03 pattern=  0 task=  1 (u=2410)  {'docs': u'Motivation: Clustering is a frequently used concept in variety ofbioinformatical applications. We present a new method for hierarchicalclustering of data called mutual information clustering (MIC) algorithm. Ituses mutual information (MI) as a similarity measure and exploits its groupingproperty: The MI between three objects X, Y, and Z is equal to the sum of theMI between X and Y, plus the MI between Z and the combined object (XY).  Results: We use this both in the Shannon (probabilistic) version ofinformation theory, where the "objects" are probability distributionsrepresented by random samples, and in the Kolmogorov (algorithmic) version,where the "objects" are symbol sequences. We apply our method to theconstruction of mammal phylogenetic trees from mitochondrial DNA sequences andwe reconstruct the fetal ECG from the output of independent components analysis(ICA) applied to the ECG of a pregnant woman.  Availability: The programs for estimation of MI and for clustering(probabilistic version) are available athttp://www.fz-juelich.de/nic/cs/software'}
2.56e+03 pattern=  0 task=  0 (u=2467)  {'docs': u'PHENIX is one of the two large experiments at the Relativistic Heavy IonCollider (RHIC) at Brookhaven National Laboratory (BNL) and archives roughly100TB of experimental data per year. In addition, large volumes of simulateddata are produced at multiple off-site computing centers. For any file catalogto play a central role in data management it has to face problems associatedwith the need for distributed access and updates. To be used effectively by thehundreds of PHENIX collaborators in 12 countries the catalog must satisfy thefollowing requirements: 1) contain up-to-date data, 2) provide fast andreliable access to the data, 3) have write permissions for the sites that storeportions of data. We present an analysis of several available RelationalDatabase Management Systems (RDBMS) to support a catalog meeting the aboverequirements and discuss the PHENIX experience with building and using thedistributed file catalog.'}
2.56e+03 pattern=  0 task=  0 (u=2469)  {'docs': u'Many entities managed by HEP Software Frameworks represent spatial(3-dimensional) real objects. Effective definition, manipulation andvisualization of such objects is an indispensable functionality.  GraXML is a modular Geometric Modeling toolkit capable of processinggeometric data of various kinds (detector geometry, event geometry) fromdifferent sources and delivering them in ways suitable for further use.Geometric data are first modeled in one of the Generic Models. Those Models arethen used to populate powerful Geometric Model based on the Java3D technology.While Java3D has been originally created just to provide visualization of 3Dobjects, its light weight and high functionality allow an effective reuse as ageneral geometric component. This is possible also thanks to a large overlapbetween graphical and general geometric functionality and modular design ofJava3D itself. Its graphical functionalities also allow a natural visualizationof all manipulated elements.  All these techniques have been developed primarily (or only) for the Javaenvironment. It is, however, possible to interface them transparently toFrameworks built in other languages, like for example C++.  The GraXML toolkit has been tested with data from several sources, as forexample ATLAS and ALICE detector description and ATLAS event data. Prototypesfor other sources, like Geometry Description Markup Language (GDML) exist tooand interface to any other source is easy to add.'}
2.56e+03 pattern=  0 task=  0 (u=2478)  {'docs': u'We give a purely model-theoretic characterization of the semantics of logicprograms with negation-as-failure allowed in clause bodies. In our semanticsthe meaning of a program is, as in the classical case, the unique minimum modelin a program-independent ordering. We use an expanded truth domain that has anuncountable linearly ordered set of truth values between False (the minimumelement) and True (the maximum), with a Zero element in the middle. The truthvalues below Zero are ordered like the countable ordinals. The values aboveZero have exactly the reverse order. Negation is interpreted as reflectionabout Zero followed by a step towards Zero; the only truth value that remainsunaffected by negation is Zero. We show that every program has a unique minimummodel M_P, and that this model can be constructed with a T_P iteration whichproceeds through the countable ordinals. Furthermore, we demonstrate that M_Pcan also be obtained through a model intersection construction whichgeneralizes the well-known model intersection theorem for classical logicprogramming. Finally, we show that by collapsing the true and false values ofthe infinite-valued model M_P to (the classical) True and False, we obtain athree-valued model identical to the well-founded one.'}
2.56e+03 pattern=  0 task=  0 (u=2481)  {'docs': u"There are several numerical methods for computing approximate zeros of agiven univariate polynomial. In this paper, we develop a simple and novelmethod for determining sharp upper bounds on errors in approximate zeros of agiven polynomial using Rouche's theorem from complex analysis. We compute theerror bounds using non-linear optimization. Our bounds are scalable in thesense that we compute sharper error bounds for better approximations of zeros.We use high precision computations using the LEDA/real floating-point filterfor computing our bounds robustly."}
2.94e+03 pattern=  0 task=  0 (u=2485)  {'docs': u'We develop graph theoretic methods for analysing maximally entangled purestates distributed between a number of different parties. We introduce atechnique called {\\it bicolored merging}, based on the monotonicity feature ofentanglement measures, for determining combinatorial conditions that must besatisfied for any two distinct multiparticle states to be comparable underlocal operations and classical communication (LOCC). We present several resultsbased on the possibility or impossibility of comparability of pure multipartitestates. We show that there are exponentially many such entangled multipartitestates among $n$ agents. Further, we discuss a new graph theoretic metric on aclass of multi-partite states, and its implications.'}
2.98e+03 pattern=  0 task=  1 (u=2485)  {'docs': u'A restriction on quantum secret sharing (QSS) that comes from the no-cloningtheorem is that any pair of authorized sets in an access structure shouldoverlap. From the viewpoint of application, this places an unnatural constrainton secret sharing. We present a generalization, called assisted QSS (AQSS),where access structures without pairwise overlap of authorized sets ispermissible, provided some shares are withheld by the share dealer. We showthat no more than $\\lambda-1$ withheld shares are required, where $\\lambda$ isthe minimum number of {\\em partially linked classes} among the authorized setsfor the QSS. This is useful in QSS schemes where the share dealer is honest bydefinition and is equivalent to a secret reconstructor. Our result means thatsuch applications of QSS need not be thwarted by the no-cloning theorem.'}
2.1e+03 pattern=  0 task=  0 (u=2752)  {'docs': u'Cores are, besides connectivity components, one among few concepts thatprovides us with efficient decompositions of large graphs and networks.  In the paper a generalization of the notion of core of a graph based onvertex property function is presented. It is shown that for the local monotonevertex property functions the corresponding cores can be determined in $O(m\\max (\\Delta, \\log n))$ time.'}
2.62e+03 pattern=  0 task=  1 (u=2752)  {'docs': u'Short cycles connectivity is a generalization of ordinary connectivity.Instead by a path (sequence of edges), two vertices have to be connected by asequence of short cycles, in which two adjacent cycles have at least one commonvertex. If all adjacent cycles in the sequence share at least one edge, we talkabout edge short cycles connectivity.  It is shown that the short cycles connectivity is an equivalence relation onthe set of vertices, while the edge short cycles connectivity componentsdetermine an equivalence relation on the set of edges. Efficient algorithms fordetermining equivalence classes are presented.  Short cycles connectivity can be extended to directed graphs (cyclic andtransitive connectivity). For further generalization we can also considerconnectivity by small cliques or other families of graphs.'}
2.7e+03 pattern=  0 task=  2 (u=2752)  {'docs': u'The structure of large networks can be revealed by partitioning them tosmaller parts, which are easier to handle. One of such decompositions is basedon $k$--cores, proposed in 1983 by Seidman. In the paper an efficient, $O(m)$,$m$ is the number of lines, algorithm for determining the cores decompositionof a given network is presented.'}
1.86e+03 pattern=  0 task=  0 (u=2809)  {'docs': u"We present new algorithms to compute fundamental properties of a Booleanfunction given in truth-table form. Specifically, we give an O(N^2.322 log N)algorithm for block sensitivity, an O(N^1.585 log N) algorithm for `treedecomposition,' and an O(N) algorithm for `quasisymmetry.' These algorithms arebased on new insights into the structure of Boolean functions that may be ofindependent interest. We also give a subexponential-time algorithm for thespace-bounded quantum query complexity of a Boolean function. To prove thisalgorithm correct, we develop a theory of limited-precision representation ofunitary operators, building on work of Bernstein and Vazirani."}
2.29e+03 pattern=  0 task=  1 (u=2809)  {'docs': u'One of the earliest quantum algorithms was discovered by Bernstein andVazirani, for a problem called Recursive Fourier Sampling. This paper showsthat the Bernstein-Vazirani algorithm is not far from optimal. The moral isthat the need to "uncompute" garbage can impose a fundamental limit onefficient quantum computation. The proof introduces a new parameter of Booleanfunctions called the "nonparity coefficient," which might be of independentinterest.'}
2.71e+03 pattern=  0 task=  2 (u=2809)  {'docs': u"Several researchers, including Leonid Levin, Gerard 't Hooft, and StephenWolfram, have argued that quantum mechanics will break down before thefactoring of large numbers becomes possible. If this is true, then there shouldbe a natural set of quantum states that can account for all experimentsperformed to date, but not for Shor's factoring algorithm. We investigate as acandidate the set of states expressible by a polynomial number of additions andtensor products. Using a recent lower bound on multilinear formula size due toRaz, we then show that states arising in quantum error-correction requiren^{Omega(log n)} additions and tensor products even to approximate, whichincidentally yields the first superpolynomial gap between general andmultilinear formula size of functions. More broadly, we introduce a complexityclassification of pure quantum states, and prove many basic facts about thisclassification. Our goal is to refine vague ideas about a breakdown of quantummechanics into specific hypotheses that might be experimentally testable in thenear future."}
2.81e+03 pattern=  0 task=  3 (u=2809)  {'docs': u'Although a quantum state requires exponentially many classical bits todescribe, the laws of quantum mechanics impose severe restrictions on how thatstate can be accessed. This paper shows in three settings that quantum messageshave only limited advantages over classical ones. First, we show that BQP/qpolyis contained in PP/poly, where BQP/qpoly is the class of problems solvable inquantum polynomial time, given a polynomial-size "quantum advice state" thatdepends only on the input length. This resolves a question of Buhrman, andmeans that we should not hope for an unrelativized separation between quantumand classical advice. Underlying our complexity result is a general newrelation between deterministic and quantum one-way communication complexities,which applies to partial as well as total functions. Second, we construct anoracle relative to which NP is not contained in BQP/qpoly. To do so, we use thepolynomial method to give the first correct proof of a direct product theoremfor quantum search. This theorem has other applications; for example, it can beused to fix a flawed result of Klauck about quantum time-space tradeoffs forsorting. Third, we introduce a new trace distance method for proving lowerbounds on quantum one-way communication complexity. Using this method, weobtain optimal quantum lower bounds for two problems of Ambainis, for which nonontrivial lower bounds were previously known even for classical randomizedprotocols.'}
2.94e+03 pattern=  0 task=  4 (u=2809)  {'docs': u'The Gottesman-Knill theorem says that a stabilizer circuit -- that is, aquantum circuit consisting solely of CNOT, Hadamard, and phase gates -- can besimulated efficiently on a classical computer. This paper improves that theoremin several directions. First, by removing the need for Gaussian elimination, wemake the simulation algorithm much faster at the cost of a factor-2 increase inthe number of bits needed to represent a state. We have implemented theimproved algorithm in a freely-available program called CHP(CNOT-Hadamard-Phase), which can handle thousands of qubits easily. Second, weshow that the problem of simulating stabilizer circuits is complete for theclassical complexity class ParityL, which means that stabilizer circuits areprobably not even universal for classical computation. Third, we give efficientalgorithms for computing the inner product between two stabilizer states,putting any n-qubit stabilizer circuit into a "canonical form" that requires atmost O(n^2/log n) gates, and other useful tasks. Fourth, we extend oursimulation algorithm to circuits acting on mixed states, circuits containing alimited number of non-stabilizer gates, and circuits acting on generaltensor-product initial states but containing only a limited number ofmeasurements.'}
2.95e+03 pattern=  0 task=  5 (u=2809)  {'docs': u'A celebrated 1976 theorem of Aumann asserts that honest, rational Bayesianagents with common priors will never "agree to disagree": if their opinionsabout any topic are common knowledge, then those opinions must be equal.Economists have written numerous papers examining the assumptions behind thistheorem. But two key questions went unaddressed: first, can the agents reachagreement after a conversation of reasonable length? Second, can thecomputations needed for that conversation be performed efficiently? This paperanswers both questions in the affirmative, thereby strengthening Aumann\'soriginal conclusion.  We first show that, for two agents with a common prior to agree withinepsilon about the expectation of a [0,1] variable with high probability overtheir prior, it suffices for them to exchange order 1/epsilon^2 bits. Thisbound is completely independent of the number of bits n of relevant knowledgethat the agents have. We then extend the bound to three or more agents; and wegive an example where the economists\' "standard protocol" (which consists ofrepeatedly announcing one\'s current expectation) nearly saturates the bound,while a new "attenuated protocol" does better. Finally, we give a protocol thatwould cause two Bayesians to agree within epsilon after exchanging order1/epsilon^2 messages, and that can be simulated by agents with limitedcomputational resources. By this we mean that, after examining the agents\'knowledge and a transcript of their conversation, no one would be able todistinguish the agents from perfect Bayesians. The time used by the simulationprocedure is exponential in 1/epsilon^6 but not in n.'}
3.12e+03 pattern=  0 task=  6 (u=2809)  {'docs': u'More than a speculative technology, quantum computing seems to challenge ourmost basic intuitions about how the physical world should behave. In thisthesis I show that, while some intuitions from classical computer science mustbe jettisoned in the light of modern physics, many others emerge nearlyunscathed; and I use powerful tools from computational complexity theory tohelp determine which are which.'}
3.13e+03 pattern=  0 task=  7 (u=2809)  {'docs': u'I study the class of problems efficiently solvable by a quantum computer,given the ability to "postselect" on the outcomes of measurements. I prove thatthis class coincides with a classical complexity class called PP, orProbabilistic Polynomial-Time. Using this result, I show that several simplechanges to the axioms of quantum mechanics would let us solve PP-completeproblems efficiently. The result also implies, as an easy corollary, acelebrated theorem of Beigel, Reingold, and Spielman that PP is closed underintersection, as well as a generalization of that theorem due to Fortnow andReingold. This illustrates that quantum computing can yield new and simplerproofs of major results about classical computation.'}
3.18e+03 pattern=  0 task=  8 (u=2809)  {'docs': u'Can NP-complete problems be solved efficiently in the physical universe? Isurvey proposals including soap bubbles, protein folding, quantum computing,quantum advice, quantum adiabatic algorithms, quantum-mechanicalnonlinearities, hidden variables, relativistic time dilation, analog computing,Malament-Hogarth spacetimes, quantum gravity, closed timelike curves, and"anthropic computing." The section on soap bubbles even includes some"experimental" results. While I do not believe that any of the proposals willlet us solve NP-complete problems efficiently, I argue that by studying them,we can learn something not only about computation but also about physics.'}
3.24e+03 pattern=  0 task=  9 (u=2809)  {'docs': u'Theoretical computer scientists have been debating the role of oracles sincethe 1970\'s. This paper illustrates both that oracles can give us nontrivialinsights about the barrier problems in circuit complexity, and that they neednot prevent us from trying to solve those problems.  First, we give an oracle relative to which PP has linear-sized circuits, byproving a new lower bound for perceptrons and low- degree thresholdpolynomials. This oracle settles a longstanding open question, and generalizesearlier results due to Beigel and to Buhrman, Fortnow, and Thierauf. Moreimportantly, it implies the first nonrelativizing separation of "traditional"complexity classes, as opposed to interactive proof classes such as MIP andMA-EXP. For Vinodchandran showed, by a nonrelativizing argument, that PP doesnot have circuits of size n^k for any fixed k. We present an alternative proofof this fact, which shows that PP does not even have quantum circuits of sizen^k with quantum advice. To our knowledge, this is the first nontrivial lowerbound on quantum circuit size.  Second, we study a beautiful algorithm of Bshouty et al. for learning Booleancircuits in ZPP^NP. We show that the NP queries in this algorithm cannot beparallelized by any relativizing technique, by giving an oracle relative towhich ZPP^||NP and even BPP^||NP have linear-size circuits. On the other hand,we also show that the NP queries could be parallelized if P=NP. Thus, classessuch as ZPP^||NP inhabit a "twilight zone," where we need to distinguishbetween relativizing and black-box techniques. Our results on this subject haveimplications for computational learning theory as well as for the circuitminimization problem.'}
1.91e+03 pattern=  0 task=  0 (u=2871)  {'docs': u'In a recent paper it was shown that No Free Lunch results hold for any subsetF of the set of all possible functions from a finite set X to a finite set Yiff F is closed under permutation of X. In this article, we prove that thenumber of those subsets can be neglected compared to the overall number ofpossible subsets. Further, we present some arguments why problem classesrelevant in practice are not likely to be closed under permutation.'}
2.49e+03 pattern=  0 task=  1 (u=2871)  {'docs': u'The sharpened No-Free-Lunch-theorem (NFL-theorem) states that the performanceof all optimization algorithms averaged over any finite set F of functions isequal if and only if F is closed under permutation (c.u.p.) and each targetfunction in F is equally likely. In this paper, we first summarize someconsequences of this theorem, which have been proven recently: The averagenumber of evaluations needed to find a desirable (e.g., optimal) solution canbe calculated; the number of subsets c.u.p. can be neglected compared to theoverall number of possible subsets; and problem classes relevant in practiceare not likely to be c.u.p. Second, as the main result, the NFL-theorem isextended. Necessary and sufficient conditions for NFL-results to hold are givenfor arbitrary, non-uniform distributions of target functions. This yields themost general NFL-theorem for optimization presented so far.'}
2.14e+03 pattern=  0 task=  0 (u=2872)  {'docs': u'Self-adaptation is used in all main paradigms of evolutionary computation toincrease efficiency. We claim that the basis of self-adaptation is the use ofneutrality. In the absence of external control neutrality allows a variation ofthe search distribution without the risk of fitness loss.'}
2.49e+03 pattern=  0 task=  0 (u=2873)  {'docs': u'We present Matrix Distributed Processing, a C++ library for fast developmentof efficient parallel algorithms. MDP is based on MPI and consists of acollection of C++ classes and functions such as lattice, site and field. Oncean algorithm is written using these components the algorithm is automaticallyparallel and no explicit call to communication functions is required. MDP isparticularly suitable for implementing parallel solvers for multi-dimensionaldifferential equations and mesh-like problems.'}
3.26e+03 pattern=  0 task=  1 (u=2873)  {'docs': u'Matrix Distributed Processing (MDP) is a C++ library for fast development ofefficient parallel algorithms. It constitues the core of FermiQCD. MDP enablesprogrammers to focus on algorithms, while parallelization is dealt withautomatically and transparently. Here we present a brief overview of MDP andexamples of applications in Computer Science (Cellular Automata), Engineering(PDE Solver) and Physics (Ising Model).'}
1.75e+03 pattern=  0 task=  0 (u=2891)  {'docs': u'In evolutionary algorithms, the fitness of a population increases with timeby mutating and recombining individuals and by a biased selection of more fitindividuals. The right selection pressure is critical in ensuring sufficientoptimization progress on the one hand and in preserving genetic diversity to beable to escape from local optima on the other. We propose a new selectionscheme, which is uniform in the fitness values. It generates selection pressuretowards sparsely populated fitness regions, not necessarily towards higherfitness, as is the case for all other selection schemes. We show that the newselection scheme can be much more effective than standard selection schemes.'}
2.2e+03 pattern=  0 task=  1 (u=2891)  {'docs': u"An algorithm $M$ is described that solves any well-defined problem $p$ asquickly as the fastest algorithm computing a solution to $p$, save for a factorof 5 and low-order additive terms. $M$ optimally distributes resources betweenthe execution of provably correct $p$-solving programs and an enumeration ofall proofs, including relevant proofs of program correctness and of time boundson program runtimes. $M$ avoids Blum's speed-up theorem by ignoring programswithout correctness proof. $M$ has broader applicability and can be faster thanLevin's universal search, the fastest method for inverting functions save for alarge multiplicative constant. An extension of Kolmogorov complexity and twonovel natural measures of function complexity are used to show that the mostefficient program computing some function $f$ is also among the shortestprograms provably computing $f$."}
2.58e+03 pattern=  0 task=  2 (u=2891)  {'docs': u'Given the joint chances of a pair of random variables one can computequantities of interest, like the mutual information. The Bayesian treatment ofunknown chances involves computing, from a second order prior distribution andthe data likelihood, a posterior distribution of the chances. A commontreatment of incomplete data is to assume ignorability and determine thechances by the expectation maximization (EM) algorithm. The two differentmethods above are well established but typically separated. This paper joinsthe two approaches in the case of Dirichlet priors, and derives efficientapproximations for the mean, mode and the (co)variance of the chances and themutual information. Furthermore, we prove the unimodality of the posteriordistribution, whence the important property of convergence of EM to the globalmaximum in the chosen framework. These results are applied to the problem ofselecting features for incremental learning and naive Bayes classification. Afast filter based on the distribution of mutual information is shown tooutperform the traditional filter based on empirical mutual information on anumber of incomplete real data sets.'}
2.84e+03 pattern=  0 task=  3 (u=2891)  {'docs': u'Mutual information is widely used, in a descriptive way, to measure thestochastic dependence of categorical random variables. In order to addressquestions such as the reliability of the descriptive value, one must considersample-to-population inferential approaches. This paper deals with theposterior distribution of mutual information, as obtained in a Bayesianframework by a second-order Dirichlet prior distribution. The exact analyticalexpression for the mean, and analytical approximations for the variance,skewness and kurtosis are derived. These approximations have a guaranteedaccuracy level of the order O(1/n^3), where n is the sample size. Leading orderapproximations for the mean and the variance are derived in the case ofincomplete samples. The derived analytical expressions allow the distributionof mutual information to be approximated reliably and quickly. In fact, thederived expressions can be computed with the same order of complexity neededfor descriptive mutual information. This makes the distribution of mutualinformation become a concrete alternative to descriptive mutual information inmany applications which would benefit from moving to the inductive side. Someof these prospective applications are discussed, and one of them, namelyfeature selection, is shown to perform significantly better when inductivemutual information is used.'}
3.1e+03 pattern=  0 task=  4 (u=2891)  {'docs': u'Given i.i.d. data from an unknown distribution, we consider the problem ofpredicting future items. An adaptive way to estimate the probability density isto recursively subdivide the domain to an appropriate data-dependentgranularity. A Bayesian would assign a data-independent prior probability to"subdivide", which leads to a prior over infinite(ly many) trees. We derive anexact, fast, and simple inference algorithm for such a prior, for the dataevidence, the predictive distribution, the effective model dimension, and otherquantities.'}
3.09e+03 pattern=  0 task=  0 (u=2904)  {'docs': u'We prove that every weighted graph contains a spanning tree subgraph ofaverage stretch O((log n log log n)^2). Moreover, we show how to construct sucha tree in time O(m log^2 n).'}
1.03e+03 pattern=  0 task=  0 (u=2994)  {'docs': u'We wish to tile a rectangle or a torus with only vertical and horizontal barsof a given length, such that the number of bars in every column and row equalsgiven numbers. We present results for particular instances and for a moregeneral problem, while leaving open the initial problem.'}
1.11e+03 pattern=  0 task=  1 (u=2994)  {'docs': u"We address a discrete tomography problem that arises in the study of theatomic structure of crystal lattices. A polyatomic structure T can be definedas an integer lattice in dimension D>=2, whose points may be occupied by $c$distinct types of atoms. To ``analyze'' T, we conduct ell measurements that wecall_discrete X-rays_. A discrete X-ray in direction xi determines the numberof atoms of each type on each line parallel to xi. Given ell such non-parallelX-rays, we wish to reconstruct T.  The complexity of the problem for c=1 (one atom type) has been completelydetermined by Gardner, Gritzmann and Prangenberg, who proved that the problemis NP-complete for any dimension D>=2 and ell>=3 non-parallel X-rays, and thatit can be solved in polynomial time otherwise.  The NP-completeness result above clearly extends to any c>=2, and thereforewhen studying the polyatomic case we can assume that ell=2. As shown in anotherarticle by the same authors, this problem is also NP-complete for c>=6 atoms,even for dimension D=2 and axis-parallel X-rays. They conjecture that theproblem remains NP-complete for c=3,4,5, although, as they point out, the proofidea does not seem to extend to c<=5.  We resolve the conjecture by proving that the problem is indeed NP-completefor c>=3 in 2D, even for axis-parallel X-rays. Our construction relies heavilyon some structure results for the realizations of 0-1 matrices with given rowand column sums."}
1.11e+03 pattern=  0 task=  2 (u=2994)  {'docs': u'Tomography is the area of reconstructing objects from projections. Here wewish to reconstruct a set of cells in a two dimensional grid, given the numberof cells in every row and column. The set is required to be an hv-convexpolyomino, that is all its cells must be connected and the cells in every rowand column must be consecutive. A simple, polynomial algorithm forreconstructing hv-convex polyominoes is provided, which is several orders ofmagnitudes faster than the best previously known algorithm from Barcucci et al.In addition, the problem of reconstructing a special class of centeredhv-convex polyominoes is addressed. (An object is centered if it contains a rowwhose length equals the total width of the object). It is shown that in thiscase the reconstruction problem can be solved in linear time.'}
2.32e+03 pattern=  0 task=  3 (u=2994)  {'docs': u'The model of cellular automata is fascinating because very simple local rulescan generate complex global behaviors. The relationship between local andglobal function is subject of many studies. We tackle this question by usingresults on communication complexity theory and, as a by-product, we provide(yet another) classification of cellular automata.'}
1.91e+03 pattern=  0 task=  0 (u=2995)  {'docs': u'Given a tiling of a 2D grid with several types of tiles, we can count forevery row and column how many tiles of each type it intersects. These numbersare called the_projections_. We are interested in the problem of reconstructinga tiling which has given projections. Some simple variants of this problem,involving tiles that are 1x1 or 1x2 rectangles, have been studied in the past,and were proved to be either solvable in polynomial time or NP-complete. Inthis note we make progress toward a comprehensive classification of varioustiling reconstruction problems, by proving NP-completeness results for severalsets of tiles.'}
3.06e+03 pattern=  0 task=  1 (u=2995)  {'docs': u"We study the problem of scheduling equal-length jobs with release times anddeadlines, where the objective is to maximize the number of completed jobs.Preemptions are not allowed. In Graham's notation, the problem is described as1|r_j;p_j=p|\\sum U_j. We give the following results: (1) We show that the oftencited algorithm by Carlier from 1981 is not correct. (2) We give an algorithmfor this problem with running time O(n^5)."}
3.25e+03 pattern=  0 task=  2 (u=2995)  {'docs': u"Following Mettu and Plaxton, we study online algorithms for the k-mediansproblem. Such an algorithm must produce a nested sequence F_1\\subseteqF_2\\subseteq...\\subseteq F_n of sets of facilities. Mettu and Plaxton show thatonline metric medians has a (roughly) 40-competitive deterministicpolynomial-time algorithm. We give improved algorithms, including a(24+\\epsilon)-competitive deterministic polynomial-time algorithm and a5.44-competitive, randomized, non-polynomial-time algorithm.  We also consider the competitive ratio with respect to size. An algorithm iss-size-competitive if, for each k, the cost of F_k is at most the minimum costof any set of k facilities, while the size of F_k is at most s k. We presentoptimally competitive algorithms for this problem.  Our proofs reduce online medians to the following online bidding problem:faced with some unknown threshold T>0, an algorithm must submit ``bids'' b>0until it submits a bid as large as T. The algorithm pays the sum of its bids.We describe optimally competitive algorithms for online bidding.  Our results on cost-competitive online medians extend to approximately metricdistance functions, online fractional medians, and online bicriteriaapproximation."}
3.25e+03 pattern=  0 task=  3 (u=2995)  {'docs': u'The Reverse Greedy algorithm (RGreedy) for the k-median problem works asfollows. It starts by placing facilities on all nodes. At each step, it removesa facility to minimize the resulting total distance from the customers to theremaining facilities. It stops when k facilities remain. We prove that, if thedistance function is metric, then the approximation ratio of RGreedy is between?(log n/ log log n) and O(log n).'}
1.15e+03 pattern=  0 task=  0 (u=3076)  {'docs': u"A decade ago, a beautiful paper by Wagner developed a ``toolkit'' that incertain cases allows one to prove problems hard for parallel access to NP.However, the problems his toolkit applies to most directly are not overlynatural. During the past year, problems that previously were known only to beNP-hard or coNP-hard have been shown to be hard even for the class of setssolvable via parallel access to NP. Many of these problems are longstanding andextremely natural, such as the Minimum Equivalent Expression problem (which wasthe original motivation for creating the polynomial hierarchy), the problem ofdetermining the winner in the election system introduced by Lewis Carroll in1876, and the problem of determining on which inputs heuristic algorithmsperform well. In the present article, we survey this recent progress in raisinglower bounds."}
1.22e+03 pattern=  0 task=  1 (u=3076)  {'docs': u"Downward translation of equality refers to cases where a collapse of somepair of complexity classes would induce a collapse of some other pair ofcomplexity classes that (a priori) one expects are smaller. Recently, the firstdownward translation of equality was obtained that applied to the polynomialhierarchy-in particular, to bounded access to its levels [cs.CC/9910007]. Inthis paper, we provide a much broader downward translation that extends notonly that downward translation but also that translation's elegant enhancementby Buhrman and Fortnow. Our work also sheds light on previous research on thestructure of refined polynomial hierarchies, and strengthens the connectionbetween the collapse of bounded query hierarchies and the collapse of thepolynomial hierarchy."}
1.22e+03 pattern=  0 task=  2 (u=3076)  {'docs': u'During the past decade, nine papers have obtained increasingly strongconsequences from the assumption that boolean or bounded-query hierarchiescollapse. The final four papers of this nine-paper progression actually achievedownward collapse---that is, they show that high-level collapses inducecollapses at (what beforehand were thought to be) lower complexity levels. Forexample, for each $k\\geq 2$ it is now known that if $\\psigkone=\\psigktwo$ then$\\ph=\\sigmak$. This article surveys the history, the results, and thetechnique---the so-called easy-hard method---of these nine papers.'}
1.22e+03 pattern=  0 task=  1 (u=3076)  {'docs': u'Hemaspaandra, Hempel, and Wechsung [cs.CC/9909020] raised the followingquestions: If one is allowed one question to each of two different informationsources, does the order in which one asks the questions affect the class ofproblems that one can solve with the given access? If so, which order yieldsthe greater computational power?  The answers to these questions have been learned-inasfar as they can belearned without resolving whether or not the polynomial hierarchy collapses-forboth the polynomial hierarchy and the boolean hierarchy. In the polynomialhierarchy, query order never matters. In the boolean hierarchy, query ordersometimes does not matter and, unless the polynomial hierarchy collapses,sometimes does matter. Furthermore, the study of query order has yieldeddividends in seemingly unrelated areas, such as bottleneck computations anddownward translation of equality.  In this article, we present some of the central results on query order. Thearticle is written in such a way as to encourage the reader to try his or herown hand at proving some of these results. We also give literature pointers tothe quickly growing set of related results and applications.'}
1.96e+03 pattern=  0 task=  3 (u=3076)  {'docs': u'For both the edge deletion heuristic and the maximum-degree greedy heuristic,we study the problem of recognizing those graphs for which that heuristic canapproximate the size of a minimum vertex cover within a constant factor of r,where r is a fixed rational number. Our main results are that these problemsare complete for the class of problems solvable via parallel access to NP. Toachieve these main results, we also show that the restriction of the vertexcover problem to those graphs for which either of these heuristics can find anoptimal solution remains NP-hard.'}
2.58e+03 pattern=  0 task=  4 (u=3076)  {'docs': u'The even cycle problem for both undirected and directed graphs has been thetopic of intense research in the last decade. In this paper, we study thecomputational complexity of \\emph{cycle length modularity problems}. Roughlyspeaking, in a cycle length modularity problem, given an input (undirected ordirected) graph, one has to determine whether the graph has a cycle $C$ of aspecific length (or one of several different lengths), modulo a fixed integer.We denote the two families (one for undirected graphs and one for directedgraphs) of problems by $(S,m)\\hbox{-}{\\rm UC}$ and $(S,m)\\hbox{-}{\\rm DC}$,where $m \\in \\mathcal{N}$ and $S \\subseteq \\{0,1, ..., m-1\\}$.$(S,m)\\hbox{-}{\\rm UC}$ (respectively, $(S,m)\\hbox{-}{\\rm DC}$) is defined asfollows: Given an undirected (respectively, directed) graph $G$, is there acycle in $G$ whose length, modulo $m$, is a member of $S$? In this paper, wefully classify (i.e., as either polynomial-time solvable or as ${\\rmNP}$-complete) each problem $(S,m)\\hbox{-}{\\rm UC}$ such that $0 \\in S$ andeach problem $(S,m)\\hbox{-}{\\rm DC}$ such that $0 \\notin S$. We also give asufficient condition on $S$ and $m$ for the following problem to bepolynomial-time computable: $(S,m)\\hbox{-}{\\rm UC}$ such that $0 \\notin S$.'}
2.82e+03 pattern=  0 task=  0 (u=3080)  {'docs': u'This paper presents a parameter-less optimization framework that uses theextended compact genetic algorithm (ECGA) and iterated local search (ILS), butis not restricted to these algorithms. The presented optimization algorithm(ILS+ECGA) comes as an extension of the parameter-less genetic algorithm (GA),where the parameters of a selecto-recombinative GA are eliminated. The approachthat we propose is tested on several well known problems. In the absence ofdomain knowledge, it is shown that ILS+ECGA is a robust and easy-to-useoptimization method.'}
2.82e+03 pattern=  0 task=  0 (u=3081)  {'docs': u'This paper makes a number of connections between life and various facets ofgenetic and evolutionary algorithms research. Specifically, it addresses thetopics of adaptation, multiobjective optimization, decision making, deception,and search operators, among others. It argues that human life, from birth todeath, is an adaptive or dynamic optimization problem where people arecontinuously searching for happiness. More important, the paper speculates thatgenetic algorithms can be used as a source of inspiration for helping peoplemake decisions in their everyday life.'}
2.82e+03 pattern=  0 task=  1 (u=3081)  {'docs': u'This paper presents an architecture which is suitable for a massiveparallelization of the compact genetic algorithm. The resulting scheme hasthree major advantages. First, it has low synchronization costs. Second, it isfault tolerant, and third, it is scalable.  The paper argues that the benefits that can be obtained with the proposedapproach is potentially higher than those obtained with traditional parallelgenetic algorithms. In addition, the ideas suggested in the paper may also berelevant towards parallelizing more complex probabilistic model buildinggenetic algorithms.'}
3.21e+03 pattern=  0 task=  0 (u=3098)  {'docs': u'The zeta-dimension of a set A of positive integers is the infimum s such thatthe sum of the reciprocals of the s-th powers of the elements of A is finite.  Zeta-dimension serves as a fractal dimension on the positive integers thatextends naturally usefully to discrete lattices such as the set of all integerlattice points in d-dimensional space.  This paper reviews the origins of zeta-dimension (which date to theeighteenth and nineteenth centuries) and develops its basic theory, withparticular attention to its relationship with algorithmic information theory.New results presented include extended connections between zeta-dimension andclassical fractal dimensions, a gale characterization of zeta-dimension, and atheorem on the zeta-dimensions of pointwise sums and products of sets ofpositive integers.'}
3.24e+03 pattern=  0 task=  1 (u=3098)  {'docs': u'This paper develops the theory of pushdown dimension and explores itsrelationship with finite-state dimension. Pushdown dimension is triviallybounded above by finite-state dimension for all sequences, since a pushdowngambler can simulate any finite-state gambler. We show that for every rational0 < d < 1, there exists a sequence with finite-state dimension d whose pushdowndimension is at most d/2. This establishes a quantitative analogue of thewell-known fact that pushdown automata decide strictly more languages thanfinite automata.'}
2.42e+03 pattern=  0 task=  0 (u=3134)  {'docs': u'We develop a purely set-theoretic formalism for binary trees and binarygraphs. We define a category of binary automata, and display it as a fibredcategory over the category of binary graphs. We also relate the notion ofbinary graphs to transition systems, which arise in the theory of concurrentcomputing.'}
2.57e+03 pattern=  0 task=  0 (u=3151)  {'docs': u'Strand spaces are a popular framework for the analysis of security protocols.Strand spaces have some similarities to a formalism used successfully to modelprotocols for distributed systems, namely multi-agent systems. We explore theexact relationship between these two frameworks here. It turns out that a keydifference is the handling of agents, which are unspecified in strand spacesand explicit in multi-agent systems. We provide a family of translations fromstrand spaces to multi-agent systems parameterized by the choice of agents inthe strand space. We also show that not every multi-agent system of interestcan be expressed as a strand space. This reveals a lack of expressiveness inthe strand-space framework that can be characterized by our translation. Tohighlight this lack of expressiveness, we show one simple way in which strandspaces can be extended to model more systems.'}
2.61e+03 pattern=  0 task=  1 (u=3151)  {'docs': u'We present a propositional logic %which can be used to reason about theuncertainty of events, where the uncertainty is modeled by a set of probabilitymeasures assigning an interval of probability to each event. We give a soundand complete axiomatization for the logic, and show that the satisfiabilityproblem is NP-complete, no harder than satisfiability for propositional logic.'}
2.75e+03 pattern=  0 task=  2 (u=3151)  {'docs': u'Expectation is a central notion in probability theory. The notion ofexpectation also makes sense for other notions of uncertainty. We introduce apropositional logic for reasoning about expectation, where the semanticsdepends on the underlying representation of uncertainty. We give sound andcomplete axiomatizations for the logic in the case that the underlyingrepresentation is (a) probability, (b) sets of probability measures, (c) belieffunctions, and (d) possibility measures. We show that this logic is moreexpressive than the corresponding logic for reasoning about likelihood in thecase of sets of probability measures, but equi-expressive in the case ofprobability, belief, and possibility. Finally, we show that satisfiability forthese logics is NP-complete, no harder than satisfiability for propositionallogic.'}
2.82e+03 pattern=  0 task=  3 (u=3151)  {'docs': u"We provide a framework for reasoning about information-hiding requirements inmultiagent systems and for reasoning about anonymity in particular. Ourframework employs the modal logic of knowledge within the context of the runsand systems framework, much in the spirit of our earlier work on secrecy[Halpern and O'Neill 2002]. We give several definitions of anonymity withrespect to agents, actions, and observers in multiagent systems, and we relateour definitions of anonymity to other definitions of information hiding, suchas secrecy. We also give probabilistic definitions of anonymity that are ableto quantify an observer s uncertainty about the state of the system. Finally,we relate our definitions of anonymity to other formalizations of anonymity andinformation hiding, including definitions of anonymity in the process algebraCSP and definitions of information hiding using function views."}
2.91e+03 pattern=  0 task=  4 (u=3151)  {'docs': u'We introduce a logic for reasoning about evidence that essentially viewsevidence as a function from prior beliefs (before making an observation) toposterior beliefs (after making the observation). We provide a sound andcomplete axiomatization for the logic, and consider the complexity of thedecision problem. Although the reasoning in the logic is mainly propositional,we allow variables representing numbers and quantification over them. Thisexpressive power seems necessary to capture important properties of evidence.'}
3.06e+03 pattern=  0 task=  5 (u=3151)  {'docs': u"A careful analysis of conditioning in the Sleeping Beauty problem is done,using the formal model for reasoning about knowledge and probability developedby Halpern and Tuttle. While the Sleeping Beauty problem has been viewed asrevealing problems with conditioning in the presence of imperfect recall, theanalysis done here reveals that the problems are not so much due to imperfectrecall as to asynchrony. The implications of this analysis for van Fraassen'sReflection Principle and Savage's Sure-Thing Principle are considered."}
3.06e+03 pattern=  0 task=  6 (u=3151)  {'docs': u'There are many examples in the literature that suggest thatindistinguishability is intransitive, despite the fact that theindistinguishability relation is typically taken to be an equivalence relation(and thus transitive). It is shown that if the uncertainty perception and thequestion of when an agent reports that two things are indistinguishable areboth carefully modeled, the problems disappear, and indistinguishability canindeed be taken to be an equivalence relation. Moreover, this model alsosuggests a logic of vagueness that seems to solve many of the problems relatedto vagueness discussed in the philosophical literature. In particular, it isshown here how the logic can handle the sorites paradox.'}
3.13e+03 pattern=  0 task=  0 (u=3192)  {'docs': u"Combining two or more items and selling them as one good, a practice calledbundling, can be a very effective strategy for reducing the costs of producing,marketing, and selling goods. In this paper, we consider a form of multi-issuenegotiation where a shop negotiates both the contents and the price of bundlesof goods with his customers. We present some key insights about, as well as atechnique for, locating mutually beneficial alternatives to the bundlecurrently under negotiation. The essence of our approach lies in combininghistorical sales data, condensed into aggregate knowledge, with current dataabout the ongoing negotiation process, to exploit these insights. Inparticular, when negotiating a given bundle of goods with a customer, the shopanalyzes the sequence of the customer's offers to determine the progress in thenegotiation process. In addition, it uses aggregate knowledge concerningcustomers' valuations of goods in general. We show how the shop can use thesetwo sources of data to locate promising alternatives to the current bundle.When the current negotiation's progress slows down, the shop may suggest themost promising of those alternatives and, depending on the customer's response,continue negotiating about the alternative bundle, or propose anotheralternative. Extensive computer simulation experiments show that our approachincreases the speed with which deals are reached, as well as the number andquality of the deals reached, as compared to a benchmark. In addition, we showthat the performance of our system is robust to a variety of changes in thenegotiation strategies employed by the customers."}
3.05e+03 pattern=  0 task=  0 (u=3195)  {'docs': u'Normal forms for logic programs under stable/answer set semantics areintroduced. We argue that these forms can simplify the study of programproperties, mainly consistency. The first normal form, called the {\\em kernel}of the program, is useful for studying existence and number of answer sets. Akernel program is composed of the atoms which are undefined in the Well-foundedsemantics, which are those that directly affect the existence of answer sets.The body of rules is composed of negative literals only. Thus, the kernel formtends to be significantly more compact than other formulations. Also, it ispossible to check consistency of kernel programs in terms of colorings of theExtended Dependency Graph program representation which we previously developed.The second normal form is called {\\em 3-kernel.} A 3-kernel program is composedof the atoms which are undefined in the Well-founded semantics. Rules in3-kernel programs have at most two conditions, and each rule either belongs toa cycle, or defines a connection between cycles. 3-kernel programs may havepositive conditions. The 3-kernel normal form is very useful for the staticanalysis of program consistency, i.e., the syntactic characterization ofexistence of answer sets. This result can be obtained thanks to a novelgraph-like representation of programs, called Cycle Graph which presented inthe companion article \\cite{Cos04b}.'}
3.13e+03 pattern=  0 task=  1 (u=3195)  {'docs': u'This paper introduces a fundamental result, which is relevant for Answer Setprogramming, and planning. For the first time since the definition of thestable model semantics, the class of logic programs for which a stable modelexists is given a syntactic characterization. This condition may have apractical importance both for defining new algorithms for checking consistencyand computing answer sets, and for improving the existing systems. The approachof this paper is to introduce a new canonical form (to which any logic programcan be reduced to), to focus the attention on cyclic dependencies. Thetechnical result is then given in terms of programs in canonical form(canonical programs), without loss of generality. The result is based onidentifying the cycles contained in the program, showing that stable models ofthe overall program are composed of stable models of suitable sub-programs,corresponding to the cycles, and on defining the Cycle Graph. Each vertex ofthis graph corresponds to one cycle, and each edge corresponds to onehandle,which is a literal containing an atom that, occurring in both cycles, actuallydetermines a connection between them. In fact, the truth value of the handle inthe cycle where it appears as the head of a rule, influences the truth value ofthe atoms of the cycle(s) where it occurs in the body. We can thereforeintroduce the concept of a handle path, connecting different cycles. If forevery odd cycle we can find a handle path with certain properties, then theexistence of stable model is guaranteed.'}
2.52e+03 pattern=  0 task=  0 (u=3372)  {'docs': u'Recent results established exponential lower bounds for the length of anyResolution proof for the weak pigeonhole principle. More formally, it wasproved that any Resolution proof for the weak pigeonhole principle, with $n$holes and any number of pigeons, is of length $\\Omega(2^{n^{\\epsilon}})$, (fora constant $\\epsilon = 1/3$). One corollary is that certain propositionalformulations of the statement $P \\ne NP$ do not have short Resolution proofs.After a short introduction to the problem of $P \\ne NP$ and to the researcharea of propositional proof complexity, I will discuss the above mentionedlower bounds for the weak pigeonhole principle and the connections to thehardness of proving $P \\ne NP$.'}
2.52e+03 pattern=  0 task=  0 (u=3373)  {'docs': u'We survey recent developments in the study of probabilistic complexityclasses. While the evidence seems to support the conjecture that probabilismcan be deterministically simulated with relatively low overhead, i.e., that$P=BPP$, it also indicates that this may be a difficult question to resolve. Infact, proving that probabilistic algorithms have non-trivial deterministicsimulations is basically equivalent to proving circuit lower bounds, either inthe algebraic or Boolean models.'}
2.62e+03 pattern=  0 task=  1 (u=3373)  {'docs': u'We show that constant-depth Frege systems with counting axioms modulo $m$polynomially simulate Nullstellensatz refutations modulo $m$. Central to thisis a new definition of reducibility from formulas to systems of polynomialswith the property that, for most previously studied translations of formulas tosystems of polynomials, a formula reduces to its translation. When combinedwith a previous result of the authors, this establishes the first sizeseparation between Nullstellensatz and polynomial calculus refutations. We alsoobtain new, small refutations for certain CNFs by constant-depth Frege systemswith counting axioms.'}
  980 pattern=  0 task=  0 (u=3378)  {'docs': u'We present here a generalization of the work done by Rabin and Ben-Or. Wegive a protocol for multiparty computation which tolerates any Q^2 activeadversary structure based on the existence of a broadcast channel, securecommunication between each pair of participants, and a monotone span programwith multiplication tolerating the structure. The secrecy achieved isunconditional although we allow an exponentially small probability of error.This is possible due to a protocol for computing the product of two valuesalready shared by means of a homomorphic commitment scheme which appearedoriginally in a paper of Chaum, Evertse and van de Graaf.'}
3.27e+03 pattern=  0 task=  0 (u=3702)  {'docs': u'This paper is mainly concerned with the relation-algebraical aspects of thewell-known Region Connection Calculus (RCC). We show that the contact relationalgebra (CRA) of certain RCC model is not atomic complete and hence infinite.So in general an extensional composition table for the RCC cannot be obtainedby simply refining the RCC8 relations. After having shown that each RCC modelis a consistent model of the RCC11 CT, we give an exhaustive investigationabout extensional interpretation of the RCC11 CT. More important, we show thecomplemented closed disk algebra is a representation for the relation algebradetermined by the RCC11 table. The domain of this algebra contains two classesof regions, the closed disks and closures of their complements in the realplane.'}
2.07e+03 pattern=  0 task=  0 (u=3747)  {'docs': u"A secure timeline is a tamper-evident historic record of the states throughwhich a system goes throughout its operational history. Secure timelines canhelp us reason about the temporal ordering of system states in a provablemanner. We extend secure timelines to encompass multiple, mutually distrustfulservices, using timeline entanglement. Timeline entanglement associatesdisparate timelines maintained at independent systems, by linking undeniablythe past of one timeline to the future of another. Timeline entanglement is asound method to map a time step in the history of one service onto the timelineof another, and helps clients of entangled services to get persistent temporalproofs for services rendered that survive the demise or non-cooperation of theoriginating service. In this paper we present the design and implementation ofTimeweave, our service development framework for timeline entanglement based ontwo novel disk-based authenticated data structures. We evaluate Timeweave'sperformance characteristics and show that it can be efficiently deployed in aloosely-coupled distributed system of a few hundred services with overhead ofroughly 2-8% of the processing resources of a PC-grade system."}
2.33e+03 pattern=  0 task=  1 (u=3747)  {'docs': u"People change the identifiers through which they are reachable online as theychange jobs or residences or Internet service providers. This kind of personalmobility makes reaching people online error-prone. As people move, they do notalways know who or what has cached their now obsolete identifiers so as toinform them of the move. Use of these old identifiers can cause deliveryfailure of important messages, or worse, may cause delivery of messages tounintended recipients. For example, a sensitive email message sent to my nowobsolete work address at a former place of employment may reach my unfriendlyformer boss instead of me.  In this paper we describe HINTS, a historic name-trail service. This serviceprovides a persistent way to name willing participants online using today'stransient online identifiers. HINTS accomplishes this by connecting togetherthe names a person uses along with the times during which those names werevalid for the person, thus giving people control over the historic use of theirnames. A correspondent who wishes to reach a mobile person can use an obsoleteonline name for that person, qualified with a time at which the online name wassuccessfully used; HINTS resolves this historic name to a current valid onlineidentifier for the intended recipient, if that recipient has chosen to leave aname trail in HINTS."}
2.44e+03 pattern=  0 task=  2 (u=3747)  {'docs': u"In this work we describe, design and analyze the security of atamper-evident, append-only data structure for maintaining secure datasequences in a loosely coupled distributed system where individual systemcomponents may be mutually distrustful. The resulting data structure, called anAuthenticated Append-Only Skip List (AASL), allows its maintainers to produceone-way digests of the entire data sequence, which they can publish to othersas a commitment on the contents and order of the sequence. The maintainer canproduce efficiently succinct proofs that authenticate a particular datum in aparticular position of the data sequence against a published digest. AASLs aresecure against tampering even by malicious data structure maintainers. First,we show that a maintainer cannot ``invent'' and authenticate data elements forthe AASL after he has committed to the structure. Second, he cannot equivocateby being able to prove conflicting facts about a particular position of thedata sequence. This is the case even when the data sequence grows with time andits maintainer publishes successive commitments at times of his own choosing.  AASLs can be invaluable in reasoning about the integrity of system logsmaintained by untrusted components of a loosely-coupled distributed system."}
2.07e+03 pattern=  0 task=  0 (u=3760)  {'docs': u'This paper describes the LDL++ system and the research advances that haveenabled its design and development. We begin by discussing the new nonmonotonicand nondeterministic constructs that extend the functionality of the LDL++language, while preserving its model-theoretic and fixpoint semantics. Then, wedescribe the execution model and the open architecture designed to supportthese new constructs and to facilitate the integration with existing DBMSs andapplications. Finally, we describe the lessons learned by using LDL++ onvarious tested applications, such as middleware and datamining.'}
2.08e+03 pattern=  0 task=  0 (u=3770)  {'docs': u'Recently the problem of indexing and locating content in peer-to-peernetworks has received much attention. Previous work suggests caching indexentries at intermediate nodes that lie on the paths taken by search queries,but until now there has been little focus on how to maintain these intermediatecaches. This paper proposes CUP, a new comprehensive architecture forControlled Update Propagation in peer-to-peer networks. CUP asynchronouslybuilds caches of index entries while answering search queries. It thenpropagates updates of index entries to maintain these caches. Under unfavorableconditions, when compared with standard caching based on expiration times, CUPreduces the average miss latency by as much as a factor of three. Underfavorable conditions, CUP can reduce the average miss latency by more than afactor of ten.  CUP refreshes intermediate caches, reduces query latency, and reduces networkload by coalescing bursts of queries for the same item. CUP controls andconfines propagation to updates whose cost is likely to be recovered bysubsequent queries. CUP gives peer-to-peer nodes the flexibility to use theirown incentive-based policies to determine when to receive and when to propagateupdates. Finally, the small propagation overhead incurred by CUP is more thancompensated for by its savings in cache misses.'}
2.3e+03 pattern=  0 task=  1 (u=3770)  {'docs': u'This paper studies the problem of load-balancing the demand for content in apeer-to-peer network across heterogeneous peer nodes that hold replicas of thecontent. Previous decentralized load balancing techniques in distributedsystems base their decisions on periodic updates containing information aboutload or available capacity observed at the serving entities. We show that thesetechniques do not work well in the peer-to-peer context; either they do notaddress peer node heterogeneity, or they suffer from significant loadoscillations. We propose a new decentralized algorithm, Max-Cap, based on themaximum inherent capacities of the replica nodes and show that unlike previousalgorithms, it is not tied to the timeliness or frequency of updates. Yet,Max-Cap can handle the heterogeneity of a peer-to-peer environment withoutsuffering from load oscillations.'}
2.72e+03 pattern=  0 task=  2 (u=3770)  {'docs': u'In the hope of stimulating discussion, we present a heuristic decision treethat designers can use to judge the likely suitability of a P2P architecturefor their applications. It is based on the characteristics of a wide range ofP2P systems from the literature, both proposed and deployed.'}
2.86e+03 pattern=  0 task=  0 (u=3845)  {'docs': u'We review existing approaches to mathematical modeling and analysis ofmulti-agent systems in which complex collective behavior arises out of localinteractions between many simple agents. Though the behavior of an individualagent can be considered to be stochastic and unpredictable, the collectivebehavior of such systems can have a simple probabilistic description. We showthat a class of mathematical models that describe the dynamics of collectivebehavior of multi-agent systems can be written down from the details of theindividual agent controller. The models are valid for Markov or memorylessagents, in which each agents future state depends only on its present state andnot any of the past states. We illustrate the approach by analyzing in detailapplications from the robotics domain: collaboration and foraging in groups ofrobots.'}
2.96e+03 pattern=  0 task=  0 (u=3869)  {'docs': u'Recently, Jadbabaie, Lin, and Morse (IEEE TAC, 48(6)2003:988-1001) offered amathematical analysis of the discrete time model of groups of mobile autonomousagents raised by Vicsek et al. in 1995. In their paper, Jadbabaie et al. showedthat all agents shall move in the same heading, provided that these agents areperiodically linked together. This paper sharpens this result by showing thatcoordination will be reached under a very weak condition that requires allagents are finally linked together. This condition is also strictly weaker thanthe one Jadbabaie et al. desired.'}
3.27e+03 pattern=  0 task=  0 (u=3883)  {'docs': u'The general intractability of the constraint satisfaction problem hasmotivated the study of restrictions on this problem that permit polynomial-timesolvability. One major line of work has focused on structural restrictions,which arise from restricting the interaction among constraint scopes. In thispaper, we engage in a mathematical investigation of generalized hypertreewidth, a structural measure that has up to recently eluded study. We obtain anumber of computational results, including a simple proof of the tractabilityof CSP instances having bounded generalized hypertree width.'}
2.75e+03 pattern=  0 task=  0 (u=3904)  {'docs': u'Object oriented constraint programs (OOCPs) emerge as a leading evolution ofconstraint programming and artificial intelligence, first applied to a range ofindustrial applications called configuration problems. The rich variety oftechnical approaches to solving configuration problems (CLP(FD), CC(FD), DCSP,Terminological systems, constraint programs with set variables ...) is a sourceof difficulty. No universally accepted formal language exists for communicatingabout OOCPs, which makes the comparison of systems difficult. We present here aZ based specification of OOCPs which avoids the falltrap of hidden objectsemantics. The object system is part of the specification, and captures all ofthe most advanced notions from the object oriented modeling standard UML. Thepaper illustrates these issues and the conciseness and precision of Z by thespecification of a working OOCP that solves an historical AI problem : parsinga context free grammar. Being written in Z, an OOCP specification also supportsformal proofs. The whole builds the foundation of an adaptative and evolvingframework for communicating about constrained object models and programs.'}
2.42e+03 pattern=  0 task=  0 (u=3991)  {'docs': u'An analysis of the average properties of a local search resolution procedurefor the satisfaction of random Boolean constraints is presented. Depending onthe ratio alpha of constraints per variable, resolution takes a time T_resgrowing linearly (T_res \\sim tau(alpha) N, alpha < alpha_d) or exponentially(T_res \\sim exp(N zeta(alpha)), alpha > alpha_d) with the size N of theinstance. The relaxation time tau(alpha) in the linear phase is calculatedthrough a systematic expansion scheme based on a quantum formulation of theevolution operator. For alpha > alpha_d, the system is trapped in somemetastable state, and resolution occurs from escape from this state throughcrossing of a large barrier. An annealed calculation of the height zeta(alpha)of this barrier is proposed. The polynomial/exponentiel cross-over alpha_d isnot related to the onset of clustering among solutions.'}
3.19e+03 pattern=  0 task=  0 (u=4061)  {'docs': u'The speech code is a vehicle of language: it defines a set of forms used by acommunity to carry information. Such a code is necessary to support thelinguistic interactions that allow humans to communicate. How then may a speechcode be formed prior to the existence of linguistic interactions? Moreover, thehuman speech code is discrete and compositional, shared by all the individualsof a community but different across communities, and phoneme inventories arecharacterized by statistical regularities. How can a speech code with theseproperties form? We try to approach these questions in the paper, using the"methodology of the artificial". We build a society of artificial agents, anddetail a mechanism that shows the formation of a discrete speech code withoutpre-supposing the existence of linguistic capacities or of coordinatedinteractions. The mechanism is based on a low-level model of sensory-motorinteractions. We show that the integration of certain very simple and nonlanguage-specific neural devices leads to the formation of a speech code thathas properties similar to the human speech code. This result relies on theself-organizing properties of a generic coupling between perception andproduction within agents, and on the interactions between agents. Theartificial system helps us to develop better intuitions on how speech mighthave appeared, by showing how self-organization might have helped naturalselection to find speech.'}
3.17e+03 pattern=  0 task=  0 (u=4160)  {'docs': u"We discuss stability for a class of learning algorithms with respect to noisylabels. The algorithms we consider are for regression, and they involve theminimization of regularized risk functionals, such as L(f) := 1/N sum_i(f(x_i)-y_i)^2+ lambda ||f||_H^2. We shall call the algorithm `stable' if, wheny_i is a noisy version of f*(x_i) for some function f* in H, the output of thealgorithm converges to f* as the regularization term and noise simultaneouslyvanish. We consider two flavors of this problem, one where a data set of Npoints remains fixed, and the other where N -> infinity. For the case where N-> infinity, we give conditions for convergence to f_E (the function which isthe expectation of y(x) for each x), as lambda -> 0. For the fixed N case, wedescribe the limiting 'non-noisy', 'non-regularized' function f*, and giveconditions for convergence. In the process, we develop a set of tools fordealing with functionals such as L(f), which are applicable to many otherproblems in learning theory."}
3.24e+03 pattern=  0 task=  0 (u=4218)  {'docs': u'An important task for Homeland Security is the prediction of threatvulnerabilities, such as through the detection of relationships betweenseemingly disjoint entities. A structure used for this task is a "semanticgraph", also known as a "relational data graph" or an "attributed relationalgraph". These graphs encode relationships as "typed" links between a pair of"typed" nodes. Indeed, semantic graphs are very similar to semantic networksused in AI. The node and link types are related through an ontology graph (alsoknown as a schema). Furthermore, each node has a set of attributes associatedwith it (e.g., "age" may be an attribute of a node of type "person").Unfortunately, the selection of types and attributes for both nodes and linksdepends on human expertise and is somewhat subjective and even arbitrary. Thissubjectiveness introduces biases into any algorithm that operates on semanticgraphs. Here, we raise some knowledge representation issues for semantic graphsand provide some possible solutions using recently developed ideas in the fieldof complex networks. In particular, we use the concept of transitivity toevaluate the relevance of individual links in the semantic graph for detectingrelationships. We also propose new statistical measures for semantic graphs andillustrate these semantic measures on graphs constructed from movies andterrorism data.'}
  957 pattern=  2 task=  0 (u=4224)  {'docs': u'The universal object oriented languages made programming more simple andefficient. In the article is considered possibilities of using similar methodsin computer algebra. A clear and powerful universal language is useful ifparticular problem was not implemented in standard software packages likeREDUCE, MATHEMATICA, etc. and if the using of internal programming languages ofthe packages looks not very efficient.  Functional languages like LISP had some advantages and traditions foralgebraic and symbolic manipulations. Functional and object orientedprogramming are not incompatible ones. An extension of the model of an objectfor manipulation with pure functions and algebraic expressions is considered.'}
2.94e+03 pattern=  0 task=  1 (u=4224)  {'docs': u'In recent work [quant-ph/0405174] by Schumacher and Werner was discussed anabstract algebraic approach to a model of reversible quantum cellular automata(CA) on a lattice. It was used special model of CA based on partitioning schemeand so there is a question about quantum CA derived from more general, standardmodel of classical CA. In present work is considered an approach to definitionof a scheme with "history", valid for quantization both irreversible andreversible classical CA directly using local transition rules. It is usedlanguage of vectors in Hilbert spaces instead of C*-algebras, but results maybe compared in some cases. Finally, the quantum lattice gases, quantum walk and"bots" are also discussed briefly.'}
3.11e+03 pattern=  0 task=  0 (u=4240)  {'docs': u'A widely adopted approach to solving constraint satisfaction problemscombines systematic tree search with constraint propagation for pruning thesearch space. Constraint propagation is performed by propagators implementing acertain notion of consistency. Bounds consistency is the method of choice forbuilding propagators for arithmetic constraints and several global constraintsin the finite integer domain. However, there has been some confusion in thedefinition of bounds consistency. In this paper we clarify the differences andsimilarities among the three commonly used notions of bounds consistency.'}
2.72e+03 pattern=  0 task=  0 (u=4243)  {'docs': u"In this tool demonstration, we give an overview of the Chameleon typedebugger. The type debugger's primary use is to identify locations within asource program which are involved in a type error. By further examining these(potentially) problematic program locations, users gain a better understandingof their program and are able to work towards the actual mistake which was thecause of the type error. The debugger is interactive, allowing the user toprovide additional information to narrow down the search space. One of thenovel aspects of the debugger is the ability to explain erroneous-lookingtypes. In the event that an unexpected type is inferred, the debugger canhighlight program locations which contributed to that result. Furthermore, dueto the flexible constraint-based foundation that the debugger is built upon, itcan naturally handle advanced type system features such as Haskell's typeclasses and functional dependencies."}
2.23e+03 pattern=  0 task=  0 (u=4257)  {'docs': u'In this paper, we will formalize the method of dual fitting and the idea offactor-revealing LP. This combination is used to design and analyze two greedyalgorithms for the metric uncapacitated facility location problem. Theirapproximation factors are 1.861 and 1.61, with running times of O(mlog m) andO(n^3), respectively, where n is the total number of vertices and m is thenumber of edges in the underlying complete bipartite graph between cities andfacilities. The algorithms are used to improve recent results for severalvariants of the problem.'}
3.16e+03 pattern=  0 task=  0 (u=4281)  {'docs': u"Most earlier studies of Distributed Hash Tables (DHTs) under churn haveeither depended on simulations as the primary investigation tool, or onestablishing bounds for DHTs to function. In this paper, we present a completeanalytical study of churn using a master-equation-based approach, usedtraditionally in non-equilibrium statistical mechanics to describe steady-stateor transient phenomena. Simulations are used to verify all theoreticalpredictions. We demonstrate the application of our methodology to the Chordsystem. For any rate of churn and stabilization rates, and any system size, weaccurately predict the fraction of failed or incorrect successor and fingerpointers and show how we can use these quantities to predict the performanceand consistency of lookups under churn. We also discuss briefly how churn mayactually be of different 'types' and the implications this will have for thefunctioning of DHTs in general."}
1.57e+03 pattern=  0 task=  0 (u=4433)  {'docs': u'We show that for every fixed non-negative integer k there is a quadratic timealgorithm that decides whether a given graph has crossing number at most k and,if this is the case, computes a drawing of the graph in the plane with at mostk crossings.'}
  443 pattern=  0 task=  0 (u=4682)  {'docs': u'This thesis presents two similarity-based approaches to sparse data problems.The first approach is to build soft, hierarchical clusters: soft, because eachevent belongs to each cluster with some probability; hierarchical, becausecluster centroids are iteratively split to model finer distinctions. Our secondapproach is a nearest-neighbor approach: instead of calculating a centroid foreach class, as in the hierarchical clustering approach, we in essence build acluster around each word. We compare several such nearest-neighbor approacheson a word sense disambiguation task and find that as a whole, their performanceis far superior to that of standard methods. In another set of experiments, weshow that using estimation techniques based on the nearest-neighbor modelenables us to achieve perplexity reductions of more than 20 percent overstandard techniques in the prediction of low-frequency events, andstatistically significant speech recognition error-rate reduction.'}
1.86e+03 pattern=  0 task=  0 (u=4722)  {'docs': u"A multi-hop synchronous wirelss network is said to be unknown if the nodeshave no knowledge of the topology. A basic task in wireless network is that ofbroadcasting a message (created by a fixed source node) to all nodes of thenetwork. The multi-broadcast that consists in performing a set of r independentbroadcasts. In this paper, we study the completion and the termination time ofdistributed protocols for both the (single) broadcast and the multi-broadcastoperations on unknown networks as functions of the number of nodes n, themaximum eccentricity D, the maximum in-degree Delta, and the congestion c ofthe networks. We establish new connections between these operations and somecombinatorial concepts, such as selective families, strongly-selective families(also known as superimposed codes), and pairwise r-different families. Suchconnections, combined with a set of new lower and upper bounds on the size ofthe above families, allow us to derive new lower bounds and new distributedprotocols for the broadcast and multi-broadcast operations. In particular, ourupper bounds are almost tight and improve exponentially over the previousbounds when D and Delta are polylogarithmic in n. Network topologies having``small'' eccentricity and ``small'' degree (such as bounded-degree expanders)are often used in practice to achieve efficient communication."}
1.86e+03 pattern=  0 task=  0 (u=4729)  {'docs': u'In this note, we present results for the colouring problem on small worldgraphs created by rewiring square, triangular, and two kinds of cubic (withcoordination numbers 5 and 6) lattices. As the rewiring parameter p tends to 1,we find the expected crossover to the behaviour of random graphs withcorresponding connectivity. However, for the cubic lattices there is a regionnear p=0 for which the graphs are colourable. This could in principle be usedas an additional heuristic for solving real world colouring or schedulingproblems. Small worlds with connectivity 5 and p ~ 0.1 provide an interestingensemble of graphs whose colourability is hard to determine. For squarelattices, we get good data collapse plotting the fraction of colourable graphsagainst the rescaled parameter parameter $p N^{-\\nu}$ with $\\nu = 1.35$. Nosuch collapse can be obtained for the data from lattices with coordinationnumber 5 or 6.'}
2.5e+03 pattern=  0 task=  1 (u=4729)  {'docs': u"Threat assessment is an important part of level 3 data fusion. Here we studya subproblem of this, worst-case risk assessment. Inspired by agent-basedmodels used for simulation of trail formation for urban planning, we use antcolony optimization (ANTS) to determine possible avenues of approach for theenemy, given a situation picture.  One way of determining such avenues would be to calculate the ``potentialfield'' caused by placing sources at possible goals for the enemy. Thisrequires postulating a functional form for the potential, and also takes longtime. Here we instead seek a method for quickly obtaining an effectivepotential. ANTS, which has previously been used to obtain approximate solutionsto various optimization problems, is well suited for this. The output of ourmethod describes possible avenues of approach for the enemy, i.e, areas wherewe should be prepared for attack. (The algorithm can also be run ``reversed''to instead get areas of opportunity for our forces to exploit.)  Using real geographical data, we found that our method gives a fast andreliable way of determining such avenues. Our method can be used in acomputer-based command and control system to replace the first step of humanintelligence analysis."}
1.86e+03 pattern=  0 task=  0 (u=4730)  {'docs': u'We introduce a transformation system for concurrent constraint programming(CCP). We define suitable applicability conditions for the transformationswhich guarantee that the input/output CCP semantics is preserved also whendistinguishing deadlocked computations from successful ones and whenconsidering intermediate results of (possibly) non-terminating computations.  The system allows us to optimize CCP programs while preserving their intendedmeaning: In addition to the usual benefits that one has for sequentialdeclarative languages, the transformation of concurrent programs can also leadto the elimination of communication channels and of synchronization points, tothe transformation of non-deterministic computations into deterministic ones,and to the crucial saving of computational space. Furthermore, since thetransformation system preserves the deadlock behavior of programs, it can beused for proving deadlock freeness of a given program wrt a class of queries.To this aim it is sometimes sufficient to apply our transformations and tospecialize the resulting program wrt the given queries in such a way that theobtained program is trivially deadlock free.'}
3.22e+03 pattern=  0 task=  1 (u=4730)  {'docs': u'We introduce the use, monitoring, and enforcement of integrity constraints intrust management-style authorization systems. We consider what portions of thepolicy state must be monitored to detect violations of integrity constraints.Then we address the fact that not all participants in a trust management systemcan be trusted to assist in such monitoring, and show how many integrityconstraints can be monitored in a conservative manner so that trustedparticipants detect and report if the system enters a policy state from whichevolution in unmonitored portions of the policy could lead to a constraintviolation.'}
3.04e+03 pattern=  0 task=  0 (u=4752)  {'docs': u"Capacity formulas and random-coding exponents are derived for a generalizedfamily of Gel'fand-Pinsker coding problems. These exponents yield asymptoticupper bounds on the achievable log probability of error. In our model,information is to be reliably transmitted through a noisy channel with finiteinput and output alphabets and random state sequence, and the channel isselected by a hypothetical adversary. Partial information about the statesequence is available to the encoder, adversary, and decoder. The design of thetransmitter is subject to a cost constraint. Two families of channels areconsidered: 1) compound discrete memoryless channels (CDMC), and 2) channelswith arbitrary memory, subject to an additive cost constraint, or moregenerally to a hard constraint on the conditional type of the channel outputgiven the input. Both problems are closely connected. The random-codingexponent is achieved using a stacked binning scheme and a maximum penalizedmutual information decoder, which may be thought of as an empirical generalizedMaximum a Posteriori decoder. For channels with arbitrary memory, therandom-coding exponents are larger than their CDMC counterparts. Applicationsof this study include watermarking, data hiding, communication in presence ofpartially known interferers, and problems such as broadcast channels, all ofwhich involve the fundamental idea of binning."}
2.7e+03 pattern=  0 task=  0 (u=4758)  {'docs': u'In order to design and implement tracers, one must decide what exactly totrace and how to produce this trace. On the one hand, trace designs are toooften guided by implementation concerns and are not as useful as they shouldbe. On the other hand, an interesting trace which cannot be producedefficiently, is not very useful either. In this article we propose amethodology which helps to efficiently produce accurate traces. Firstly, designa formal specification of the trace model. Secondly, derive a prototype tracerfrom this specification. Thirdly, analyze the produced traces. Fourthly,implement an efficient tracer. Lastly, compare the traces of the two tracers.At each step, problems can be found. In that case one has to iterate theprocess. We have successfully applied the proposed methodology to the designand implementation of a real tracer for constraint logic programming which isable to efficiently generate information required to build interestinggraphical views of executions.'}
2.87e+03 pattern=  0 task=  0 (u=4918)  {'docs': u'Characterisations of interval graphs, comparability graphs, co-comparabilitygraphs, permutation graphs, and split graphs in terms of linear orderings ofthe vertex set are presented. As an application, it is proved that intervalgraphs, co-comparability graphs, AT-free graphs, and split graphs havebandwidth bounded by their maximum degree.'}
1.96e+03 pattern=  0 task=  0 (u=4933)  {'docs': u'In this note, we show that any distributive lattice is isomorphic to the setof reachable configurations of an Edge Firing Game. Together with the result ofJames Propp, saying that the set of reachable configurations of any Edge FiringGame is always a distributive lattice, this shows that the two concepts areequivalent.'}
2.6e+03 pattern=  0 task=  0 (u=4990)  {'docs': u'The aim of this work is to provide a family of qualitative theories forspatial change in general, and for motion of spatial scenes in particular. Toachieve this, we consider a spatio-temporalisation MTALC(D_x), of thewell-known ALC(D) family of Description Logics (DLs) with a concrete domainan.In particular, the concrete domain D_x is generated by a qualitative spatialRelation Algebra (RA) x. We show the important result that satisfiability of anMTALC(D_x) concept with respect to a weakly cyclic TBox is decidable innondeterministic exponential time, by reducing it to the emptiness problem of aweak alternating automaton augmented with spatial constraints, which we show toremain decidable, although the accepting condition of a run involves,additionally to the standard case, consistency of a CSP (ConstraintSatisfaction Problem) potentially infinite. The result provides an effectivetableaux-like satisfiability procedure which is discussed.'}
2.6e+03 pattern=  0 task=  1 (u=4990)  {'docs': u"We propose a calculus integrating two calculi well-known in QualitativeSpatial Reasoning (QSR): Frank's projection-based cardinal direction calculus,and a coarser version of Freksa's relative orientation calculus. An originalconstraint propagation procedure is presented, which implements the interactionbetween the two integrated calculi. The importance of taking into account theinteraction is shown with a real example providing an inconsistent knowledgebase, whose inconsistency (a) cannot be detected by reasoning separately abouteach of the two components of the knowledge, just because, taken separately,each is consistent, but (b) is detected by the proposed algorithm, thanks tothe interaction knowledge propagated from each of the two compnents to theother."}
2.73e+03 pattern=  0 task=  2 (u=4990)  {'docs': u'We consider the integration of existing cone-shaped and projection-basedcalculi of cardinal direction relations, well-known in QSR. The more general,integrating language we consider is based on convex constraints of thequalitative form $r(x,y)$, $r$ being a cone-shaped or projection-based cardinaldirection atomic relation, or of the quantitative form $(\\alpha ,\\beta)(x,y)$,with $\\alpha ,\\beta\\in [0,2\\pi)$ and $(\\beta -\\alpha)\\in [0,\\pi ]$: the meaningof the quantitative constraint, in particular, is that point $x$ belongs to the(convex) cone-shaped area rooted at $y$, and bounded by angles $\\alpha$ and$\\beta$. The general form of a constraint is a disjunction of the form$[r_1\\vee...\\vee r_{n_1}\\vee (\\alpha_1,\\beta_1)\\vee...\\vee (\\alpha_{n_2},\\beta_{n_2})](x,y)$, with $r_i(x,y)$, $i=1... n_1$, and $(\\alpha_i,\\beta_i)(x,y)$, $i=1... n_2$, being convex constraints as described above:the meaning of such a general constraint is that, for some $i=1... n_1$,$r_i(x,y)$ holds, or, for some $i=1... n_2$, $(\\alpha_i,\\beta_i)(x,y)$ holds. Aconjunction of such general constraints is a $\\tcsp$-like CSP, which we willrefer to as an $\\scsp$ (Spatial Constraint Satisfaction Problem). An effectivesolution search algorithm for an $\\scsp$ will be described, which uses (1)constraint propagation, based on a composition operation to be defined, as thefiltering method during the search, and (2) the Simplex algorithm, guaranteeingcompleteness, at the leaves of the search tree. The approach is particularlysuited for large-scale high-level vision, such as, e.g., satellite-likesurveillance of a geographic area.'}
3.04e+03 pattern=  0 task=  3 (u=4990)  {'docs': u'We consider the well-known family ALC(D) of description logics with aconcrete domain, and provide first results on a framework obtained byaugmenting ALC(D) atemporal roles and aspatial concrete domain with temporalroles and a spatial concrete domain.'}
3.04e+03 pattern=  0 task=  4 (u=4990)  {'docs': u'We present a family of spatio-temporal theories suitable for continuousspatial change in general, and for continuous motion of spatial scenes inparticular. The family is obtained by spatio-temporalising the well-knownALC(D) family of Description Logics (DLs) with a concrete domain D, as follows,where TCSPs denotes "Temporal Constraint Satisfaction Problems", a well-knownconstraint-based framework:  (1) temporalisation of the roles, so that they consist of TCSP constraints(specifically, of an adaptation of TCSP constraints to interval variables); and  (2) spatialisation of the concrete domain D: the concrete domain is now$D_x$, and is generated by a spatial Relation Algebra (RA) $x$, in the style ofthe Region-Connection Calculus RCC8.  We assume durative truth (i.e., holding during a durative interval). We alsoassume the homogeneity property (if a truth holds during a given interval, itholds during all of its subintervals). Among other things, these assumptionsraise the "conflicting" problem of overlapping truths, which the work solveswith the use of a specific partition of the 13 atomic relations of Allen\'sinterval algebra.'}
3.04e+03 pattern=  0 task=  5 (u=4990)  {'docs': u"We define a quantitative constraint language subsuming two calculi well-knownin QSR (Qualitative Spatial Reasoning): Frank's cone-shaped andprojection-based calculi of cardinal direction relations. We show how to solvea CSP (Constraint Satisfaction Problem) expressed in the language."}
3.09e+03 pattern=  0 task=  0 (u=5056)  {'docs': u"The spatial distribution of neuronal cells is an important requirement forachieving proper neuronal function in several parts of the nervous system ofmost animals. For instance, specific distribution of photoreceptors and relatedneuronal cells, particularly the ganglion cells, in mammal's retina is requiredin order to properly sample the projected scene. This work presents how twoconcepts from the areas of statistical mechanics and complex systems, namelythe \\emph{lacunarity} and the \\emph{multiscale entropy} (i.e. the entropycalculated over progressively diffused representations of the cell mosaic),have allowed effective characterization of the spatial distribution of retinalcells."}
1.46e+03 pattern=  0 task=  0 (u=5062)  {'docs': u'Phase transitions in combinatorial problems have recently been shown to beuseful in locating "hard" instances of combinatorial problems. The connectionbetween computational complexity and the existence of phase transitions hasbeen addressed in Statistical Mechanics and Artificial Intelligence, but notstudied rigorously.  We take a step in this direction by investigating the existence of sharpthresholds for the class of generalized satisfiability problems defined bySchaefer. In the case when all constraints are clauses we give a completecharacterization of such problems that have a sharp threshold.  While NP-completeness does not imply (even in this restricted case) theexistence of a sharp threshold, it "almost implies" this, since clausalgeneralized satisfiability problems that lack a sharp threshold are either  1. polynomial time solvable, or  2. predicted, with success probability lower bounded by some positiveconstant by across all the probability range, by a single, trivial procedure.'}
2.35e+03 pattern=  0 task=  1 (u=5062)  {'docs': u'The paper (as posted originally) contains several errors. It has beensubsequently split into two papers, the corrected (and accepted forpublication) versions appear in the archive as papers cs.CC/0503082 andcs.DM/0503083.'}
3.22e+03 pattern=  0 task=  2 (u=5062)  {'docs': u'We study threshold properties of random constraint satisfaction problemsunder a probabilistic model due to Molloy. We give a sufficient condition forthe existence of a sharp threshold that leads (for boolean constraints) to anecessary and sufficient for the existence of a sharp threshold in the casewhere constraint templates are applied with equal probability, solving thus anopen problem of Creignou and Daude.'}
3.22e+03 pattern=  0 task=  3 (u=5062)  {'docs': u"We study the connection between the order of phase transitions incombinatorial problems and the complexity of decision algorithms for suchproblems. We rigorously show that, for a class of random constraintsatisfaction problems, a limited connection between the two phenomena indeedexists. Specifically, we extend the definition of the spine order parameter ofBollobas et al. to random constraint satisfaction problems, rigorously showingthat for such problems a discontinuity of the spine is associated with a$2^{\\Omega(n)}$ resolution complexity (and thus a $2^{\\Omega(n)}$ complexity ofDPLL algorithms) on random instances. The two phenomena have a commonunderlying cause: the emergence of ``large'' (linear size) minimallyunsatisfiable subformulas of a random formula at the satisfiability phasetransition.  We present several further results that add weight to the intuition thatrandom constraint satisfaction problems with a sharp threshold and a continuousspine are ``qualitatively similar to random 2-SAT''. Finally, we argue that itis the spine rather than the backbone parameter whose continuity hasimplications for the decision complexity of combinatorial problems, and weprovide experimental evidence that the two parameters can behave in a differentmanner."}
2.7e+03 pattern=  0 task=  0 (u=5073)  {'docs': u"A theory of one-tape (one-head) linear-time Turing machines is essentiallydifferent from its polynomial-time counterpart since these machines are closelyrelated to finite state automata. This paper discusses structural-complexityissues of one-tape Turing machines of various types (deterministic,nondeterministic, reversible, alternating, probabilistic, counting, and quantumTuring machines) that halt in linear time, where the running time of a machineis defined as the length of any longest computation path. We explore structuralproperties of one-tape linear-time Turing machines and clarify how themachines' resources affect their computational patterns and power."}
1.51e+03 pattern=  0 task=  0 (u=5083)  {'docs': u'In existing simulation proof techniques, a single step in a lower-levelspecification may be simulated by an extended execution fragment in ahigher-level one. As a result, it is cumbersome to mechanize these techniquesusing general purpose theorem provers. Moreover, it is undecidable whether agiven relation is a simulation, even if tautology checking is decidable for theunderlying specification logic. This paper introduces various types of normedsimulations. In a normed simulation, each step in a lower-level specificationcan be simulated by at most one step in the higher-level one, for any relatedpair of states. In earlier work we demonstrated that normed simulations arequite useful as a vehicle for the formalization of refinement proofs viatheorem provers. Here we show that normed simulations also have pleasanttheoretical properties: (1) under some reasonable assumptions, it is decidablewhether a given relation is a normed forward simulation, provided tautologychecking is decidable for the underlying logic; (2) at the semantic level,normed forward and backward simulations together form a complete proof methodfor establishing behavior inclusion, provided that the higher-levelspecification has finite invisible nondeterminism.'}
1.51e+03 pattern=  0 task=  0 (u=5090)  {'docs': u'Formal language techniques have been used in the past to study autonomousdynamical systems. However, for controlled systems, new features are needed todistinguish between information generated by the system and input control. Weshow how the modelling framework for controlled dynamical systems leadsnaturally to a formulation in terms of context-dependent grammars. A learningalgorithm is proposed for on-line generation of the grammar productions, thisformulation being then used for modelling, control and anomaly detection.Practical applications are described for electromechanical drives. Grammaticalinterpolation techniques yield accurate results and the pattern detectioncapabilities of the language-based formulation makes it a promising techniquefor the early detection of anomalies or faulty behaviour.'}
1.58e+03 pattern=  0 task=  0 (u=5131)  {'docs': u"We describe an approach to programming rule-based systems in Standard ML,with a focus on so-called overlapping rules, that is rules that can still beactive when other rules are fired. Such rules are useful when implementingrule-based reactive systems, and to that effect we show a simple implementationof Loyall's Active Behavior Trees, used to control goal-directed agents in theOz virtual environment. We discuss an implementation of our framework using areactive library geared towards implementing those kind of systems."}
2.9e+03 pattern=  0 task=  1 (u=5131)  {'docs': u'The framework of algorithmic knowledge assumes that agents use algorithms tocompute the facts they explicitly know. In many cases of interest, a deductivesystem, rather than a particular algorithm, captures the formal reasoning usedby the agents to compute what they explicitly know. We introduce a logic forreasoning about both implicit and explicit knowledge with the latter definedwith respect to a deductive system formalizing a logical theory for agents. Thehighly structured nature of deductive systems leads to very naturalaxiomatizations of the resulting logic when interpreted over any fixeddeductive system. The decision problem for the logic, in the presence of asingle agent, is NP-complete in general, no harder than propositional logic. Itremains NP-complete when we fix a deductive system that is decidable innondeterministic polynomial time. These results extend in a straightforward wayto multiple agents.'}
2.91e+03 pattern=  0 task=  2 (u=5131)  {'docs': u"We present a logic for reasoning about licenses, which are ``terms of use''for digital resources. The logic provides a language for writing bothproperties of licenses and specifications that govern a client's actions. Wediscuss the complexity of checking properties and specifications written in ourlogic and propose a technique for verification. A key feature of our approachis that it is essentially parameterized by the language in which the licensesare written, provided that this language can be given a trace-based semantics.We consider two license languages to illustrate this flexibility."}
2.91e+03 pattern=  0 task=  3 (u=5131)  {'docs': u'Reactive systems are systems that maintain an ongoing interaction with theirenvironment, activated by receiving input events from the environment andproducing output events in response. Modern programming languages designed toprogram such systems use a paradigm based on the notions of instants andactivations. We describe a library for Standard ML that provides basicprimitives for programming reactive systems. The library is a low-level systemupon which more sophisticated reactive behaviors can be built, which provides aconvenient framework for prototyping extensions to existing reactive languages.'}
2.91e+03 pattern=  0 task=  4 (u=5131)  {'docs': u"Lambek's production machines may be used to generate and recognize sentencesin a subset of the language described by a production grammar. We determine inthis paper the subset of the language of a grammar generated and recognized bysuch machines."}
2.91e+03 pattern=  0 task=  5 (u=5131)  {'docs': u'We present a concurrent framework for Win32 programming based on ConcurrentML, a concurrent language with higher-order functions, static typing,lightweight threads and synchronous communication channels. The key points ofthe framework are the move from an event loop model to a threaded model for theprocessing of window messages, and the decoupling of controls notificationsfrom the system messages. This last point allows us to derive a general way ofwriting controls that leads to easy composition, and can accommodate ActiveXControls in a transparent way.'}
2.91e+03 pattern=  0 task=  6 (u=5131)  {'docs': u'A useful programming language needs to support writing programs that takeadvantage of services and communication mechanisms supplied by the operatingsystem. We examine the problem of programming native Win32 applications underWindows with Standard ML. We introduce an framework based on the IDL interfacelanguage et a minimal foreign-functions interface to explore the Win32 API etCOM in the context of Standard ML.'}
2.91e+03 pattern=  0 task=  7 (u=5131)  {'docs': u'We present in this paper the preliminary design of a module system based on anotion of components such as they are found in COM. This module system isinspired from that of Standard ML, and features first-class instances ofcomponents, first-class interfaces, and interface-polymorphic functions, aswell as allowing components to be both imported from the environment andexported to the environment using simple mechanisms. The module systemautomates the memory management of interfaces and hides the IUnknown interfaceand QueryInterface mechanisms from the programmer, favoring instead ahigher-level approach to handling interfaces.'}
2.91e+03 pattern=  0 task=  8 (u=5131)  {'docs': u"Recent results of Bucciarelli show that the semilattice of degrees ofparallelism of first-order boolean functions in PCF has both infinite chainsand infinite antichains. By considering a simple subclass of Sieber'ssequentiality relations, we identify levels in the semilattice and deriveinexpressibility results concerning functions on different levels. This allowsus to further explore the structure of the semilattice of degrees ofparallelism: we identify semilattices characterized by simple level properties,and show the existence of new infinite hierarchies which are in a certain sensenatural with respect to the levels."}
2.51e+03 pattern=  0 task=  0 (u=5150)  {'docs': u'Given a $k$-uniform hyper-graph, the E$k$-Vertex-Cover problem is to find thesmallest subset of vertices that intersects every hyper-edge. We present a newmultilayered PCP construction that extends the Raz verifier. This enables us toprove that E$k$-Vertex-Cover is NP-hard to approximate within factor$(k-1-\\epsilon)$ for any $k \\geq 3$ and any $\\epsilon>0$. The result isessentially tight as this problem can be easily approximated within factor $k$.Our construction makes use of the biased Long-Code and is analyzed usingcombinatorial properties of $s$-wise $t$-intersecting families of subsets.'}
3.24e+03 pattern=  0 task=  1 (u=5150)  {'docs': u"We study the coloring problem: Given a graph G, decide whether $c(G) \\leq q$or $c(G) \\ge Q$, where c(G) is the chromatic number of G. We derive conditionalhardness for this problem for any constant $3 \\le q < Q$. For $q\\ge 4$, ourresult is based on Khot's 2-to-1 conjecture [Khot'02]. For $q=3$, we base ourhardness result on a certain `fish shaped' variant of his conjecture.  We also prove that the problem almost coloring is hard for any constant$\\eps>0$, assuming Khot's Unique Games conjecture. This is the problem ofdeciding for a given graph, between the case where one can 3-color all but a$\\eps$ fraction of the vertices without monochromatic edges, and the case wherethe graph contains no independent set of relative size at least $\\eps$.  Our result is based on bounding various generalized noise-stabilityquantities using the invariance principle of Mossel et al [MOO'05]."}
2.95e+03 pattern=  0 task=  0 (u=5156)  {'docs': u"We address the problem of bounding below the probability of error undermaximum likelihood decoding of a binary code with a known distance distributionused on a binary symmetric channel. An improved upper bound is given for themaximum attainable exponent of this probability (the reliability function ofthe channel). In particular, we prove that the ``random coding exponent'' isthe true value of the channel reliability for code rate $R$ in some intervalimmediately below the critical rate of the channel. An analogous result isobtained for the Gaussian channel."}
3.02e+03 pattern=  0 task=  1 (u=5156)  {'docs': u'We study the minimum distance of codes defined on bipartite graphs. Weightspectrum and the minimum distance of a random ensemble of such codes arecomputed. It is shown that if the vertex codes have minimum distance $\\ge 3$,the overall code is asymptotically good, and sometimes meets theGilbert-Varshamov bound.  Constructive families of expander codes are presented whose minimum distanceasymptotically exceeds the product bound for all code rates between 0 and 1.'}
3.13e+03 pattern=  0 task=  2 (u=5156)  {'docs': u'A new lower bound on the error probability of maximum likelihood decoding ofa binary code on a binary symmetric channel was proved in Barg and McGregor(2004, cs.IT/0407011). It was observed in that paper that this bound leads to anew region of code rates in which the random coding exponent is asymptoticallytight, giving a new region in which the reliability of the BSC is knownexactly. The present paper explains the relation of these results to the unionbound on the error probability.'}
3.16e+03 pattern=  0 task=  0 (u=5223)  {'docs': u'Transmission of information over a discrete-time memoryless Rician fadingchannel is considered where neither the receiver nor the transmitter knows thefading coefficients. First the structure of the capacity-achieving inputsignals is investigated when the input is constrained to have limitedpeakedness by imposing either a fourth moment or a peak constraint. When theinput is subject to second and fourth moment limitations, it is shown that thecapacity-achieving input amplitude distribution is discrete with a finitenumber of mass points in the low-power regime. A similar discrete structure forthe optimal amplitude is proven over the entire SNR range when there is only apeak power constraint. The Rician fading with phase-noise channel model, wherethere is phase uncertainty in the specular component, is analyzed. For thismodel it is shown that, with only an average power constraint, thecapacity-achieving input amplitude is discrete with a finite number of levels.For the classical average power limited Rician fading channel, it is proventhat the optimal input amplitude distribution has bounded support.'}
3.21e+03 pattern= 11 task=  0 (u=5334)  {'docs': u'The characterization of fibonacci cobweb poset as d.a.g. and o.d.a.g. isgiven. The dim 2 poset such that its hasse diagram coincide with digraf offibonacci cobweb poset is constructed.'}
1.15e+03 pattern=  0 task=  0 (u=5371)  {'docs': u"We prove that the join of two sets may actually fall into a lower level ofthe extended low hierarchy than either of the sets. In particular, there existsets that are not in the second level of the extended low hierarchy, EL_2, yettheir join is in EL_2. That is, in terms of extended lowness, the join operatorcan lower complexity. Since in a strong intuitive sense the join does not lowercomplexity, our result suggests that the extended low hierarchy is unnatural asa complexity measure. We also study the closure properties of EL_ and provethat EL_2 is not closed under certain Boolean operations. To this end, weestablish the first known (and optimal) EL_2 lower bounds for certain notionsgeneralizing Selman's P-selectivity, which may be regarded as an interestingresult in its own right."}
1.15e+03 pattern=  0 task=  1 (u=5371)  {'docs': u"Rice's Theorem states that every nontrivial language property of therecursively enumerable sets is undecidable. Borchert and Stephan initiated thesearch for complexity-theoretic analogs of Rice's Theorem. In particular, theyproved that every nontrivial counting property of circuits is UP-hard, and thata number of closely related problems are SPP-hard.  The present paper studies whether their UP-hardness result itself can beimproved to SPP-hardness. We show that their UP-hardness result cannot bestrengthened to SPP-hardness unless unlikely complexity class containmentshold. Nonetheless, we prove that every P-constructibly bi-infinite countingproperty of circuits is SPP-hard. We also raise their general lower bound fromunambiguous nondeterminism to constant-ambiguity nondeterminism."}
1.33e+03 pattern=  0 task=  2 (u=5371)  {'docs': u'We discuss the use of projects in first-year graduate complexity theorycourses.'}
1.73e+03 pattern=  8 task=  3 (u=5371)  {'docs': u'No P-immune set having exponential gaps is positive-Turing self-reducible.'}
2.68e+03 pattern=  0 task=  0 (u=5417)  {'docs': u'Massive data sets have radically changed our understanding of how to designefficient algorithms; the streaming paradigm, whether it in terms of number ofpasses of an external memory algorithm, or the single pass and limited memoryof a stream algorithm, appears to be the dominant method for coping with largedata.  A very different kind of massive computation has had the same effect at thelevel of the CPU. The most prominent example is that of the computationsperformed by a graphics card. The operations themselves are very simple, andrequire very little memory, but require the ability to perform manycomputations extremely fast and in parallel to whatever degree possible. Whathas resulted is a stream processor that is highly optimized for streamcomputations. An intriguing side effect of this is the growing use of agraphics card as a general purpose stream processing engine. In anever-increasing array of applications, researchers are discovering thatperforming a computation on a graphics card is far faster than performing it ona CPU, and so are using a GPU as a stream co-processor.'}
2.81e+03 pattern=  0 task=  0 (u=5418)  {'docs': u'We discuss the computational complexity of random 2D Ising spin glasses,which represent an interesting class of constraint satisfaction problems forblack box optimization. Two extremal cases are considered: (1) the +/- J spinglass, and (2) the Gaussian spin glass. We also study a smooth transitionbetween these two extremal cases. The computational complexity of all studiedspin glass systems is found to be dominated by rare events of extremely hardspin glass samples. We show that complexity of all studied spin glass systemsis closely related to Frechet extremal value distribution. In a hybridalgorithm that combines the hierarchical Bayesian optimization algorithm (hBOA)with a deterministic bit-flip hill climber, the number of steps performed byboth the global searcher (hBOA) and the local searcher follow Frechetdistributions. Nonetheless, unlike in methods based purely on local search, theparameters of these distributions confirm good scalability of hBOA with localsearch. We further argue that standard performance measures for optimizationalgorithms--such as the average number of evaluations until convergence--can bemisleading. Finally, our results indicate that for highly multimodal constraintsatisfaction problems, such as Ising spin glasses, recombination-based searchcan provide qualitatively better results than mutation-based search.'}
2.81e+03 pattern=  0 task=  1 (u=5418)  {'docs': u'This paper describes how fitness inheritance can be used to estimate fitnessfor a proportion of newly sampled candidate solutions in the Bayesianoptimization algorithm (BOA). The goal of estimating fitness for some candidatesolutions is to reduce the number of fitness evaluations for problems wherefitness evaluation is expensive. Bayesian networks used in BOA to modelpromising solutions and generate the new ones are extended to allow not onlyfor modeling and sampling candidate solutions, but also for estimating theirfitness. The results indicate that fitness inheritance is a promising conceptin BOA, because population-sizing requirements for building appropriate modelsof promising solutions lead to good fitness estimates even if only a smallproportion of candidate solutions is evaluated using the actual fitnessfunction. This can lead to a reduction of the number of actual fitnessevaluations by a factor of 30 or more.'}
2.81e+03 pattern=  0 task=  2 (u=5418)  {'docs': u'The parameter-less hierarchical Bayesian optimization algorithm (hBOA)enables the use of hBOA without the need for tuning parameters for solving eachproblem instance. There are three crucial parameters in hBOA: (1) the selectionpressure, (2) the window size for restricted tournaments, and (3) thepopulation size. Although both the selection pressure and the window sizeinfluence hBOA performance, performance should remain low-order polynomial withstandard choices of these two parameters. However, there is no standardpopulation size that would work for all problems of interest and the populationsize must thus be eliminated in a different way. To eliminate the populationsize, the parameter-less hBOA adopts the population-sizing technique of theparameter-less genetic algorithm. Based on the existing theory, theparameter-less hBOA should be able to solve nearly decomposable andhierarchical problems in quadratic or subquadratic number of functionevaluations without the need for setting any parameters whatsoever. A number ofexperiments are presented to verify scalability of the parameter-less hBOA.'}
3.17e+03 pattern=  0 task=  3 (u=5418)  {'docs': u'This paper describes a scalable algorithm for solving multiobjectivedecomposable problems by combining the hierarchical Bayesian optimizationalgorithm (hBOA) with the nondominated sorting genetic algorithm (NSGA-II) andclustering in the objective space. It is first argued that for goodscalability, clustering or some other form of niching in the objective space isnecessary and the size of each niche should be approximately equal.Multiobjective hBOA (mohBOA) is then described that combines hBOA, NSGA-II andclustering in the objective space. The algorithm mohBOA differs from themultiobjective variants of BOA and hBOA proposed in the past by includingclustering in the objective space and allocating an approximately equally sizedportion of the population to each cluster. The algorithm mohBOA is shown toscale up well on a number of problems on which standard multiobjectiveevolutionary algorithms perform poorly.'}
1.05e+03 pattern=  0 task=  0 (u=5483)  {'docs': u'This paper provides a proof of the proposed Internet standard Transport LevelSecurity protocol using the Gong-Needham-Yahalom logic. It is intended as ateaching aid and hopes to show to students: the potency of a formal method forprotocol design; some of the subtleties of authenticating parties on a networkwhere all messages can be intercepted; the design of what should be a widelyaccepted standard.'}
1.07e+03 pattern=  0 task=  1 (u=5483)  {'docs': u"This paper presents some fundamental collective choice theory for informationsystem designers, particularly those working in the field of computer-supportedcooperative work. This paper is focused on a presentation of Arrow'sPossibility and Impossibility theorems which form the fundamental boundary onthe efficacy of collective choice: voting and selection procedures. It restatesthe conditions that Arrow placed on collective choice functions in morerigorous second-order logic, which could be used as a set of test conditionsfor implementations, and a useful probabilistic result for analyzing votes onissue pairs. It also describes some simple collective choice functions. Thereis also some discussion of how enterprises should approach putting theirresources under collective control: giving an outline of a superstructure ofperformative agents to carry out this function and what distributing processingtechnology would be needed."}
1.47e+03 pattern=  0 task=  0 (u=5540)  {'docs': u'An elastic ideal 2D propagation medium, i.e., a membrane, can be simulated bymodels discretizing the wave equation on the time-space grid (finite differencemethods), or locally discretizing the solution of the wave equation (waveguidemeshes). The two approaches provide equivalent computational structures, andintroduce numerical dispersion that induces a misalignment of the modes fromtheir theoretical positions. Prior literature shows that dispersion can bearbitrarily reduced by oversizing and oversampling the mesh, or by adptingoffline warping techniques. In this paper we propose to reduce numericaldispersion by embedding warping elements, i.e., properly tuned allpass filters,in the structure. The resulting model exhibits a significant reduction indispersion, and requires less computational resources than a regular meshstructure having comparable accuracy.'}
1.68e+03 pattern=  0 task=  1 (u=5540)  {'docs': u'Waveguide Meshes are efficient and versatile models of wave propagation alonga multidimensional ideal medium. The choice of the mesh geometry affects boththe computational cost and the accuracy of simulations. In this paper, we focuson 2D geometries and use multidimensional sampling theory to compare thesquare, triangular, and hexagonal meshes in terms of sampling efficiency anddispersion error under conditions of critical sampling. The analysis shows thatthe triangular geometry exhibits the most desirable tradeoff between accuracyand computational cost.'}
1.59e+03 pattern=  0 task=  0 (u=5541)  {'docs': u'A rectangular enclosure has such an even distribution of resonances that itcan be accurately and efficiently modelled using a feedback delay network.Conversely, a non rectangular shape such as a sphere has a distribution ofresonances that challenges the construction of an efficient model. This workproposes an extension of the already known feedback delay network structure tomodel the resonant properties of a sphere. A specific frequency distribution ofresonances can be approximated, up to a certain frequency, by inserting anallpass filter of moderate order after each delay line of a feedback delaynetwork. The structure used for rectangular boxes is therefore augmented with aset of allpass filters allowing parametric control over the enclosure size andthe boundary properties. This work was motivated by informal listening testswhich have shown that it is possible to identify a basic shape just from thedistribution of its audible resonances.'}
2.66e+03 pattern=  0 task=  0 (u=5605)  {'docs': u"The mu-calculus is a powerful tool for specifying and verifying transitionsystems, including those with both demonic and angelic choice; its quantitativegeneralisation qMu extends that to probabilistic choice.  We show that for a finite-state system the logical interpretation of qMu, viafixed-points in a domain of real-valued functions into [0,1], is equivalent toan operational interpretation given as a turn-based gambling game between twoplayers.  The logical interpretation provides direct access to axioms, laws andmeta-theorems. The operational, game- based interpretation aids the intuitionand continues in the more general context to provide a surprisingly practicalspecification tool.  A corollary of our proofs is an extension of Everett's singly-nested gamesresult in the finite turn-based case: we prove well-definedness of the minimaxvalue, and existence of fixed memoriless strategies, for all qMugames/formulae, of arbitrary (including alternating) nesting structure."}
1.5e+03 pattern=  0 task=  0 (u=5616)  {'docs': u'We study the quantum complexity of the static set membership problem: given asubset S (|S| \\leq n) of a universe of size m (m \\gg n), store it as a table ofbits so that queries of the form `Is x \\in S?\' can be answered. The goal is touse a small table and yet answer queries using few bitprobes. This problem wasconsidered recently by Buhrman, Miltersen, Radhakrishnan and Venkatesh, wherelower and upper bounds were shown for this problem in the classicaldeterministic and randomized models. In this paper, we formulate this problemin the "quantum bitprobe model" and show tradeoff results between space andtime.In this model, the storage scheme is classical but the query scheme isquantum.We show, roughly speaking, that similar lower bounds hold in thequantum model as in the classical model, which imply that the classical upperbounds are more or less tight even in the quantum case. Our lower bounds areproved using linear algebraic techniques.'}
1.96e+03 pattern=  0 task=  1 (u=5616)  {'docs': u'We consider the problem of computing the second elementary symmetricpolynomial S^2_n(X) using depth-three arithmetic circuits of the form "sum ofproducts of linear forms". We consider this problem over several fields anddetermine EXACTLY the number of multiplication gates required. The lower boundsare proved for inhomogeneous circuits where the linear forms are allowed tohave constants; the upper bounds are proved in the homogeneous model. For realsand rationals, the number of multiplication gates required is exactly n-1; inmost other cases, it is \\ceil{n/2}. This problem is related to theGraham-Pollack theorem in algebraic graph theory. In particular, our resultsanswer the following question of Babai and Frankl: what is the minimum numberof complete bipartite graphs required to cover each edge of a complete graph anodd number of times? We show that for infinitely many n, the answer is\\ceil{n/2}.'}
3.11e+03 pattern=  0 task=  0 (u=5679)  {'docs': u'We prove fixed points results for sandpiles starting with arbitrary initialconditions. We give an effective algorithm for computing such fixed points, andwe refine it in the particular case of SPM.'}
3.02e+03 pattern=  0 task=  0 (u=5740)  {'docs': u'We consider functions mapping non-negative integers to non-negative realnumbers such that a and a+n are mapped to values at least 1/n apart. In thispaper we use a novel method to construct such a function. We conjecture thatthe supremum of the generated function is optimal and pose some unsolvedproblems.'}
3.24e+03 pattern=  0 task=  0 (u=5788)  {'docs': u'In this paper we compare three different formalisms that can be used in thearea of models for distributed, concurrent and mobile systems. In particular weanalyze the relationships between a process calculus, the Fusion Calculus,graph transformations in the Synchronized Hyperedge Replacement with Hoaresynchronization (HSHR) approach and logic programming. We present a translationfrom Fusion Calculus into HSHR (whereas Fusion Calculus uses Milnersynchronization) and prove a correspondence between the reduction semantics ofFusion Calculus and HSHR transitions. We also present a mapping from HSHR intoa transactional version of logic programming and prove that there is a fullcorrespondence between the two formalisms. The resulting mapping from FusionCalculus to logic programming is interesting since it shows the tight analogiesbetween the two formalisms, in particular for handling name generation andmobility. The intermediate step in terms of HSHR is convenient since graphtransformations allow for multiple, remote synchronizations, as required byFusion Calculus semantics.'}
  813 pattern=  0 task=  0 (u=5865)  {'docs': u'In this document we study the application of weighted proportional fairnessto data flows in the Internet. We let the users set the weights of theirconnections in order to maximise the utility they get from the network. Whencombined with a pricing scheme where connections are billed by weight and time,such a system is known to maximise the total utility of the network. Our studycase is a national Web cache server connected to long distance links. Wepropose two ways of weighting TCP connections by manipulating some parametersof the protocol and present results from simulations and prototypes. We finallydiscuss how proportional fairness could be used to implement an Internet withdifferentiated services.'}
2.74e+03 pattern=  0 task=  0 (u=5877)  {'docs': u'We revisit the standard axioms of domain theory with emphasis on theirrelation to the concept of partiality, explain how this idea arises naturallyin probability theory and quantum mechanics, and then search for a mathematicalsetting capable of providing a satisfactory unification of the two.'}
2.8e+03 pattern=  0 task=  1 (u=5877)  {'docs': u"We expose the information flow capabilities of pure bipartite entanglement asa theorem -- which embodies the exact statement on the `seemingly acausal flowof information' in protocols such as teleportation. We use this theorem tore-design and analyze known protocols (e.g. logic gate teleportation andentanglement swapping) and show how to produce some new ones (e.g. parallelcomposition of logic gates). We also show how our results extend to themultipartite case and how they indicate that entanglement can be measured interms of `information flow capabilities'. Ultimately, we propose a scheme forautomated design of protocols involving measurements, local unitarytransformations and classical communication."}
1.95e+03 pattern=  0 task=  0 (u=5911)  {'docs': u'The unification problem in algebras capable of describing sets has beentackled, directly or indirectly, by many researchers and it finds importantapplications in various research areas--e.g., deductive databases, theoremproving, static analysis, rapid software prototyping. The various solutionsproposed are spread across a large literature. In this paper we provide auniform presentation of unification of sets, formalizing it at the level of settheory. We address the problem of deciding existence of solutions at anabstract level. This provides also the ability to classify different types ofset unification problems. Unification algorithms are uniformly proposed tosolve the unification problem in each of such classes.  The algorithms presented are partly drawn from the literature--and properlyrevisited and analyzed--and partly novel proposals. In particular, we present anew goal-driven algorithm for general ACI1 unification and a new simpleralgorithm for general (Ab)(Cl) unification.'}
  683 pattern=  0 task=  0 (u=6040)  {'docs': u'We consider the possibility of encoding m classical bits into much fewer nquantum bits so that an arbitrary bit from the original m bits can be recoveredwith a good probability, and we show that non-trivial quantum encodings existthat have no classical counterparts. On the other hand, we show that quantumencodings cannot be much more succint as compared to classical encodings, andwe provide a lower bound on such quantum encodings. Finally, using this lowerbound, we prove an exponential lower bound on the size of 1-way quantum finiteautomata for a family of languages accepted by linear sized deterministicfinite automata.'}
2.53e+03 pattern=  0 task=  1 (u=6040)  {'docs': u'The degree of a polynomial representing (or approximating) a function f is alower bound for the number of quantum queries needed to compute f. Thisobservation has been a source of many lower bounds on quantum algorithms. Ithas been an open problem whether this lower bound is tight.  We exhibit a function with polynomial degree M and quantum query complexity\\Omega(M^{1.321...}). This is the first superlinear separation betweenpolynomial degree and quantum query complexity. The lower bound is shown by anew, more general version of quantum adversary method.'}
2.71e+03 pattern=  0 task=  2 (u=6040)  {'docs': u'We use quantum walks to construct a new quantum algorithm for elementdistinctness and its generalization. For element distinctness (the problem offinding two equal items among N given items), we get an O(N^{2/3}) queryquantum algorithm. This improves the previous O(N^{3/4}) query quantumalgorithm of Buhrman et.al. (quant-ph/0007016) and matches the lower bound byShi (quant-ph/0112086). The algorithm also solves the generalization of elementdistinctness in which we have to find k equal items among N items. For thisproblem, we get an O(N^{k/(k+1)}) query quantum algorithm.'}
2.83e+03 pattern=  0 task=  3 (u=6040)  {'docs': u'The oracle identification problem (OIP) is, given a set $S$ of $M$ Booleanoracles out of $2^{N}$ ones, to determine which oracle in $S$ is the currentblack-box oracle. We can exploit the information that candidates of the currentoracle is restricted to $S$. The OIP contains several concrete problems such asthe original Grover search and the Bernstein-Vazirani problem. Our interest isin the quantum query complexity, for which we present several upper and lowerbounds. They are quite general and mostly optimal: (i) The query complexity ofOIP is $O(\\sqrt{N\\log M \\log N}\\log\\log M)$ for {\\it any} $S$ such that $M =|S| > N$, which is better than the obvious bound $N$ if $M < 2^{N/\\log^{3}N}$.(ii) It is $O(\\sqrt{N})$ for {\\it any} $S$ if $|S| = N$, which includes theupper bound for the Grover search as a special case. (iii) For a wide range oforacles ($|S| = N$) such as random oracles and balanced oracles, the querycomplexity is $\\Theta(\\sqrt{N/K})$, where $K$ is a simple parameter determinedby $S$.'}
2.84e+03 pattern=  0 task=  4 (u=6040)  {'docs': u'Quantum walks are quantum counterparts of Markov chains. In this article, wegive a brief overview of quantum walks, with emphasis on their algorithmicapplications.'}
2.84e+03 pattern=  0 task=  0 (u=6042)  {'docs': u'We introduce a computational problem of distinguishing between two specificquantum states as a new cryptographic problem to design a quantum cryptographicscheme that is "secure" against any polynomial-time quantum adversary. Ourproblem, QSCDff, is to distinguish between two types of random coset stateswith a hidden permutation over the symmetric group of finite degree. Thisnaturally generalizes the commonly-used distinction problem between twoprobability distributions in computational cryptography. As our majorcontribution, we show that QSCDff has three properties of cryptographicinterest: (i) QSCDff has a trapdoor; (ii) the average-case hardness of QSCDffcoincides with its worst-case hardness; and (iii) QSCDff is computationally atleast as hard as the graph automorphism problem in the worst case. Thesecryptographic properties enable us to construct a quantum public-keycryptosystem, which is likely to withstand any chosen plaintext attack of apolynomial-time quantum adversary. We further discuss a generalization ofQSCDff, called QSCDcyc, and introduce a multi-bit encryption scheme that relieson similar cryptographic properties of QSCDcyc.'}
2.94e+03 pattern=  0 task=  0 (u=6095)  {'docs': u'Refactoring is an established technique from the OO-community to restructurecode: it aims at improving software readability, maintainability andextensibility. Although refactoring is not tied to the OO-paradigm inparticular, its ideas have not been applied to Logic Programming until now.  This paper applies the ideas of refactoring to Prolog programs. A catalogueis presented listing refactorings classified according to scope. Some of therefactorings have been adapted from the OO-paradigm, while others have beenspecifically designed for Prolog. Also the discrepancy between intended andoperational semantics in Prolog is addressed by some of the refactorings.  In addition, ViPReSS, a semi-automatic refactoring browser, is discussed andthe experience with applying \\vipress to a large Prolog legacy system isreported. Our main conclusion is that refactoring is not only a viabletechnique in Prolog but also a rather desirable one.'}
3.28e+03 pattern=  0 task=  1 (u=6095)  {'docs': u"Taylor introduced a variable binding scheme for logic variables in his PARMAsystem, that uses cycles of bindings rather than the linear chains of bindingsused in the standard WAM representation. Both the HAL and dProlog languagesmake use of the PARMA representation in their Herbrand constraint solvers.Unfortunately, PARMA's trailing scheme is considerably more expensive in bothtime and space consumption. The aim of this paper is to present severaltechniques that lower the cost.  First, we introduce a trailing analysis for HAL using the classic PARMAtrailing scheme that detects and eliminates unnecessary trailings. Theanalysis, whose accuracy comes from HAL's determinism and mode declarations,has been integrated in the HAL compiler and is shown to produce spaceimprovements as well as speed improvements. Second, we explain how to modifythe classic PARMA trailing scheme to halve its trailing cost. This technique isillustrated and evaluated both in the context of dProlog and HAL. Finally, weexplain the modifications needed by the trailing analysis in order to becombined with our modified PARMA trailing scheme. Empirical evidence shows thatthe combination is more effective than any of the techniques when used inisolation.  To appear in Theory and Practice of Logic Programming."}
2.32e+03 pattern=  0 task=  0 (u=6120)  {'docs': u'In this paper we present a theoretical analysis of the deterministic on-line{\\em Sum of Squares} algorithm ($SS$) for bin packing introduced and studiedexperimentally in \\cite{CJK99}, along with several new variants. $SS$ isapplicable to any instance of bin packing in which the bin capacity $B$ anditem sizes $s(a)$ are integral (or can be scaled to be so), and runs in time$O(nB)$. It performs remarkably well from an average case point of view: Forany discrete distribution in which the optimal expected waste is sublinear,$SS$ also has sublinear expected waste. For any discrete distribution where theoptimal expected waste is bounded, $SS$ has expected waste at most $O(\\log n)$.In addition, we discuss several interesting variants on $SS$, including arandomized $O(nB\\log B)$-time on-line algorithm $SS^*$, based on $SS$, whoseexpected behavior is essentially optimal for all discrete distributions.Algorithm $SS^*$ also depends on a new linear-programming-basedpseudopolynomial-time algorithm for solving the NP-hard problem of determining,given a discrete distribution $F$, just what is the growth rate for the optimalexpected waste. This article is a greatly expanded version of the conferencepaper \\cite{sumsq2000}.'}
2.32e+03 pattern=  0 task=  0 (u=6125)  {'docs': u'We propose a new and easily-realizable distributed hash table (DHT)peer-to-peer structure, incorporating a random caching strategy that allows for{\\em polylogarithmic search time} while having only a {\\em constant cache}size. We also show that a very large class of deterministic caching strategies,which covers almost all previously proposed DHT systems, can not achievepolylog search time with constant cache size. In general, the new scheme is thefirst known DHT structure with the following highly-desired properties: (a)Random caching strategy with constant cache size; (b) Average search time of$O(log^{2}(N))$; (c) Guaranteed search time of $O(log^{3}(N))$; (d) Truly localcache dynamics with constant overhead for node deletions and additions; (e)Self-organization from any initial network state towards the desired structure;and (f) Allows a seamless means for various trade-offs, e.g., search speed oranonymity at the expense of larger cache size.'}
2.33e+03 pattern=  0 task=  0 (u=6127)  {'docs': u"The multiprocessor effect refers to the loss of computing cycles due toprocessing overhead. Amdahl's law and the Multiprocessing Factor (MPF) are twoscaling models used in industry and academia for estimating multiprocessorcapacity in the presence of this multiprocessor effect. Both models expressdifferent laws of diminishing returns. Amdahl's law identifies diminishingprocessor capacity with a fixed degree of serialization in the workload, whilethe MPF model treats it as a constant geometric ratio. The utility of bothmodels for performance evaluation stems from the presence of a single parameterthat can be determined easily from a small set of benchmark measurements. Thisutility, however, is marred by a dilemma. The two models produce differentresults, especially for large processor configurations that are so importantfor today's applications. The question naturally arises: Which of these twomodels is the correct one to use? Ignoring this question merely reducescapacity prediction to arbitrary curve-fitting. Removing the dilemma requires adynamical interpretation of these scaling models. We present a physicalinterpretation based on queueing theory and show that Amdahl's law correspondsto synchronous queueing in a bus model while the MPF model belongs to a Coxianserver model. The latter exhibits unphysical effects such as sublinear responsetimes hence, we caution against its use for large multiprocessorconfigurations."}
2.91e+03 pattern=  0 task=  0 (u=6160)  {'docs': u'This paper introduces an abductive framework for updating knowledge basesrepresented by extended disjunctive programs. We first provide a simpletransformation from abductive programs to update programs which are logicprograms specifying changes on abductive hypotheses. Then, extended abduction,which was introduced by the same authors as a generalization of traditionalabduction, is computed by the answer sets of update programs. Next, differenttypes of updates, view updates and theory updates are characterized byabductive programs and computed by update programs. The task of consistencyrestoration is also realized as special cases of these updates. Each updateproblem is comparatively assessed from the computational complexity viewpoint.The result of this paper provides a uniform framework for different types ofknowledge base updates, and each update is computed using existing proceduresof logic programming.'}
2.06e+03 pattern=  0 task=  0 (u=6173)  {'docs': u'We rephrase the problem of 3D reconstruction from images in terms ofintersections of projections of orbits of custom built Lie groups actions. Wethen use an algorithmic method based on moving frames "a la Fels-Olver" toobtain a fundamental set of invariants of these groups actions. The invariantsare used to define a set of equations to be solved by the points of the 3Dobject, providing a new technique for recovering 3D structure from motion.'}
1.46e+03 pattern=  0 task=  0 (u=6226)  {'docs': u"The ability of a robot to detect and respond to changes in its environment ispotentially very useful, as it draws attention to new and potentially importantfeatures. We describe an algorithm for learning to filter out previouslyexperienced stimuli to allow further concentration on novel features. Thealgorithm uses a model of habituation, a biological process which causes adecrement in response with repeated presentation. Experiments with a mobilerobot are presented in which the robot detects the most novel stimulus andturns towards it (`neotaxis')."}
2.86e+03 pattern=  0 task=  0 (u=6448)  {'docs': u'We want to achieve efficiency for the exact computation of the dot product oftwo vectors over word-size finite fields. We therefore compare the practicalbehaviors of a wide range of implementation techniques using differentrepresentations. The techniques used include oating point representations,discrete logarithms, tabulations, Montgomery reduction, delayed modulus.'}
3.16e+03 pattern=  0 task=  1 (u=6448)  {'docs': u"This article deals with the computation of the characteristic polynomial ofdense matrices over small finite fields and over the integers. We first presenttwo algorithms for the finite fields: one is based on Krylov iterates andGaussian elimination. We compare it to an improvement of the second algorithmof Keller-Gehrig. Then we show that a generalization of Keller-Gehrig's thirdalgorithm could improve both complexity and computational time. We use theseresults as a basis for the computation of the characteristic polynomial ofinteger matrices. We first use early termination and Chinese remaindering fordense matrices. Then a probabilistic approach, based on integer minimalpolynomial and Hensel factorization, is particularly well suited to sparseand/or structured matrices."}
2.92e+03 pattern=  0 task=  0 (u=6517)  {'docs': u"Main purposes of the paper are followings: 1) To show examples of thecalculations in domain of QFT via ``derivative rules'' of an expert system; 2)To consider advantages and disadvantage that technology of the calculations; 3)To reflect about how one would develop new physical theories, what knowledgewould be useful in their investigations and how this problem can be connectedwith designing an expert system."}
2.92e+03 pattern=  0 task=  0 (u=6518)  {'docs': u"In peer-to-peer systems, attrition attacks include both traditional,network-level denial of service attacks as well as application-level attacks inwhich malign peers conspire to waste loyal peers' resources. We describeseveral defenses for LOCKSS, a peer-to-peer digital preservation system, thathelp ensure that application-level attacks even from powerful adversaries areless effective than simple network-level attacks, and that network-levelattacks must be intense, wide-spread, and prolonged to impair the system."}
3.09e+03 pattern=  0 task=  0 (u=6519)  {'docs': u"The design of the defenses Internet systems can deploy against attack,especially adaptive and resilient defenses, must start from a realistic modelof the threat. This requires an assessment of the capabilities of theadversary. The design typically evolves through a process of simulating boththe system and the adversary. This requires the design and implementation of asimulated adversary based on the capability assessment. Consensus on thecapabilities of a suitable adversary is not evident. Part of the recentredesign of the protocol used by peers in the LOCKSS digital preservationsystem included a conservative assessment of the adversary's capabilities. Wepresent our assessment and the implications we drew from it as a step towards areusable adversary specification."}
2.54e+03 pattern=  0 task=  0 (u=6595)  {'docs': u'Power law distributions have been found in many natural and social phenomena,and more recently in the source code and run-time characteristics ofObject-Oriented (OO) systems. A power law implies that small values areextremely common, whereas large values are extremely rare. In this paper, weidentify twelve new power laws relating to the static graph structures of Javaprograms. The graph structures analyzed represented different forms of OOcoupling, namely, inheritance, aggregation, interface, parameter type andreturn type. Identification of these new laws provide the basis for predictinglikely features of classes in future developments. The research in this paperties together work in object-based coupling and World Wide Web structures.'}
2.57e+03 pattern=  0 task=  1 (u=6595)  {'docs': u'Decisions on which classes to refactor are fraught with difficulty. Theproblem of identifying candidate classes becomes acute when confronted withlarge systems comprising hundreds or thousands of classes. In this paper, wedescribe a metric by which key classes, and hence candidates for refactoring,can be identified. Measures quantifying the usage of two forms of coupling,inheritance and aggregation, together with two other class features (number ofmethods and attributes) were extracted from the source code of three large Javasystems. Our research shows that metrics from other research domains can beadapted to the software engineering process. Substantial differences were foundbetween each of the systems in terms of the key classes identified and henceopportunities for refactoring those classes varied between those systems.'}
2.61e+03 pattern=  0 task=  2 (u=6595)  {'docs': u'We present a new application for keyword search within relational databases,which uses a novel algorithm to solve the join discovery problem by findingMemex-like trails through the graph of foreign key dependencies. It differsfrom previous efforts in the algorithms used, in the presentation mechanism andin the use of primary-key only database queries at query-time to maintain afast response for users. We present examples using the DBLP data set.'}
2.54e+03 pattern=  0 task=  0 (u=6611)  {'docs': u'The analysis of the extremal structure of the scalar potentials of gaugedmaximally extended supergravity models in five, four, and three dimensions, andhence the determination of possible vacuum states of these models is acomputationally challenging task due to the occurrence of the exceptional Liegroups $E_6$, $E_7$, $E_8$ in the definition of these potentials. At present,the most promising approach to gain information about nontrivial vacua of thesemodels is to perform a truncation of the potential to submanifolds of the $G/H$coset manifold of scalars which are invariant under a subgroup of the gaugegroup and of sufficiently low dimension to make an analytic treatment possible.  New tools are presented which allow a systematic and highly effective studyof these potentials up to a previously unreached level of complexity. Explicitforms of new truncations of the potentials of four- and three-dimensionalmodels are given, and for N=16, D=3 supergravities, which are much more rich instructure than their higher-dimensional cousins, a series of new nontrivialvacua is identified and analysed.'}
2.92e+03 pattern=  0 task=  1 (u=6611)  {'docs': u'A prototype for an extensible interactive graphical term manipulation systemis presented that combines pattern matching and nondeterministic evaluation toprovide a convenient framework for doing tedious algebraic manipulations thatso far had to be done manually in a semi-automatic fashion.'}
2.52e+03 pattern=  0 task=  0 (u=6717)  {'docs': u'An NP-hard combinatorial optimization problem $\\Pi$ is said to have an {\\emapproximation threshold} if there is some $t$ such that the optimal value of$\\Pi$ can be approximated in polynomial time within a ratio of $t$, and it isNP-hard to approximate it within a ratio better than $t$. We survey some of theknown approximation threshold results, and discuss the pattern that emergesfrom the known results.'}
2.25e+03 pattern=  0 task=  0 (u=6783)  {'docs': u'We present a novel, general, optimally fast, incremental way of searching fora universal algorithm that solves each task in a sequence of tasks. The OptimalOrdered Problem Solver (OOPS) continually organizes and exploits previouslyfound solutions to earlier tasks, efficiently searching not only the space ofdomain-specific algorithms, but also the space of search algorithms.Essentially we extend the principles of optimal nonincremental universal searchto build an incremental universal learner that is able to improve itselfthrough experience. In illustrative experiments, our self-improver becomes thefirst general system that learns to solve all n disk Towers of Hanoi tasks(solution size 2^n-1) for n up to 30, profiting from previously solved, simplertasks involving samples of a simple context free language.'}
2.44e+03 pattern=  0 task=  1 (u=6783)  {'docs': u"Most traditional artificial intelligence (AI) systems of the past 50 yearsare either very limited, or based on heuristics, or both. The new millennium,however, has brought substantial progress in the field of theoretically optimaland practically feasible algorithms for prediction, search, inductive inferencebased on Occam's razor, problem solving, decision making, and reinforcementlearning in environments of a very general type. Since inductive inference isat the heart of all inductive sciences, some of the results are relevant notonly for AI and computer science but also for physics, provoking nontraditionalpredictions based on Zuse's thesis of the computer-generated universe."}
2.1e+03 pattern=  0 task=  0 (u=6878)  {'docs': u'We study program refactoring while considering the language or even theprogramming paradigm as a parameter. We use typed functional programs, namelyHaskell programs, as the specification medium for a corresponding refactoringframework. In order to detach ourselves from language syntax, ourspecifications adhere to the following style. (I) As for primitive algorithmsfor program analysis and transformation, we employ generic function combinatorssupporting generic traversal and polymorphic functions refined by ad-hoc cases.(II) As for the language abstractions involved in refactorings, we design adedicated multi-parameter class. This class can be instantiated forabstractions as present in various languages, e.g., Java, Prolog or Haskell.'}
2.14e+03 pattern=  0 task=  1 (u=6878)  {'docs': u'In previous work, we have introduced functional strategies, that is,first-class generic functions that can traverse into terms of any type whilemixing uniform and type-specific behaviour. In the present paper, we give adetailed description of one particular Haskell-based model of functionalstrategies. This model is characterised as follows. Firstly, we employfirst-class polymorphism as a form of second-order polymorphism as for the meretypes of functional strategies. Secondly, we use an encoding scheme of run-timetype case for mixing uniform and type-specific behaviour. Thirdly, we base alltraversal on a fundamental combinator for folding over constructorapplications.  Using this model, we capture common strategic traversal schemes in a highlyparameterised style. We study two original forms of parameterisation. Firstly,we design parameters for the specific control-flow, data-flow and traversalcharacteristics of more concrete traversal schemes. Secondly, we useoverloading to postpone commitment to a specific type scheme of traversal. Theresulting portfolio of traversal schemes can be regarded as a challengingbenchmark for setups for typed generic programming.  The way we develop the model and the suite of traversal schemes, it becomesclear that parameterised + typed strategic programming is best viewed as apotent combination of certain bits of parametric, intensional, polytypic, andad-hoc polymorphism.'}
2.17e+03 pattern=  0 task=  2 (u=6878)  {'docs': u"A typed model of strategic term rewriting is developed. The key innovation isthat generic traversal is covered. To this end, we define a typed rewritingcalculus S'_{gamma}. The calculus employs a many-sorted type system extended bydesignated generic strategy types gamma. We consider two generic strategytypes, namely the types of type-preserving and type-unifying strategies.S'_{gamma} offers traversal combinators to construct traversals or schemesthereof from many-sorted and generic strategies. The traversal combinatorsmodel different forms of one-step traversal, that is, they process theimmediate subterms of a given term without anticipating any scheme of recursioninto terms. To inhabit generic types, we need to add a fundamental combinatorto lift a many-sorted strategy $s$ to a generic type gamma. This step is calledstrategy extension. The semantics of the corresponding combinator states that sis only applied if the type of the term at hand fits, otherwise the extendedstrategy fails. This approach dictates that the semantics of strategyapplication must be type-dependent to a certain extent. Typed strategic termrewriting with coverage of generic term traversal is a simple but expressivemodel of generic programming. It has applications in program transformation andprogram analysis."}
2.39e+03 pattern=  0 task=  3 (u=6878)  {'docs': u'In previous work, we introduced the notion of functional strategies:first-class generic functions that can traverse terms of any type while mixinguniform and type-specific behaviour. Functional strategies transpose the notionof term rewriting strategies (with coverage of traversal) to the functionalprogramming paradigm. Meanwhile, a number of Haskell-based models andcombinator suites were proposed to support generic programming with functionalstrategies.  In the present paper, we provide a compact and matured reconstruction offunctional strategies. We capture strategic polymorphism by just two primitivecombinators. This is done without commitment to a specific functional language.We analyse the design space for implementational models of functionalstrategies. For completeness, we also provide an operational reference modelfor implementing functional strategies (in Haskell). We demonstrate thegenerality of our approach by reconstructing representative fragments of theStrafunski library for functional strategies.'}
2.38e+03 pattern=  0 task=  0 (u=6880)  {'docs': u'In this note we study the existence of a solution to the survey-propagationequations for the random K-satisfiability problem for a given instance. Weconjecture that when the number of variables goes to infinity, the solution ofthese equations for a given instance can be approximated by the solution of thecorresponding equations on an infinite tree. We conjecture (and we bringnumerical evidence) that the survey-propagation equations on the infinite treehave an unique solution in the suitable range of parameters.'}
2.39e+03 pattern=  0 task=  1 (u=6880)  {'docs': u'In this note we show that local equilibrium equations (the generalization ofthe TAP equations or of the belief propagation equations) do have solutions inthe colorable phase of the coloring problem. The same results extend to otheroptimization problems where the solutions has cost zero (e.g.K-satisfiability). On a random graph the solutions of the local equilibriumequations are associated to clusters of configurations (clustering states). Ona random graph the local equilibrium equations have solutions almost everywherein the uncolored phase; in this case we have to introduce the conceptquasi-solution of the local equilibrium equations.'}
2.42e+03 pattern=  0 task=  2 (u=6880)  {'docs': u'In this note we study the convergence of the survey decimation algorithm. Ananalytic formula for the reduction of the complexity during the decimation isderived. The limit of the converge of the algorithm are estimated in the randomcase: interesting phenomena appear near the boundary of convergence.'}
2.62e+03 pattern=  0 task=  3 (u=6880)  {'docs': u'In this note I will review some of the recent results that have been obtainedin the probabilistic approach to the random satisfiability problem. At thepresent moment the results are only heuristic. In the case of the random3-satisfiability problem a phase transition from the satisfiable to theunsatisfiable phase is found at $\\alpha=4.267$. There are other values of$\\alpha$ that separates different regimes and they will be described indetails. In this context the properties of the survey decimation algorithm willalso be discussed.'}
2.63e+03 pattern=  0 task=  0 (u=6889)  {'docs': u"We propose a sufficient condition for invertibility of a polynomial mappingfunction defined on a cube or simplex. This condition is applicable to finiteelement analysis using curved meshes. The sufficient condition is based on ananalysis of the Bernstein-B\\'ezier form of the columns of the derivative."}
2.63e+03 pattern=  0 task=  0 (u=6894)  {'docs': u'We describe R-GMA (Relational Grid Monitoring Architecture) which has beendeveloped within the European DataGrid Project as a Grid Information andMonitoring System. Is is based on the GMA from GGF, which is a simpleConsumer-Producer model. The special strength of this implementation comes fromthe power of the relational model. We offer a global view of the information asif each Virtual Organisation had one large relational database. We provide anumber of different Producer types with different characteristics; for examplesome support streaming of information. We also provide combinedConsumer/Producers, which are able to combine information and republish it. Atthe heart of the system is the mediator, which for any query is able to findand connect to the best Producers for the job. We have developed components toallow a measure of inter-working between MDS and R-GMA. We have used it bothfor information about the grid (primarily to find out about what services areavailable at any one time) and for application monitoring. R-GMA has beendeployed in various testbeds; we describe some preliminary results andexperiences of this deployment.'}
2.63e+03 pattern=  0 task=  0 (u=6914)  {'docs': u'Secure networks rely upon players to maintain security and reliability.However not every player can be assumed to have total loyalty and one must usemethods to uncover traitors in such networks. We use the original concept ofthe Byzantine Generals Problem by Lamport, and the more formal ByzantineAgreement describe by Linial, to nd traitors in secure networks. By applyinggeneral fault-tolerance methods to develop a more formal design of securenetworks we are able to uncover traitors amongst a group of players. We alsopropose methods to integrate this system with insecure channels. This newresiliency can be applied to broadcast and peer-to-peer secure communicationsystems where agents may be traitors or become unreliable due to faults.'}
2.66e+03 pattern=  0 task=  0 (u=6915)  {'docs': u'Within the literature on non-cooperative game theory, there have been anumber of attempts to propose logorithms which will compute Nash equilibria.Rather than derive a new algorithm, this paper shows that the family ofalgorithms known as Markov chain Monte Carlo (MCMC) can be used to calculateNash equilibria. MCMC is a type of Monte Carlo simulation that relies on Markovchains to ensure its regularity conditions. MCMC has been widely usedthroughout the statistics and optimization literature, where variants of thisalgorithm are known as simulated annealing. This paper shows that there isinteresting connection between the trembles that underlie the functioning ofthis algorithm and the type of Nash refinement known as trembling handperfection.'}
1.48e+03 pattern=  0 task=  0 (u=7179)  {'docs': u'The vertex-cover problem is studied for random graphs $G_{N,cN}$ having $N$vertices and $cN$ edges. Exact numerical results are obtained by abranch-and-bound algorithm. It is found that a transition in the coverabilityat a $c$-dependent threshold $x=x_c(c)$ appears, where $xN$ is the cardinalityof the vertex cover. This transition coincides with a sharp peak of the typicalnumerical effort, which is needed to decide whether there exists a cover with$xN$ vertices or not. For small edge concentrations $c\\ll 0.5$, a clusterexpansion is performed, giving very accurate results in this regime. Theseresults are extended using methods developed in statistical physics. The socalled annealed approximation reproduces a rigorous bound on $x_c(c)$ which wasknown previously. The main part of the paper contains an application of thereplica method. Within the replica symmetric ansatz the threshold $x_c(c)$ andthe critical backbone size $b_c(c)$ can be calculated. For $c<e/2$ the resultsshow an excellent agreement with the numerical findings. At average vertexdegree $2c=e$, an instability of the simple replica symmetric solution occurs.'}
3.04e+03 pattern=  0 task=  0 (u=7317)  {'docs': u'Many existing algorithms for model checking of infinite-state systems operateon constraints which are used to represent (potentially infinite) sets ofstates. A general powerful technique which can be employed for provingtermination of these algorithms is that of well quasi-orderings. Severalmethodologies have been proposed for derivation of new well quasi-orderedconstraint systems. However, many of these constraint systems suffer from a"constraint explosion problem", as the number of the generated constraintsgrows exponentially with the size of the problem. In this paper, we demonstratethat a refinement of the theory of well quasi-orderings, called the theory ofbetter quasi-orderings, is more appropriate for symbolic model checking, sinceit allows inventing constraint systems which are both well quasi-ordered andcompact. As a main application, we introduce existential zones, a constraintsystem for verification of systems with unboundedly many clocks and use ourmethodology to prove that existential zones are better quasi-ordered. We showhow to use existential zones in verification of timed Petri nets and presentsome experimental results. Also, we apply our methodology to derive newconstraint systems for verification of broadcast protocols, lossy channelsystems, and integral relational automata. The new constraint systems areexponentially more succinct than existing ones, and their well quasi-orderingcannot be shown by previous methods in the literature.'}
2.66e+03 pattern=  0 task=  0 (u=7398)  {'docs': u'Many programmers have had to deal with an overwritten variable resulting forexample from an aliasing problem. The culprit is obviously the lastwrite-access to that memory location before the manifestation of the bug. Theusual technique for removing such bugs starts with the debugger by (1) findingthe last write and (2) moving the control point of execution back to that timeby re-executing the program from the beginning. We wish to automate this. Step(2) is easy if we can somehow mark the last write found in step (1) and controlthe execution-point to move it back to this time.  In this paper we propose a new concept, position, that is, a point in theprogram execution trace, as needed for step (2) above. The position enablesdebuggers to automate the control of program execution to support commondebugging activities. We have implemented position in C by modifying GCC and inJava with a bytecode transformer. Measurements show that position can beprovided with an acceptable amount of overhead.'}
2.66e+03 pattern=  0 task=  0 (u=7400)  {'docs': u'This paper introduces an automatic debugging framework that relies onmodel-based reasoning techniques to locate faults in programs. In particular,model-based diagnosis, together with an abstract interpretation based conflictdetection mechanism is used to derive diagnoses, which correspond to possiblefaults in programs. Design information and partial specifications are appliedto guide a model revision process, which allows for automatic detection andcorrection of structural faults.'}
1.78e+03 pattern=  0 task=  0 (u=7402)  {'docs': u'We introduce a new model for studying quantum data structure problems -- the"quantum cell probe model". We prove a lower bound for the static predecessorproblem in the address-only version of this model where we allow quantumparallelism only over the `address lines\' of the queries. The address-onlyquantum cell probe model subsumes the classical cell probe model, and manyquantum query algorithms like Grover\'s algorithm fall into this framework. Ourlower bound improves the previous known lower bound for the predecessor problemin the classical cell probe model with randomised query schemes, and matchesthe classical deterministic upper bound of Beame and Fich. Beame and Fich havealso proved a matching lower bound for the predecessor problem, but only in theclassical deterministic setting. Our lower bound has the advantage that itholds for the more general quantum model, and also, its proof is substantiallysimpler than that of Beame and Fich. We prove our lower bound by obtaining around elimination lemma for quantum communication complexity. A similar lemmawas proved by Miltersen, Nisan, Safra and Wigderson for classical communicationcomplexity, but it was not strong enough to prove a lower bound matching theupper bound of Beame and Fich. Our quantum round elimination lemma also allowsus to prove rounds versus communication tradeoffs for some quantumcommunication complexity problems like the "greater-than" problem. We alsostudy the "static membership" problem in the quantum cell probe model.Generalising a result of Yao, we show that if the storage scheme is implicit,that is it can only store members of the subset and `pointers\', then anyquantum query scheme must make $\\Omega(\\log n)$ probes.'}
2.66e+03 pattern=  0 task=  1 (u=7402)  {'docs': u'We consider a fundamental problem in data structures, static predecessorsearching: Given a subset S of size n from the universe [m], store S so thatqueries of the form "What is the predecessor of x in S?" can be answeredefficiently. We study this problem in the cell probe model introduced by Yao.Recently, Beame and Fich obtained optimal bounds on the number of probes neededby any deterministic query scheme if the associated storage scheme uses onlyn^{O(1)} cells of word size (\\log m)^{O(1)} bits. We give a new lower boundproof for this problem that matches the bounds of Beame and Fich. Our lowerbound proof has the following advantages: it works for randomised query schemestoo, while Beame and Fich\'s proof works for deterministic query schemes only.It also extends to `quantum address-only\' query schemes that we define in thispaper, and is simpler than Beame and Fich\'s proof. We prove our lower boundusing the round elimination approach of Miltersen, Nisan, Safra and Wigderson.Using tools from information theory, we prove a strong round elimination lemmafor communication complexity that enables us to obtain a tight lower bound forthe predecessor problem. Our strong round elimination lemma also extends toquantum communication complexity. We also use our round elimination lemma toobtain a rounds versus communication tradeoff for the `greater-than\' problem,improving on the tradeoff in Miltersen et al. We believe that our roundelimination lemma is of independent interest and should have otherapplications.'}
1.79e+03 pattern=  0 task=  0 (u=7698)  {'docs': u'We propose a decoding method for the generalized algebraic geometry codesproposed by Xing et al. To show its practical usefulness, we give an example ofgeneralized algebraic geometry codes of length 567 over F_8 whose numbers ofcorrectable errors by the proposed method are larger than the shortened codesof the primitive BCH codes of length 4095 in the most range of dimension.'}
2.17e+03 pattern=  0 task=  0 (u=7724)  {'docs': u'This document gives an overview of a Grid testbed architecture proposal forthe NorduGrid project. The aim of the project is to establish an inter-Nordictestbed facility for implementation of wide area computing and data handling.The architecture is supposed to define a Grid system suitable for solving dataintensive problems at the Large Hadron Collider at CERN. We present the variousarchitecture components needed for such a system. After that we go on to give adescription of the dynamics by showing the task flow.'}
2.17e+03 pattern=  0 task=  0 (u=7725)  {'docs': u'This report presents results of the tests measuring the performance ofmulti-threaded file transfers, using the GridFTP implementation of the Globusproject over the NorduGrid network resources. Point to point WAN tests, carriedout between the sites of Copenhagen, Lund, Oslo and Uppsala, are described. Itwas found that multiple threaded download via the high performance GridFTPprotocol can significantly improve file transfer performance, and can serve asa reliable data'}
3.22e+03 pattern=  0 task=  0 (u=7754)  {'docs': u"In this paper we study functions with low influences on product probabilityspaces. The analysis of boolean functions with low influences has become acentral problem in discrete Fourier analysis. It is motivated by fundamentalquestions arising from the construction of probabilistically checkable proofsin theoretical computer science and from problems in the theory of socialchoice in economics.  We prove an invariance principle for multilinear polynomials with lowinfluences and bounded degree; it shows that under mild conditions thedistribution of such polynomials is essentially invariant for all productspaces. Ours is one of the very few known non-linear invariance principles. Ithas the advantage that its proof is simple and that the error bounds areexplicit. We also show that the assumption of bounded degree can be eliminatedif the polynomials are slightly ``smoothed''; this extension is essential forour applications to ``noise stability''-type problems.  In particular, as applications of the invariance principle we prove twoconjectures: the ``Majority Is Stablest'' conjecture from theoretical computerscience, which was the original motivation for this work, and the ``It Ain'tOver Till It's Over'' conjecture from social choice theory."}
2.65e+03 pattern=  0 task=  0 (u=7763)  {'docs': u'Error correcting codes are defined and important parameters for a code areexplained. Parameters of new codes constructed on algebraic surfaces arestudied. In particular, codes resulting from blowing up points in $\\proj^2$ arebriefly studied, then codes resulting from ruled surfaces are covered. Codesresulting from ruled surfaces over curves of genus 0 are completely analyzed,and some codes are discovered that are better than direct product Reed Solomoncodes of similar length. Ruled surfaces over genus 1 curves are also studied,but not all classes are completely analyzed. However, in this case a family ofcodes are found that are comparable in performance to the direct product codeof a Reed Solomon code and a Goppa code. Some further work is done on surfacesfrom higher genus curves, but there remains much work to be done in thisdirection to understand fully the resulting codes. Codes resulting from blowingpoints on surfaces are also studied, obtaining necessary parameters forconstructing infinite families of such codes.  Also included is a paper giving explicit formulas for curves with more\\field{q}-rational points than were previously known for certain combinationsof field size and genus. Some upper bounds are now known to be optimal fromthese examples.'}
2.95e+03 pattern=  0 task=  0 (u=7811)  {'docs': u'Quantum computations usually take place under the control of the classicalworld. We introduce a Classically-controlled Quantum Turing Machine (CQTM)which is a Turing Machine (TM) with a quantum tape for acting on quantum data,and a classical transition function for a formalized classical control. InCQTM, unitary transformations and measurements are allowed. We show that anyclassical TM is simulated by a CQTM without loss of efficiency. The gap betweenclassical and quantum computations, already pointed out in the framework ofmeasurement-based quantum computation is confirmed. To appreciate thesimilarity of programming classical TM and CQTM, examples are given.'}
3.16e+03 pattern=  0 task=  0 (u=7981)  {'docs': u'An attractive feature of BCH codes is that one can infer valuable informationfrom their design parameters (length, size of the finite field, and designeddistance), such as bounds on the minimum distance and dimension of the code. Inthis paper, it is shown that one can also deduce from the design parameterswhether or not a primitive, narrow-sense BCH contains its Euclidean orHermitian dual code. This information is invaluable in the construction ofquantum BCH codes. A new proof is provided for the dimension of BCH codes withsmall designed distance, and simple bounds on the minimum distance of suchcodes and their duals are derived as a consequence. These results allow us toderive the parameters of two families of primitive quantum BCH codes as afunction of their design parameters.'}
1.37e+03 pattern=  0 task=  0 (u=8001)  {'docs': u'The scalability of massively parallel algorithms is a fundamental question incomputer science. We study the scalability and the efficiency of a conservativemassively parallel algorithm for discrete-event simulations where the discreteevents are Poisson arrivals. The parallel algorithm is applicable to a widerange of problems, including dynamic Monte Carlo simulations for largeasynchronous systems with short-range interactions. The evolution of thesimulated time horizon is analogous to a growing and fluctuating surface, andthe efficiency of the algorithm corresponds to the density of local minima ofthis surface. In one dimension we find that the steady state of the macroscopiclandscape is governed by the Edwards-Wilkinson Hamiltonian, which implies thatthe algorithm is scalable. Preliminary results for higher-dimensional logicaltopologies are discussed.'}
2.24e+03 pattern=  0 task=  0 (u=8036)  {'docs': u"We improve on random sampling techniques for approximately solving problemsthat involve cuts and flows in graphs. We give a near-linear-time constructionthat transforms any graph on n vertices into an O(n\\log n)-edge graph on thesame vertices whose cuts have approximately the same value as the originalgraph's. In this new graph, for example, we can run the O(m^{3/2})-time maximumflow algorithm of Goldberg and Rao to find an s--t minimum cut in O(n^{3/2})time. This corresponds to a (1+epsilon)-times minimum s--t cut in the originalgraph. In a similar way, we can approximate a sparsest cut to within O(log n)in O(n^2) time using a previous O(mn)-time algorithm. A related approach leadsto a randomized divide and conquer algorithm producing an approximately maximumflow in O(m sqrt{n}) time."}
  919 pattern=  0 task=  0 (u=8037)  {'docs': u"We significantly improve known time bounds for solving the minimum cutproblem on undirected graphs. We use a ``semi-duality'' between minimum cutsand maximum spanning tree packings combined with our previously developedrandom sampling techniques. We give a randomized algorithm that finds a minimumcut in an m-edge, n-vertex graph with high probability in O(m log^3 n) time. Wealso give a simpler randomized algorithm that finds all minimum cuts with highprobability in O(n^2 log n) time. This variant has an optimal RNCparallelization. Both variants improve on the previous best time bound of O(n^2log^3 n). Other applications of the tree-packing approach are new, nearly tightbounds on the number of near minimum cuts a graph may have and a new datastructure for representing them in a space-efficient manner."}
3.26e+03 pattern=  0 task=  0 (u=8057)  {'docs': u'The language Timed Concurrent Constraint (tccp) is the extension over time ofthe Concurrent Constraint Programming (cc) paradigm that allows us to specifyconcurrent systems where timing is critical, for example reactive systems.Systems which may have an infinite number of states can be specified in tccp.Model checking is a technique which is able to verify finite-state systems witha huge number of states in an automatic way. In the last years several studieshave investigated how to extend model checking techniques to systems with aninfinite number of states. In this paper we propose an approach which exploitsthe computation model of tccp. Constraint based computations allow us to definea methodology for applying a model checking algorithm to (a class of)infinite-state systems. We extend the classical algorithm of model checking forLTL to a specific logic defined for the verification of tccp and to the tccpStructure which we define in this work for modeling the program behavior. Wedefine a restriction on the time in order to get a finite model and then wedevelop some illustrative examples. To the best of our knowledge this is thefirst approach that defines a model checking methodology for tccp.'}
3.26e+03 pattern=  0 task=  0 (u=8064)  {'docs': u'The performance of collaborative beamforming is analyzed using the theory ofrandom arrays. The statistical average and distribution of the beampattern ofrandomly generated phased arrays is derived in the framework of wireless ad hocsensor networks. Each sensor node is assumed to have a single isotropic antennaand nodes in the cluster collaboratively transmit the signal such that thesignal in the target direction is coherently added in the far- eld region. Itis shown that with N sensor nodes uniformly distributed over a disk, thedirectivity can approach N, provided that the nodes are located sparselyenough. The distribution of the maximum sidelobe peak is also studied. With theapplication to ad hoc networks in mind, two scenarios, closed-loop andopen-loop, are considered. Associated with these scenarios, the effects ofphase jitter and location estimation errors on the average beampattern are alsoanalyzed.'}
3.26e+03 pattern=  0 task=  0 (u=8067)  {'docs': u'The theory of Petri Nets provides a general framework to specify thebehaviors of real-time reactive systems and Time Petri Nets were introduced totake also temporal specifications into account. We present in this paper aforward zone-based algorithm to compute the state space of a bounded Time PetriNet: the method is different and more efficient than the classical State ClassGraph. We prove the algorithm to be exact with respect to the reachabilityproblem. Furthermore, we propose a translation of the computed state space intoa Timed Automaton, proved to be timed bisimilar to the original Time Petri Net.As the method produce a single Timed Automaton, syntactical clocks reductionmethods (Daws and Yovine for instance) may be applied to produce an automatonwith fewer clocks. Then, our method allows to model-check TTPN by the use ofefficient Timed Automata tools.  To appear in Theory and Practice of Logic Programming (TPLP).'}
2.57e+03 pattern=  0 task=  0 (u=8115)  {'docs': u'Every endofunctor of the category of classes is proved to be set-based in thesense of Aczel and Mendler, therefore, it has a final coalgebra. Other basicproperties of these endofunctors are proved, e.g. the existence of a freecompletely iterative theory.'}
3.26e+03 pattern=  0 task=  0 (u=8128)  {'docs': u'Carbon nanotubes are often seen as the only alternative technology to silicontransistors. While they are the most likely short-term one, other longer-termalternatives should be studied as well. While contemplating biological neuronsas an alternative component may seem preposterous at first sight, significantrecent progress in CMOS-neuron interface suggests this direction may not beunrealistic; moreover, biological neurons are known to self-assemble into verylarge networks capable of complex information processing tasks, something thathas yet to be achieved with other emerging technologies. The first step todesigning computing systems on top of biological neurons is to build anabstract model of self-assembled biological neural networks, much like computerarchitects manipulate abstract models of transistors and circuits. In thisarticle, we propose a first model of the structure of biological neuralnetworks. We provide empirical evidence that this model matches the biologicalneural networks found in living organisms, and exhibits the small-world graphstructure properties commonly found in many large and self-organized systems,including biological neural networks. More importantly, we extract the simplelocal rules and characteristics governing the growth of such networks, enablingthe development of potentially large but realistic biological neural networks,as would be needed for complex information processing/computing tasks. Based onthis model, future work will be targeted to understanding the evolution andlearning properties of such networks, and how they can be used to buildcomputing systems.'}
2.14e+03 pattern=  0 task=  0 (u=8161)  {'docs': u"This paper explores the connection between semantic equivalences andpreorders for concrete sequential processes, represented by means of labelledtransition systems, and formats of transition system specifications usingPlotkin's structural approach. For several preorders in the linear time -branching time spectrum a format is given, as general as possible, such thatthis preorder is a precongruence for all operators specifiable in that format.The formats are derived using the modal characterizations of the correspondingpreorders."}
3.22e+03 pattern=  0 task=  0 (u=8356)  {'docs': u'We study here preference revision, considering both the monotonic case wherethe original preferences are preserved and the nonmonotonic case where the newpreferences may override the original ones. We use a relational framework inwhich preferences are represented using binary relations (not necessarilyfinite). We identify several classes of revisions that preserve order axioms,for example the axioms of strict partial or weak orders. We considerapplications of our results to preference querying in relational databases.'}
3.22e+03 pattern=  0 task=  0 (u=8358)  {'docs': u'This paper is concerned with the characterization of the relationship betweentopology and traffic dynamics. We use a model of network generation that allowsthe transition from random to scale free networks. Specifically, we considerthree different topological types of network: random, scale-free with \\gamma =3, scale-free with \\gamma = 2. By using a novel LRD traffic generator, weobserve best performance, in terms of transmission rates and delivered packets,in the case of random networks. We show that, even if scale-free networks arecharacterized by shorter characteristic-path- length (the lower the exponent,the lower the path-length), they show worst performances in terms ofcommunication. We conjecture this could be explained in terms of changes in theload distribution, defined here as the number of shortest paths going through agiven vertex. In fact, that distribu- tion is characterized by (i) a decreasingmean (ii) an increas- ing standard deviation, as the networks becomesscale-free (especially scale-free networks with low exponents). The use of adegree-independent server also discriminates against a scale-free structure. Asa result, since the model is un- controlled, most packets will go through thesame vertices, favoring the onset of congestion.'}
2.53e+03 pattern=  0 task=  0 (u=8431)  {'docs': u'After a discussion of the Griesmer and Heller bound for the distance of aconvolutional code we present several codes with various parameters, overvarious fields, and meeting the given distance bounds. Moreover, the Griesmerbound is used for deriving a lower bound for the field size of an MDSconvolutional code and examples are presented showing that, in most cases, thelower bound is tight. Most of the examples in this paper are cyclicconvolutional codes in a generalized sense as it has been introduced in theseventies. A brief introduction to this promising type of cyclicity is given atthe end of the paper in order to make the examples more transparent.'}
2.74e+03 pattern=  0 task=  1 (u=8431)  {'docs': u'In this paper convolutional codes with cyclic structure will be investigated.These codes can be understood as left principal ideals in a suitableskew-polynomial ring. It has been shown in [3] that only certain combinationsof the parameters (field size, length, dimension, and Forney indices) can occurfor cyclic codes. We will investigate whether all these combinations can indeedbe realized by a suitable cyclic code and, if so, how to construct such a code.A complete characterization and construction will be given for minimal cycliccodes. It is derived from a detailed investigation of the units in theskew-polynomial ring.'}
3.05e+03 pattern=  0 task=  2 (u=8431)  {'docs': u"Cyclicity of a convolutional code (CC) is relying on a nontrivialautomorphism of the algebra F[x]/(x^n-1), where F is a finite field. If thisautomorphism itself has certain specific cyclicity properties one is lead tothe class of doubly-cyclic CC's. Within this large class Reed-Solomon and BCHconvolutional codes can be defined. After constructing doubly-cyclic CC's,basic properties are derived on the basis of which distance properties ofReed-Solomon convolutional codes are investigated.This shows that some of themare optimal or near optimal with respect to distance and performance."}
3.12e+03 pattern=  0 task=  3 (u=8431)  {'docs': u'A class of one-dimensional convolutional codes will be presented. They areall MDS codes, i. e., have the largest distance among all one-dimensional codesof the same length n and overall constraint length delta. Furthermore, theirextended row distances are computed, and they increase with slope n-delta. Incertain cases of the algebraic parameters, we will also derive parity checkmatrices of Vandermonde type for these codes. Finally, cyclicity in theconvolutional sense will be discussed for our class of codes. It will turn outthat they are cyclic if and only if the field element used in the generatormatrix has order n. This can be regarded as a generalization of the block codecase.'}
3.14e+03 pattern=  0 task=  4 (u=8431)  {'docs': u'Detailed information about the weight distribution of a convolutional code isgiven by the adjacency matrix of the state diagram associated with a controllercanonical form of the code. We will show that this matrix is an invariant ofthe code. Moreover, it will be proven that codes with the same adjacency matrixhave the same dimension and the same Forney indices and finally that forone-dimensional binary convolutional codes the adjacency matrix determines thecode uniquely up to monomial equivalence.'}
2.19e+03 pattern=  0 task=  0 (u=8508)  {'docs': u'This paper discusses the requirements of current and emerging applicationsbased on the Open Archives Initiative (OAI) and emphasizes the need for acommon infrastructure to support them. Inspired by HTTP proxy, cache, gatewayand web service concepts, a design for a scalable and reliable infrastructurethat aims at satisfying these requirements is presented. Moreover it is shownhow various applications can exploit the services included in the proposedinfrastructure. The paper concludes by discussing the current status of severalprototype implementations.'}
3.18e+03 pattern=  0 task=  1 (u=8508)  {'docs': u'The field of digital libraries (DLs) coalesced in 1994: the first digitallibrary conferences were held that year, awareness of the World Wide Web wasaccelerating, and the National Science Foundation awarded $24 Million (U.S.)for the Digital Library Initiative (DLI). In this paper we examine the state ofthe DL domain after a decade of activity by applying social network analysis tothe co-authorship network of the past ACM, IEEE, and joint ACM/IEEE digitallibrary conferences. We base our analysis on a common binary undirectionalnetwork model to represent the co-authorship network, and from it we extractseveral established network measures. We also introduce a weighted directionalnetwork model to represent the co-authorship network, for which we define$AuthorRank$ as an indicator of the impact of an individual author in thenetwork. The results are validated against conference program committee membersin the same period. The results show clear advantages of PageRank andAuthorRank over degree, closeness and betweenness centrality metrics. We alsoinvestigate the amount and nature of international participation in JointConference on Digital Libraries (JCDL).'}
3.18e+03 pattern=  0 task=  0 (u=8509)  {'docs': u'The paper analyzes the scalability of multiobjective estimation ofdistribution algorithms (MOEDAs) on a class of boundedly-difficultadditively-separable multiobjective optimization problems. The paperillustrates that even if the linkage is correctly identified, massivemultimodality of the search problems can easily overwhelm the nicher and leadto exponential scale-up. Facetwise models are subsequently used to propose agrowth rate of the number of differing substructures between the two objectivesto avoid the niching method from being overwhelmed and lead to polynomialscalability of MOEDAs.'}
3.17e+03 pattern=  0 task=  0 (u=8511)  {'docs': u'Generalisations of the bent property of a boolean function are presented, byproposing spectral analysis with respect to a well-chosen set of local unitarytransforms. Quadratic boolean functions are related to simple graphs and it isshown that the orbit generated by successive Local Complementations on a graphcan be found within the transform spectra under investigation. The flat spectraof a quadratic boolean function are related to modified versions of itsassociated adjacency matrix.'}
3.17e+03 pattern=  0 task=  1 (u=8511)  {'docs': u'In the first part of this paper [16], some results on how to compute the flatspectra of Boolean constructions w.r.t. the transforms {I,H}^n, {H,N}^n and{I,H,N}^n were presented, and the relevance of Local Complementation to thequadratic case was indicated. In this second part, the results are applied todevelop recursive formulae for the numbers of flat spectra of some structuralquadratics. Observations are made as to the generalised Bent properties ofboolean functions of algebraic degree greater than two, and the number of flatspectra w.r.t. {I,H,N}^n are computed for some of them.'}
1.57e+03 pattern=  0 task=  0 (u=8542)  {'docs': u"A drawing of a graph G in the plane is said to be a rectilinear drawing of Gif the edges are required to be line segments (as opposed to Jordan curves). Weassume no three vertices are collinear. The rectilinear crossing number of G isthe fewest number of edge crossings attainable over all rectilinear drawings ofG. Thanks to Richard Guy, exact values of the rectilinear crossing number ofK_n, the complete graph on n vertices, for n = 3,...,9, are known (Guy 1972,White and Beinke 1978, Finch 2000, Sloanes A014540). Since 1971, thanks to thework of David Singer (1971, Gardiner 1986), the rectilinear crossing number ofK_10 has been known to be either 61 or 62, a deceptively innocent andtantalizing statement. The difficulty of determining the correct value isevidenced by the fact that Singer's result has withstood the test of time. Inthis paper we use a purely combinatorial argument to show that the rectilinearcrossing number of K_10 is 62. Moreover, using this result, we improve anasymptotic lower bound for a related problem. Finally, we close with some newand old open questions that were provoked, in part, by the results of thispaper, and by the tangled history of the problem itself."}
2.43e+03 pattern=  0 task=  1 (u=8542)  {'docs': u'Message passing programs commonly use buffers to avoid unnecessarysynchronizations and to improve performance by overlapping communication withcomputation. Unfortunately, using buffers makes the program no longer portable,potentially unable to complete on systems without a sufficient number ofbuffers. Effective buffer use entails that the minimum number needed for a safeexecution be allocated.  We explore a variety of problems related to buffer allocation for safe andefficient execution of message passing programs. We show that determining theminimum number of buffers or verifying a buffer assignment are intractableproblems. However, we give a polynomial time algorithm to determine the minimumnumber of buffers needed to allow for asynchronous execution. We extend theseresults to several different buffering schemes, which in some cases make theproblems tractable.'}
2.45e+03 pattern=  0 task=  2 (u=8542)  {'docs': u"Among their many uses, growth processes (probabilistic amplification), wereused for constructing reliable networks from unreliable components, andderiving complexity bounds of various classes of functions. Hence, determiningthe initial conditions for such processes is an important and challengingproblem. In this paper we characterize growth processes by their initialconditions and derive conditions under which results such as Valiant's (1984)hold. First, we completely characterize growth processes that use linearconnectives. Second, by extending Savick\\'y's (1990) analysis, via``Restriction Lemmas'', we characterize growth processes that use monotoneconnectives, and show that our technique is applicable to growth processes thatuse other connectives as well. Additionally, we obtain explicit bounds on theconvergence rates of several growth processes, including the growth processstudied by Savick\\'y (1990)."}
1.6e+03 pattern=  0 task=  0 (u=8782)  {'docs': u'Two general algorithms based on opportunity costs are given for approximatinga revenue-maximizing set of bids an auctioneer should accept, in acombinatorial auction in which each bidder offers a price for some subset ofthe available goods and the auctioneer can only accept non-intersecting bids.Since this problem is difficult even to approximate in general, the algorithmsare most useful when the bids are restricted to be connected node subsets of anunderlying object graph that represents which objects are relevant to eachother. The approximation ratios of the algorithms depend on structuralproperties of this graph and are small constants for many interesting familiesof object graphs. The running times of the algorithms are linear in the size ofthe bid graph, which describes the conflicts between bids. Extensions of thealgorithms allow for efficient processing of additional constraints, such asbudget constraints that associate bids with particular bidders and limit howmany bids from a particular bidder can be accepted.'}
1.71e+03 pattern=  0 task=  1 (u=8782)  {'docs': u'This paper develops three polynomial-time pricing techniques for EuropeanAsian options with provably small errors, where the stock prices followbinomial trees or trees of higher-degree. The first technique is the firstknown Monte Carlo algorithm with analytical error bounds suitable for pricingsingle-stock options with meaningful confidence and speed. The second techniqueis a general recursive bucketing-based scheme that can use theAingworth-Motwani-Oldham aggregation algorithm, Monte-Carlo simulation andpossibly others as the base-case subroutine. This scheme enables robusttrade-offs between accuracy and time over subtrees of different sizes. Forlong-term options or high frequency price averaging, it can price single-stockoptions with smaller errors in less time than the base-case algorithmsthemselves. The third technique combines Fast Fourier Transform withbucketing-based schemes for pricing basket options. This technique takespolynomial time in the number of days and the number of stocks, and does notadd any errors to those already incurred in the companion bucketing scheme.This technique assumes that the price of each underlying stock movesindependently.'}
1.62e+03 pattern=  0 task=  0 (u=8783)  {'docs': u'In this paper, we study the problem of designing proxies (or portfolios) forvarious stock market indices based on historical data. We use four differentmethods for computing market indices, all of which are formulas used in actualstock market analysis. For each index, we consider three criteria for designingthe proxy: the proxy must either track the market index, outperform the marketindex, or perform within a margin of error of the index while maintaining a lowvolatility. In eleven of the twelve cases (all combinations of four indiceswith three criteria except the problem of sacrificing return for lessvolatility using the price-relative index) we show that the problem is NP-hard,and hence most likely intractable.'}
1.7e+03 pattern=  0 task=  1 (u=8783)  {'docs': u'We study on-line strategies for solving problems with hybrid algorithms.There is a problem Q and w basic algorithms for solving Q. For some lambda <=w, we have a computer with lambda disjoint memory areas, each of which can beused to run a basic algorithm and store its intermediate results. In the worstcase, only one basic algorithm can solve Q in finite time, and all the otherbasic algorithms run forever without solving Q. To solve Q with a hybridalgorithm constructed from the basic algorithms, we run a basic algorithm forsome time, then switch to another, and continue this process until Q is solved.The goal is to solve Q in the least amount of time. Using competitive ratios tomeasure the efficiency of a hybrid algorithm, we construct an optimaldeterministic hybrid algorithm and an efficient randomized hybrid algorithm.This resolves an open question on searching with multiple robots posed byBaeza-Yates, Culberson and Rawlins. We also prove that our randomized algorithmis optimal for lambda = 1, settling a conjecture of Kao, Reif and Tate.'}
1.86e+03 pattern=  0 task=  2 (u=8783)  {'docs': u'This work initiates research into the problem of determining an optimalinvestment strategy for investors with different attitudes towards thetrade-offs of risk and profit. The probability distribution of the returnvalues of the stocks that are considered by the investor are assumed to beknown, while the joint distribution is unknown. The problem is to find the bestinvestment strategy in order to minimize the probability of losing a certainpercentage of the invested capital based on different attitudes of theinvestors towards future outcomes of the stock market.  For portfolios made up of two stocks, this work shows how to exactly andquickly solve the problem of finding an optimal portfolio for aggressive orrisk-averse investors, using an algorithm based on a fast greedy solution to amaximum flow problem. However, an investor looking for an average-caseguarantee (so is neither aggressive or risk-averse) must deal with a moredifficult problem. In particular, it is #P-complete to compute the distributionfunction associated with the average-case bound. On the positive side,approximate answers can be computed by using random sampling techniques similarto those for high-dimensional volume estimation. When k>2 stocks areconsidered, it is proved that a simple solution based on the same flow conceptsas the 2-stock algorithm would imply that P = NP, so is highly unlikely. Thiswork gives approximation algorithms for this case as well as exact algorithmsfor some important special cases.'}
2.01e+03 pattern=  0 task=  3 (u=8783)  {'docs': u'We propose a mathematical model of DNA self-assembly using 2D tiles to form3D nanostructures. This is the first work to combine studies in self-assemblyand nanotechnology in 3D, just as Rothemund and Winfree did in the 2D case. Ourmodel is a more precise superset of their Tile Assembly Model that facilitatesbuilding scalable 3D molecules. Under our model, we present algorithms to builda hollow cube, which is intuitively one of the simplest 3D structures toconstruct. We also introduce five basic measures of complexity to analyze thesealgorithms. Our model and algorithmic techniques are applicable to more complex2D and 3D nanostructures.'}
1.7e+03 pattern=  0 task=  0 (u=8785)  {'docs': u'We propose performance profiles-distribution functions for a performancemetric-as a tool for benchmarking and comparing optimization software. We showthat performance profiles combine the best features of other tools forperformance evaluation.'}
1.68e+03 pattern=  0 task=  0 (u=8786)  {'docs': u'We discuss the role of automatic differentiation tools in optimizationsoftware. We emphasize issues that are important to large-scale optimizationand that have proved useful in the installation of nonlinear solvers in theNEOS Server. Our discussion centers on the computation of the gradient andHessian matrix for partially separable functions and shows that the gradientand Hessian matrix can be computed with guaranteed bounds in time and memoryrequirements'}
2.99e+03 pattern=  0 task=  0 (u=8835)  {'docs': u"The solution of large, sparse constrained least-squares problems is a staplein scientific and engineering applications. However, currently available codesfor such problems are proprietary or based on MATLAB. We announce a freelyavailable C implementation of the fast block pivoting algorithm of Portugal,Judice, and Vicente. Our version is several times faster than Matstoms' MATLABimplementation of the same algorithm. Further, our code matches the accuracy ofMATLAB's built-in lsqnonneg function."}
2.99e+03 pattern=  0 task=  0 (u=8847)  {'docs': u'Future smart environments will be characterized by multiple nodes that sense,collect, and disseminate information about environmental phenomena through awireless network. In this paper, we define a set of applications that require anew form of distributed knowledge about the environment, referred to asnon-uniform information granularity. By non-uniform information granularity wemean that the required accuracy or precision of information is proportional tothe distance between a source node (information producer) and current sink node(information consumer). That is, as the distance between the source node andsink node increases, loss in information precision is acceptable. Applicationsthat can benefit from this type of knowledge range from battlefield scenariosto rescue operations. The main objectives of this paper are two-fold: first, wewill precisely define non-uniform information granularity, and second, we willdescribe different protocols that achieve non-uniform information disseminationand analyze these protocols based on complexity, energy consumption, andaccuracy of information.'}
3.22e+03 pattern=  0 task=  1 (u=8847)  {'docs': u'The heterogeneity and resource constraints of sense-and-respond systems posesignificant challenges to system and application development. In this paper, wepresent a flexible, intuitive file system abstraction for organizing andmanaging sense-and-respond systems based on the Plan 9 design principles. A keyfeature of this abstraction is the ability to support multiple views of thesystem via filesystem namespaces. Constructed logical views present anapplication-specific representation of the network, thus enabling high-levelprogramming of the network. Concurrently, structural views of the networkenable resource-efficient planning and execution of tasks. We present andmotivate the design using several examples, outline research challenges and ourresearch plan to address them, and describe the current state ofimplementation.'}
2.99e+03 pattern=  0 task=  0 (u=8851)  {'docs': u'In this paper we discuss the optimizing compilation of Constraint HandlingRules (CHRs). CHRs are a multi-headed committed choice constraint language,commonly applied for writing incremental constraint solvers. CHRs are usuallyimplemented as a language extension that compiles to the underlying language.In this paper we show how we can use different kinds of information in thecompilation of CHRs in order to obtain access efficiency, and a bettertranslation of the CHR rules into the underlying language, which in this caseis HAL. The kinds of information used include the types, modes, determinism,functional dependencies and symmetries of the CHR constraints. We also show howto analyze CHR programs to determine this information about functionaldependencies, symmetries and other kinds of information supportingoptimizations.'}
3.03e+03 pattern=  0 task=  0 (u=8852)  {'docs': u"Recent constraint logic programming (CLP) languages, such as HAL and Mercury,require type, mode and determinism declarations for predicates. Thisinformation allows the generation of efficient target code and the detection ofmany errors at compile-time. Unfortunately, mode checking in such languages isdifficult. One of the main reasons is that, for each predicate modedeclaration, the compiler is required to appropriately re-order literals in thepredicate's definition. The task is further complicated by the need to handlecomplex instantiations (which interact with type declarations and higher-orderpredicates) and automatic initialization of solver variables. Here we definemode checking for strongly typed CLP languages which require reordering ofclause body literals. In addition, we show how to handle a simple case ofpolymorphic modes by using the corresponding polymorphic types."}
2.49e+03 pattern=  0 task=  0 (u=8974)  {'docs': u'We present the first explicit connection between quantum computation andlattice problems. Namely, we show a solution to the Unique Shortest VectorProblem (SVP) under the assumption that there exists an algorithm that solvesthe hidden subgroup problem on the dihedral group by coset sampling. Moreover,we solve the hidden subgroup problem on the dihedral group by using an averagecase subset sum routine. By combining the two results, we get a quantumreduction from $\\Theta(n^{2.5})$-unique-SVP to the average case subset sumproblem.'}
2.67e+03 pattern=  0 task=  1 (u=8974)  {'docs': u"We introduce the use of Fourier analysis on lattices as an integral part of alattice based construction. The tools we develop provide an elegant descriptionof certain Gaussian distributions around lattice points. Our results includetwo cryptographic constructions which are based on the worst-case hardness ofthe unique shortest vector problem. The main result is a new public keycryptosystem whose security guarantee is considerably stronger than previousresults ($O(n^{1.5})$ instead of $O(n^7)$). This provides the first alternativeto Ajtai and Dwork's original 1996 cryptosystem. Our second result is a familyof collision resistant hash functions which, apart from improving the securityin terms of the unique shortest vector problem, is also the first example of ananalysis which is not based on Ajtai's iterative step. Surprisingly, bothresults are derived from one theorem which presents two indistinguishabledistributions on the segment $[0,1)$. It seems that this theorem can havefurther applications and as an example we mention how it can be used to solvean open problem related to quantum computation."}
2.94e+03 pattern=  0 task=  0 (u=9009)  {'docs': u'A nearly logarithmic lower bound on the randomized competitive ratio for themetrical task systems problem is presented. This implies a similar lower boundfor the extensively studied k-server problem. The proof is based on Ramsey-typetheorems for metric spaces, that state that every metric space contains a largesubspace which is approximately a hierarchically well-separated tree (and inparticular an ultrametric). These Ramsey-type theorems may be of independentinterest.'}
3.15e+03 pattern=  0 task=  0 (u=9090)  {'docs': u'This two-parts paper offers a survey of linear logic and ludics, which wereintroduced by Girard in 1986 and 2001, respectively. Both theories revisitmathematical logic from first principles, with inspiration from andapplications to computer science. The present part I covers an introduction tothe connectives and proof rules of linear logic, to its decidabilityproperties, and to its models. Part II will deal with proof nets, a graph-likerepresentation of proofs which is one of the major innovations of linear logic,and will present an introduction to ludics.'}
3.15e+03 pattern=  0 task=  1 (u=9090)  {'docs': u'This paper is the second part of an introduction to linear logic and ludics,both due to Girard. It is devoted to proof nets, in the limited, yet central,framework of multiplicative linear logic and to ludics, which has been recentlydevelopped in an aim of further unveiling the fundamental interactive nature ofcomputation and logic. We hope to offer a few computer science insights intothis new theory.'}
3.15e+03 pattern=  0 task=  0 (u=9092)  {'docs': u'in this paper we describe a method which allows agents to dynamically selectprotocols and roles when they need to execute collaborative tasks'}
2.37e+03 pattern=  0 task=  0 (u=9106)  {'docs': u'We present a new structural (or syntatic) approach for estimating thesatisfiability threshold of random 3-SAT formulae. We show its efficiency inobtaining a jump from the previous upper bounds, lowering them to 4.506. Themethod combines well with other techniques, and also applies to other problems,such as the 3-colourability of random graphs.'}
1.34e+03 pattern=  0 task=  0 (u=9204)  {'docs': u'In this paper, we focus on the problem of existence and computing of smalland large stable models. We show that for every fixed integer k, there is alinear-time algorithm to decide the problem LSM (large stable models problem):does a logic program P have a stable model of size at least |P|-k. In contrast,we show that the problem SSM (small stable models problem) to decide whether alogic program P has a stable model of size at most k is much harder. We presenttwo algorithms for this problem but their running time is given by polynomialsof order depending on k. We show that the problem SSM is fixed-parameterintractable by demonstrating that it is W[2]-hard. This result implies that itis unlikely, an algorithm exists to compute stable models of size at most kthat would run in time O(n^c), where c is a constant independent of k. We alsoprovide an upper bound on the fixed-parameter complexity of the problem SSM byshowing that it belongs to the class W[3].'}
  918 pattern=  0 task=  0 (u=9220)  {'docs': u'We consider the problem of coloring k-colorable graphs with the fewestpossible colors. We present a randomized polynomial time algorithm that colorsa 3-colorable graph on $n$ vertices with min O(Delta^{1/3} log^{1/2} Delta logn), O(n^{1/4} log^{1/2} n) colors where Delta is the maximum degree of anyvertex. Besides giving the best known approximation ratio in terms of n, thismarks the first non-trivial approximation result as a function of the maximumdegree Delta. This result can be generalized to k-colorable graphs to obtain acoloring using min O(Delta^{1-2/k} log^{1/2} Delta log n), O(n^{1-3/(k+1)}log^{1/2} n) colors. Our results are inspired by the recent work of Goemans andWilliamson who used an algorithm for semidefinite optimization problems, whichgeneralize linear programs, to obtain improved approximations for the MAX CUTand MAX 2-SAT problems. An intriguing outcome of our work is a dualityrelationship established between the value of the optimum solution to oursemidefinite program and the Lovasz theta-function. We show lower bounds on thegap between the optimum solution of our semidefinite program and the actualchromatic number; by duality this also demonstrates interesting new facts aboutthe theta-function.'}
3.05e+03 pattern=  0 task=  0 (u=9340)  {'docs': u'Project SETI@HOME has proven to be one of the biggest successes ofdistributed computing during the last years. With a quite simple approach SETImanages to process large volumes of data using a vast amount of distributedcomputer power.  To extend the generic usage of this kind of distributed computing tools,BOINC is being developed. In this paper we propose HEP@HOME, a BOINC versiontailored to the specific requirements of the High Energy Physics (HEP)community.  The HEP@HOME will be able to process large amounts of data using virtuallyunlimited computing power, as BOINC does, and it should be able to workaccording to HEP specifications. In HEP the amounts of data to be analyzed orreconstructed are of central importance. Therefore, one of the designprinciples of this tool is to avoid data transfer. This will allow scientiststo run their analysis applications and taking advantage of a large number ofCPUs. This tool also satisfies other important requirements in HEP, namely,security, fault-tolerance and monitoring.'}
  969 pattern=  0 task=  0 (u=9392)  {'docs': u'In this paper we consider quantum interactive proof systems, i.e.,interactive proof systems in which the prover and verifier may perform quantumcomputations and exchange quantum messages. It is proved that every language inPSPACE has a quantum interactive proof system that requires only two rounds ofcommunication between the prover and verifier, while having exponentially small(one-sided) probability of error. It follows that quantum interactive proofsystems are strictly more powerful than classical interactive proof systems inthe constant-round case unless the polynomial time hierarchy collapses to thesecond level.'}
2.46e+03 pattern=  0 task=  0 (u=9449)  {'docs': u'There is a growing interest in using Kalman-filter models for brainmodelling. In turn, it is of considerable importance to represent Kalman-filterin connectionist forms with local Hebbian learning rules. To our bestknowledge, Kalman-filter has not been given such local representation. It seemsthat the main obstacle is the dynamic adaptation of the Kalman-gain. Here, aconnectionist representation is presented, which is derived by means of therecursive prediction error method. We show that this method gives rise toattractive local learning rules and can adapt the Kalman-gain.'}
2.6e+03 pattern=  0 task=  0 (u=9486)  {'docs': u'We consider algorithms for preprocessing labelled lists and trees so that,for any two nodes u and v we can answer queries of the form: What is the modeor median label in the sequence of labels on the path from u to v.'}
1.94e+03 pattern=  0 task=  0 (u=9501)  {'docs': u'We address the problem of complementing higher-order patterns withoutrepetitions of existential variables. Differently from the first-order case,the complement of a pattern cannot, in general, be described by a pattern, oreven by a finite set of patterns. We therefore generalize the simply-typedlambda-calculus to include an internal notion of strict function so that we candirectly express that a term must depend on a given variable. We show that, inthis more expressive calculus, finite sets of patterns without repeatedvariables are closed under complement and intersection. Our principalapplication is the transformational approach to negation in higher-order logicprograms.'}
3.23e+03 pattern=  0 task=  0 (u=9576)  {'docs': u'It is now widely accepted that the CMOS technology implementing irreversiblelogic will hit a scaling limit beyond 2016, and that the increased powerdissipation is a major limiting factor. Reversible computing can potentiallyrequire arbitrarily small amounts of energy. Recently several nano-scaledevices which have the potential to scale, and which naturally performreversible logic, have emerged. This paper addresses several fundamental issuesthat need to be addressed before any nano-scale reversible computing systemscan be realized, including reliability and performance trade-offs andarchitecture optimization. Many nano-scale devices will be limited to only nearneighbor interactions, requiring careful optimization of circuits. We provideefficient fault-tolerant (FT) circuits when restricted to both 2D and 1D.Finally, we compute bounds on the entropy (and hence, heat) generated by our FTcircuits and provide quantitative estimates on how large can we make ourcircuits before we lose any advantage over irreversible computing.'}
2.89e+03 pattern=  0 task=  0 (u=9628)  {'docs': u"The objective of this paper is to develop a functional programming languagefor quantum computers. We develop a lambda calculus for the classical controlmodel, following the first author's work on quantum flow-charts. We define acall-by-value operational semantics, and we give a type system using affineintuitionistic linear logic. The main results of this paper are the safetyproperties of the language and the development of a type inference algorithm."}
2.24e+03 pattern=  0 task=  0 (u=9631)  {'docs': u'A relational database is inconsistent if it does not satisfy a given set ofintegrity constraints. Nevertheless, it is likely that most of the data in itis consistent with the constraints. In this paper we apply logic programmingbased on answer sets to the problem of retrieving consistent information from apossibly inconsistent database. Since consistent information persists from theoriginal database to every of its minimal repairs, the approach is based on aspecification of database repairs using disjunctive logic programs withexceptions, whose answer set semantics can be represented and computed bysystems that implement stable model semantics. These programs allow us todeclare persistence by defaults and repairing changes by exceptions. Weconcentrate mainly on logic programs for binary integrity constraints, amongwhich we find most of the integrity constraints found in practice.'}
3.03e+03 pattern=  0 task=  0 (u=9651)  {'docs': u'Biological systems rely on robust internal information processing: Survivaldepends on highly reproducible dynamics of regulatory processes. Biologicalinformation processing elements, however, are intrinsically noisy (geneticswitches, neurons, etc.). Such noise poses severe stability problems to systembehavior as it tends to desynchronize system dynamics (e.g. via fluctuatingresponse or transmission time of the elements). Synchronicity in parallelinformation processing is not readily sustained in the absence of a centralclock. Here we analyze the influence of topology on synchronicity in networksof autonomous noisy elements. In numerical and analytical studies we find aclear distinction between non-reliable and reliable dynamical attractors,depending on the topology of the circuit. In the reliable cases, synchronicityis sustained, while in the unreliable scenario, fluctuating responses of singleelements can gradually desynchronize the system, leading to non-reproduciblebehavior. We find that the fraction of reliable dynamical attractors stronglycorrelates with the underlying circuitry. Our model suggests that the observedmotif structure of biological signaling networks is shaped by the biologicalrequirement for reproducibility of attractors.'}
2.83e+03 pattern=  0 task=  0 (u=9734)  {'docs': u'Recently, researchers have applied genetic algorithms (GAs) to address someproblems in quantum computation. Also, there has been some works in thedesigning of genetic algorithms based on quantum theoretical concepts andtechniques. The so called Quantum Evolutionary Programming has two majorsub-areas: Quantum Inspired Genetic Algorithms (QIGAs) and Quantum GeneticAlgorithms (QGAs). The former adopts qubit chromosomes as representations andemploys quantum gates for the search of the best solution. The later tries tosolve a key question in this field: what GAs will look like as animplementation on quantum hardware? As we shall see, there is not a completeanswer for this question. An important point for QGAs is to build a quantumalgorithm that takes advantage of both the GA and quantum computing parallelismas well as true randomness provided by quantum computers. In the first part ofthis paper we present a survey of the main works in GAs plus quantum computingincluding also our works in this area. Henceforth, we review some basicconcepts in quantum computation and GAs and emphasize their inherentparallelism. Next, we review the application of GAs for learning quantumoperators and circuit design. Then, quantum evolutionary programming isconsidered. Finally, we present our current research in this field and someperspectives.'}
2.23e+03 pattern=  0 task=  0 (u=9787)  {'docs': u'In this paper we consider three different kinds of domain-dependent controlknowledge (temporal, procedural and HTN-based) that are useful in planning. Ourapproach is declarative and relies on the language of logic programming withanswer set semantics (AnsProlog*). AnsProlog* is designed to plan withoutcontrol knowledge. We show how temporal, procedural and HTN-based controlknowledge can be incorporated into AnsProlog* by the modular addition of asmall number of domain-dependent rules, without the need to modify the planner.We formally prove the correctness of our planner, both in the absence andpresence of the control knowledge. Finally, we perform some initialexperimentation that demonstrates the potential reduction in planning time thatcan be achieved when procedural domain knowledge is used to solve planningproblems with large plan length.'}
2.22e+03 pattern=  0 task=  0 (u=9792)  {'docs': u'We apply a Bethe-Peierls approach to statistical-mechanics models defined onrandom networks of arbitrary degree distribution and arbitrary correlationsbetween the degrees of neighboring vertices. Using the NP-hard optimizationproblem of finding minimal vertex covers on these graphs, we show that suchcorrelations may lead to a qualitatively different solution structure ascompared to uncorrelated networks. This results in a higher complexity of thenetwork in a computational sense: Simple heuristic algorithms fail to find aminimal vertex cover in the highly correlated case, whereas uncorrelatednetworks seem to be simple from the point of view of combinatorialoptimization.'}
3.17e+03 pattern=  0 task=  0 (u=10176)  {'docs': u'We consider the simulation of wireless sensor networks (WSN) using a newapproach. We present Shawn, an open-source discrete-event simulator that hasconsiderable differences to all other existing simulators. Shawn is verypowerful in simulating large scale networks with an abstract point of view. Itis, to the best of our knowledge, the first simulator to support generichigh-level algorithms as well as distributed protocols on exactly the sameunderlying networks.'}
2.69e+03 pattern=  0 task=  0 (u=10177)  {'docs': u'Higher-dimensional orthogonal packing problems have a wide range of practicalapplications, including packing, cutting, and scheduling. Previous efforts forexact algorithms have been unable to avoid structural problems that appear forinstances in two- or higher-dimensional space. We present a new approach formodeling packings, using a graph-theoretical characterization of feasiblepackings. Our characterization allows it to deal with classes of packings thatshare a certain combinatorial structure, instead of having to consider onepacking at a time. In addition, we can make use of elegant algorithmicproperties of certain classes of graphs. This allows our characterization to bethe basis for a successful branch-and-bound framework.  This is the first in a series of papers describing new approaches tohigher-dimensional packing.'}
2.82e+03 pattern=  0 task=  1 (u=10177)  {'docs': u'Higher-dimensional orthogonal packing problems have a wide range of practicalapplications, including packing, cutting, and scheduling. In the context of abranch-and-bound framework for solving these packing problems to optimality, itis of crucial importance to have good and easy bounds for an optimal solution.Previous efforts have produced a number of special classes of such bounds.Unfortunately, some of these bounds are somewhat complicated and hard togeneralize. We present a new approach for obtaining classes of lower bounds forhigher-dimensional packing problems; our bounds improve and simplify severalwell-known bounds from previous literature. In addition, our approach providesan easy framework for proving correctness of new bounds.'}
2.52e+03 pattern=  0 task=  0 (u=10203)  {'docs': u'We survey a collective achievement of a group of researchers: the PCPTheorems. They give new definitions of the class \\np, and imply that computingapproximate solutions to many \\np-hard problems is itself \\np-hard. Techniquesdeveloped to prove them have had many other consequences.'}
2.66e+03 pattern=  2 task=  0 (u=10229)  {'docs': u"In the paper very efficient, linear in number of arcs, algorithms fordetermining Hummon and Doreian's arc weights SPLC and SPNP in citation networkare proposed, and some theoretical properties of these weights are presented.The nonacyclicity problem in citation networks is discussed. An approach toidentify on the basis of arc weights an important small subnetwork is proposedand illustrated on the citation networks of SOM (self organizing maps)literature and US patents."}
2.95e+03 pattern=  0 task=  0 (u=10272)  {'docs': u'We model the evolution of the Internet at the Autonomous System level as aprocess of competition for users and adaptation of bandwidth capability. Wefind the exponent of the degree distribution as a simple function of the growthrates of the number of autonomous systems and the total number of connectionsin the Internet, both empirically measurable quantities. This fact place ourmodel apart from others in which this exponent depends on parameters that needto be adjusted in a model dependent way. Our approach also accounts for a highlevel of clustering as well as degree-degree correlations, both with the samehierarchical structure present in the real Internet. Further, it alsohighlights the interplay between bandwidth, connectivity and traffic of thenetwork.'}
3.16e+03 pattern=  0 task=  0 (u=10563)  {'docs': u'An extension of the traditional two-armed bandit problem is considered, inwhich the decision maker has access to some side information before decidingwhich arm to pull. At each time t, before making a selection, the decisionmaker is able to observe a random variable X_t that provides some informationon the rewards to be obtained. The focus is on finding uniformly good rules(that minimize the growth rate of the inferior sampling time) and onquantifying how much the additional information helps. Various settings areconsidered and for each setting, lower bounds on the achievable inferiorsampling time are developed and asymptotically optimal adaptive schemesachieving these lower bounds are constructed.'}
2.5e+03 pattern=  0 task=  0 (u=10630)  {'docs': u'We propose public-key cryptosystems with public key a system of polynomialequations, algebraic or differential, and private key a single polynomial or asmall-size ideal. We set up probabilistic encryption, signature, andsigncryption protocols.'}
2.54e+03 pattern=  0 task=  1 (u=10630)  {'docs': u'I transform the trapdoor problem of HFE into a linear algebra problem.'}
3.23e+03 pattern=  0 task=  0 (u=10735)  {'docs': u'We continue the investigation of problems concerning correlation clusteringor clustering with qualitative information, which is a clustering formulationthat has been studied recently. The basic setup here is that we are given asinput a complete graph on n nodes (which correspond to nodes to be clustered)whose edges are labeled + (for similar pairs of items) and - (for dissimilarpairs of items). Thus we have only as input qualitative information onsimilarity and no quantitative distance measure between items. The quality of aclustering is measured in terms of its number of agreements, which is simplythe number of edges it correctly classifies, that is the sum of number of -edges whose endpoints it places in different clusters plus the number of +edges both of whose endpoints it places within the same cluster.  In this paper, we study the problem of finding clusterings that maximize thenumber of agreements, and the complementary minimization version where we seekclusterings that minimize the number of disagreements. We focus on thesituation when the number of clusters is stipulated to be a small constant k.Our main result is that for every k, there is a polynomial time approximationscheme for both maximizing agreements and minimizing disagreements. (Theproblems are NP-hard for every k >= 2.) The main technical work is for theminimization version, as the PTAS for maximizing agreements follows along thelines of the property tester for Max k-CUT.  In contrast, when the number of clusters is not specified, the problem ofminimizing disagreements was shown to be APX-hard, even though the maximizationversion admits a PTAS.'}
 58.7 pattern=  1 task=  0 (u=10812)  {'docs': u'A machine translation system is said to be *complete* if all expressions thatare correct according to the source-language grammar can be translated into thetarget language. This paper addresses the completeness issue for compositionalmachine translation in general, and for compositional machine translation ofcontext-free grammars in particular. Conditions that guarantee translationcompleteness of context-free grammars are presented.'}
 57.3 pattern=  1 task=  0 (u=10816)  {'docs': u'This paper describes the development and use of a lexical semantic databasefor the Verbmobil speech-to-speech machine translation system. The motivationis to provide a common information source for the distributed development ofthe semantics, transfer and semantic evaluation modules and to store lexicalsemantic information application-independently.  The database is organized around a set of abstract semantic classes and hasbeen used to define the semantic contributions of the lemmata in the vocabularyof the system, to automatically create semantic lexica and to check thecorrectness of the semantic representations built up. The semantic classes aremodelled using an inheritance hierarchy. The database is implemented using thelexicon formalism LeX4 developed during the project.'}
2.92e+03 pattern=  0 task=  0 (u=10881)  {'docs': u'This tutorial provides an overview of and introduction to Rissanen\'s MinimumDescription Length (MDL) Principle. The first chapter provides a conceptual,entirely non-technical introduction to the subject. It serves as a basis forthe technical introduction given in the second chapter, in which all the ideasof the first chapter are made mathematically precise. The main ideas arediscussed in great conceptual and technical detail. This tutorial is anextended version of the first two chapters of the collection "Advances inMinimum Description Length: Theory and Application" (edited by P.Grunwald, I.J.Myung and M. Pitt, to be published by the MIT Press, Spring 2005).'}
3.04e+03 pattern=  0 task=  1 (u=10881)  {'docs': u"We compare the elementary theories of Shannon information and Kolmogorovcomplexity, the extent to which they have a common purpose, and where they arefundamentally different. We discuss and relate the basic notions of boththeories: Shannon entropy versus Kolmogorov complexity, the relation of both touniversal coding, Shannon mutual information versus Kolmogorov (`algorithmic')mutual information, probabilistic sufficient statistic versus algorithmicsufficient statistic (related to lossy compression in the Shannon theory versusmeaningful information in the Kolmogorov theory), and rate distortion theoryversus Kolmogorov's structure function. Part of the material has appeared inprint before, scattered through various publications, but this is the firstcomprehensive systematic comparison. The last mentioned relations are new."}
3.17e+03 pattern=  0 task=  2 (u=10881)  {'docs': u'We analyze the Dawid-Rissanen prequential maximum likelihood codes relativeto one-parameter exponential family models M. If data are i.i.d. according toan (essentially) arbitrary P, then the redundancy grows at rate c/2 ln n. Weshow that c=v1/v2, where v1 is the variance of P, and v2 is the variance of thedistribution m* in M that is closest to P in KL divergence. This shows thatprequential codes behave quite differently from other important universal codessuch as the 2-part MDL, Shtarkov and Bayes codes, for which c=1. This behavioris undesirable in an MDL model selection setting.'}
2.96e+03 pattern=  0 task=  0 (u=10938)  {'docs': u'We study the problem of covering a given set of $n$ points in a high,$d$-dimensional space by the minimum enclosing polytope of a given arbitraryshape. We present algorithms that work for a large family of shapes, providedeither only translations and no rotations are allowed, or only rotation about afixed point is allowed; that is, one is allowed to only scale and translate agiven shape, or scale and rotate the shape around a fixed point. Our algorithmsstart with a polytope guessed to be of optimal size and iteratively moves itbased on a greedy principle: simply move the current polytope directly towardsany outside point till it touches the surface. For computing the minimumenclosing ball, this gives a simple greedy algorithm with running time$O(nd/\\eps)$ producing a ball of radius $1+\\eps$ times the optimal. This simpleprinciple generalizes to arbitrary convex shape when only translations areallowed, requiring at most $O(1/\\eps^2)$ iterations. Our algorithm implies that{\\em core-sets} of size $O(1/\\eps^2)$ exist not only for minimum enclosing ballbut also for any convex shape with a fixed orientation. A {\\em Core-Set} is asmall subset of $poly(1/\\eps)$ points whose minimum enclosing polytope isalmost as large as that of the original points. Although we are unable tocombine our techniques for translations and rotations for general shapes, forthe min-cylinder problem, we give an algorithm similar to the one in\\cite{HV03}, but with an improved running time of $2^{O(\\frac{1}{\\eps^2}\\log\\frac{1}{\\eps})} nd$.'}
2.95e+03 pattern=  0 task=  0 (u=10966)  {'docs': u"We present a protocol for verification of ``no such entry'' replies fromdatabases. We introduce a new cryptographic primitive as the underlyingstructure, the keyed hash tree, which is an extension of Merkle's hash tree. Wecompare our scheme to Buldas et al.'s Undeniable Attesters and Micali et al.'sZero Knowledge Sets."}
3.15e+03 pattern=  0 task=  0 (u=10974)  {'docs': u'For practical wireless DS-CDMA systems, channel estimation is imperfect dueto noise and interference. In this paper, the impact of channel estimationerrors on multiuser detection (MUD) is analyzed under the framework of thereplica method. System performance is obtained in the large system limit foroptimal MUD, linear MUD and turbo MUD, and is validated by numerical resultsfor finite systems.'}
3.15e+03 pattern=  0 task=  1 (u=10974)  {'docs': u'Communications in dispersive direct-sequence code-division multiple-access(DS-CDMA) channels suffer from intersymbol and multiple-access interference,which can significantly impair performance. Joint maximum \\textit{a posteriori}probability (MAP) equalization and multiuser detection with error controldecoding can be used to mitigate this interference and to achieve the optimalbit error rate. Unfortunately, such optimal detection typically requiresprohibitive computational complexity. This problem is addressed in this paperthrough the development of a reduced state trellis search detection algorithm,based on decision feedback from channel decoders. The performance of thisalgorithm is analyzed in the large-system limit. This analysis and simulationsshow that this low-complexity algorithm can obtain near-optimal performanceunder moderate signal-to-noise ratio and attains larger system load capacitythan parallel interference cancellation.'}
2.66e+03 pattern=  0 task=  0 (u=11474)  {'docs': u'We present cTI, the first system for universal left-termination inference oflogic programs. Termination inference generalizes termination analysis andchecking. Traditionally, a termination analyzer tries to prove that a givenclass of queries terminates. This class must be provided to the system, forinstance by means of user annotations. Moreover, the analysis must be redoneevery time the class of queries of interest is updated. Termination inference,in contrast, requires neither user annotations nor recomputation. In thisapproach, terminating classes for all predicates are inferred at once. Wedescribe the architecture of cTI and report an extensive experimentalevaluation of the system covering many classical examples from the logicprogramming termination literature and several Prolog programs of respectablesize and complexity.'}
2.38e+03 pattern=  0 task=  0 (u=11475)  {'docs': u'Since the seminal work of J. A. Robinson on resolution, many lifting lemmasfor simplifying proofs of completeness of resolution have been proposed in theliterature. In the logic programming framework, they may also help to detectsome infinite derivations while proving goals under the SLD-resolution. In thispaper, we first generalize a version of the lifting lemma, by extending therelation "is more general than" so that it takes into account only somearguments of the atoms. The other arguments, which we call neutral arguments,are disregarded. Then we propose two syntactic conditions of increasing powerfor identifying neutral arguments from mere inspection of the text of a logicprogram.'}
3.06e+03 pattern=  0 task=  0 (u=11569)  {'docs': u'We explore the possibility of applying the framework of frequent patternmining to a class of continuous objects appearing in nature, namely knots. Weintroduce the frequent knot mining problem and present a solution. The keyobservation is that a database consisting of knots can be transformed into atransactional database. This observation is based on the Prime DecompositionTheorem of knots.'}
3.1e+03 pattern=  0 task=  0 (u=11572)  {'docs': u"In this paper, we show a way to exploit sparsity in the problem data in aprimal-dual potential reduction method for solving a class of semidefiniteprograms. When the problem data is sparse, the dual variable is also sparse,but the primal one is not. To avoid working with the dense primal variable, weapply Fukuda et al.'s theory of partial matrix completion and work with partialmatrices instead. The other place in the algorithm where sparsity should beexploited is in the computation of the search direction, where the gradient andthe Hessian-matrix product of the primal and dual barrier functions must becomputed in every iteration. By using an idea from automatic differentiation inbackward mode, both the gradient and the Hessian-matrix product can be computedin time proportional to the time needed to compute the barrier functions ofsparse variables itself. Moreover, the high space complexity that is normallyassociated with the use of automatic differentiation in backward mode can beavoided in this case. In addition, we suggest a technique to efficientlycompute the determinant of the positive definite matrix completion that isrequired to compute primal search directions. The method of obtaining one ofthe primal search directions that minimizes the number of the evaluations ofthe determinant of the positive definite completion is also proposed. We thenimplement the algorithm and test it on the problem of finding the maximum cutof a graph."}
1.12e+03 pattern=  0 task=  0 (u=11790)  {'docs': u'The notion of quantum Turing machines is a basis of quantum complexitytheory. We discuss a general model of multi-tape, multi-head Quantum Turingmachines with multi final states that also allow tape heads to stay still.'}
2.63e+03 pattern=  0 task=  1 (u=11790)  {'docs': u'We shed new light on entanglement measures in multipartite quantum systems bytaking a computational-complexity approach toward quantifying quantumentanglement with two familiar notions--approximability and distinguishability.Built upon the formal treatment of partial separability, we measure thecomplexity of an entangled quantum state by determining (i) how hard toapproximate it from a fixed classical state and (ii) how hard to distinguish itfrom all partially separable states. We further consider theKolmogorovian-style descriptive complexity of approximation and distinction ofpartial entanglement.'}
2.64e+03 pattern=  0 task=  2 (u=11790)  {'docs': u'The complexity class NP is quintessential and ubiquitous in theoreticalcomputer science. Two different approaches have been made to define "QuantumNP," the quantum analogue of NP: NQP by Adleman, DeMarrais, and Huang, and QMAby Knill, Kitaev, and Watrous. From an operator point of view, NP can be viewedas the result of the exists-operator applied to P. Recently, Green, Homer,Moore, and Pollett proposed its quantum version, called the N-operator, whichis an abstraction of NQP. This paper introduces the exists^{Q}-operator, whichis an abstraction of QMA, and its complement, the forall^{Q}-operator. Theseoperators not only define Quantum NP but also build a quantum hierarchy,similar to the Meyer-Stockmeyer polynomial hierarchy, based on two-sidedbounded-error quantum computation.'}
3.05e+03 pattern=  0 task=  0 (u=11825)  {'docs': u'A Boolean function of n bits is balanced if it takes the value 1 withprobability 1/2. We exhibit a balanced Boolean function with a randomizedevaluation procedure (with probability 0 of making a mistake) so that onuniformly random inputs, no input bit is read with probability more thanTheta(n^{-1/2} sqrt{log n}). We give a balanced monotone Boolean function forwhich the corresponding probability is Theta(n^{-1/3} log n). We then show thatfor any randomized algorithm for evaluating a balanced Boolean function, whenthe input bits are uniformly random, there is some input bit that is read withprobability at least Theta(n^{-1/2}). For balanced monotone Boolean functions,there is some input bit that is read with probability at least Theta(n^{-1/3}).'}
  332 pattern=  0 task=  0 (u=11874)  {'docs': u'Although sloppy interpretation is usually accounted for by theories ofellipsis, it often arises in non-elliptical contexts. In this paper, a theoryof sloppy interpretation is provided which captures this fact. The underlyingidea is that sloppy interpretation results from a semantic constraint onparallel structures and the theory is shown to predict sloppy readings fordeaccented and paycheck sentences as well as relational-, event-, andone-anaphora. It is further shown to capture the interaction of sloppy/strictambiguity with quantification and binding.'}
3.07e+03 pattern=  0 task=  0 (u=11877)  {'docs': u'We introduce a notion of synchronization for higher-dimensional automata,based on coskeletons of cubical sets. Categorification transports this notionto the setting of categorical transition systems. We apply the results to studythe semantics of an imperative programming language with message-passing.'}
2.13e+03 pattern=  0 task=  0 (u=11978)  {'docs': u'In many real growing networks the mean number of connections per vertexincreases with time. The Internet, the Word Wide Web, collaboration networks,and many others display this behavior. Such a growth can be called {\\emaccelerated}. We show that this acceleration influences distribution ofconnections and may determine the structure of a network. We discuss generalconsequences of the acceleration and demonstrate its features applying simpleillustrating examples. In particular, we show that the accelerated growthfairly well explains the structure of the Word Web (the network of interactingwords of human language). Also, we use the models of the accelerated growth ofnetworks to describe a wealth condensation transition in evolving societies.'}
3.25e+03 pattern=  0 task=  1 (u=11978)  {'docs': u"The distribution of numbers in human documents is determined by a variety ofdiverse natural and human factors, whose relative significance can be evaluatedby studying the numbers' frequency of occurrence. Although it has been studiedsince the 1880's, this subject remains poorly understood. Here, we obtain thedetailed statistics of numbers in the World Wide Web, finding that theirdistribution is a heavy-tailed dependence which splits in a set of power-lawones. In particular, we find that the frequency of numbers associated towestern calendar years shows an uneven behavior: 2004 represents a `singularcritical' point, appearing with a strikingly high frequency; as we move awayfrom it, the decreasing frequency allows us to compare the amounts of existinginformation on the past and on the future. Moreover, while powers of ten occurextremely often, allowing us to obtain statistics up to the huge 10^127,`non-round' numbers occur in a much more limited range, the variations of theirfrequencies being dramatically different from standard statisticalfluctuations. These findings provide a view of the array of numbers used byhumans as a highly non-equilibrium and inhomogeneous system, and shed a newlight on an issue that, once fully investigated, could lead to a betterunderstanding of many sociological and psychological phenomena."}
3.26e+03 pattern=  0 task=  2 (u=11978)  {'docs': u'We find a new structural feature of equilibrium complex random networkswithout multiple and self-connections. We show that if the number ofconnections is sufficiently high, these networks contain a core of highlyinterconnected vertices. The number of vertices in this core varies in therange between $const N^{1/2}$ and $const N^{2/3}$, where $N$ is the number ofvertices in a network. At the birth point of the core, we obtain thesize-dependent cut-off of the distribution of the number of connections andfind that its position differs from earlier estimates.'}
3.2e+03 pattern=  0 task=  0 (u=12054)  {'docs': u'ADF95 is a tool to automatically calculate numerical first derivatives forany mathematical expression as a function of user defined independentvariables. Accuracy of derivatives is achieved within machine precision. ADF95may be applied to any FORTRAN 77/90/95 conforming code and requires minimalchanges by the user. It provides a new derived data type that holds the valueand derivatives and applies forward differencing by overloading all FORTRANoperators and intrinsic functions. An efficient indexing technique leads to areduced memory usage and a substantially increased performance gain over otheravailable tools with operator overloading. This gain is especially pronouncedfor sparse systems with large number of independent variables. A wide class ofnumerical simulations, e.g., those employing implicit solvers, can profit fromADF95.'}
2.89e+03 pattern=  0 task=  0 (u=12123)  {'docs': u'In this paper show that the list and bounded-distance decoding problems ofcertain bounds for the Reed-Solomon code are at least as hard as the discretelogarithm problem over finite fields.'}
  833 pattern=  0 task=  0 (u=12125)  {'docs': u'Consider a system F of n polynomial equations in n unknowns, over analgebraically closed field of arbitrary characteristic. We present a fastmethod to find a point in every irreducible component of the zero set Z of F.Our techniques allow us to sharpen and lower prior complexity bounds for thisproblem by fully taking into account the monomial term structure. As acorollary of our development we also obtain new explicit formulae for the exactnumber of isolated roots of F and the intersection multiplicity of thepositive-dimensional part of Z. Finally, we present a combinatorialconstruction of non-degenerate polynomial systems, with specified monomial termstructure and maximally many isolated roots, which may be of independentinterest.'}
3.16e+03 pattern=  0 task=  1 (u=12125)  {'docs': u"Suppose X is the complex zero set of a finite collection of polynomials inZ[x_1,...,x_n]. We show that deciding whether X contains a point all of whosecoordinates are d_th roots of unity can be done within NP^NP (relative to thesparse encoding), under a plausible assumption on primes in arithmeticprogression. In particular, our hypothesis can still hold even under certainfailures of the Generalized Riemann Hypothesis, such as the presence ofSiegel-Landau zeroes. Furthermore, we give a similar (but UNconditional)complexity upper bound for n=1. Finally, letting T be any algebraic subgroup of(C^*)^n we show that deciding whether X contains T is coNP-complete (relativeto an even more efficient encoding),unconditionally. We thus obtain newnon-trivial families of multivariate polynomial systems where deciding theexistence of complex roots can be done unconditionally in the polynomialhierarchy -- a family of complexity classes lying between PSPACE and P,intimately connected with the P=?NP Problem. We also discuss a connection toLaurent's solution of Chabauty's Conjecture from arithmetic geometry."}
1.76e+03 pattern=  0 task=  0 (u=12152)  {'docs': u"The clock synchronization problem is to determine the time difference Tbetween two spatially separated parties. We improve on I. Chuang's quantumclock synchronization algorithm and show that it is possible to obtain T to nbits of accuracy while communicating only one qubit in one direction and usingan O(2^n) frequency range. We also prove a quantum lower bound of \\Omega(2^n)for the product of the transmitted qubits and the range of frequencies, thusshowing that our algorithm is optimal."}
3.15e+03 pattern=  0 task=  0 (u=12169)  {'docs': u'Well-known principles of induction include monotone induction and differentsorts of non-monotone induction such as inflationary induction, induction overwell-founded sets and iterated induction. In this work, we define a logicformalizing induction over well-founded sets and monotone and iteratedinduction. Just as the principle of positive induction has been formalized inFO(LFP), and the principle of inflationary induction has been formalized inFO(IFP), this paper formalizes the principle of iterated induction in a newlogic for Non-Monotone Inductive Definitions (ID-logic). The semantics of thelogic is strongly influenced by the well-founded semantics of logicprogramming. Our main result concerns the modularity properties of inductivedefinitions in ID-logic. Specifically, we formulate conditions under which asimultaneous definition $\\D$ of several relations is logically equivalent to aconjunction of smaller definitions $\\D_1 \\land ... \\land \\D_n$ with disjointsets of defined predicates. The difficulty of the result comes from the factthat predicates $P_i$ and $P_j$ defined in $\\D_i$ and $\\D_j$, respectively, maybe mutually connected by simultaneous induction. Since logic programming andabductive logic programming under well-founded semantics are proper fragmentsof our logic, our modularity results are applicable there as well.'}
1.97e+03 pattern=  0 task=  0 (u=12303)  {'docs': u'The automated categorization (or classification) of texts into predefinedcategories has witnessed a booming interest in the last ten years, due to theincreased availability of documents in digital form and the ensuing need toorganize them. In the research community the dominant approach to this problemis based on machine learning techniques: a general inductive processautomatically builds a classifier by learning, from a set of preclassifieddocuments, the characteristics of the categories. The advantages of thisapproach over the knowledge engineering approach (consisting in the manualdefinition of a classifier by domain experts) are a very good effectiveness,considerable savings in terms of expert manpower, and straightforwardportability to different domains. This survey discusses the main approaches totext categorization that fall within the machine learning paradigm. We willdiscuss in detail issues pertaining to three different problems, namelydocument representation, classifier construction, and classifier evaluation.'}
2e+03 pattern=  0 task=  0 (u=12311)  {'docs': u'We introduce the smoothed analysis of algorithms, which is a hybrid of theworst-case and average-case analysis of algorithms. In smoothed analysis, wemeasure the maximum over inputs of the expected performance of an algorithmunder small random perturbations of that input. We measure this performance interms of both the input size and the magnitude of the perturbations. We showthat the simplex algorithm has polynomial smoothed complexity.'}
2.42e+03 pattern=  0 task=  1 (u=12311)  {'docs': u"We perform a smoothed analysis of the termination phase of an interior-pointmethod. By combining this analysis with the smoothed analysis of Renegar'sinterior-point algorithm by Dunagan, Spielman and Teng, we show that thesmoothed complexity of an interior-point algorithm for linear programming is $O(m^{3} \\log (m/\\sigma))$. In contrast, the best known bound on the worst-casecomplexity of linear programming is $O (m^{3} L)$, where $L$ could be as largeas $m$. We include an introduction to smoothed analysis and a tutorial on prooftechniques that have been useful in smoothed analyses."}
2.69e+03 pattern=  0 task=  2 (u=12311)  {'docs': u"We present a linear-system solver that, given an $n$-by-$n$ symmetricpositive semi-definite, diagonally dominant matrix $A$ with $m$ non-zeroentries and an $n$-vector $\\bb $, produces a vector $\\xxt$ within relativedistance $\\epsilon$ of the solution to $A \\xx = \\bb$ in time $O (m^{1.31} \\log(n \\kappa_{f} (A)/\\epsilon)^{O (1)})$, where $\\kappa_{f} (A)$ is the log of theratio of the largest to smallest non-zero eigenvalue of $A$. In particular,$\\log (\\kappa_{f} (A)) = O (b \\log n)$, where $b$ is the logarithm of the ratioof the largest to smallest non-zero entry of $A$. If the graph of $A$ has genus$m^{2\\theta}$ or does not have a $K_{m^{\\theta}} $ minor, then the exponent of$m$ can be improved to the minimum of $1 + 5 \\theta $ and $(9/8) (1+\\theta)$.The key contribution of our work is an extension of Vaidya's techniques forconstructing and analyzing combinatorial preconditioners."}
2.85e+03 pattern=  0 task=  0 (u=12356)  {'docs': u'We investigate a technique from the literature, called the phantom-typestechnique, that uses parametric polymorphism, type constraints, and unificationof polymorphic types to model a subtyping hierarchy. Hindley-Milner typesystems, such as the one found in Standard ML, can be used to enforce thesubtyping relation, at least for first-order values. We show that thistechnique can be used to encode any finite subtyping hierarchy (includinghierarchies arising from multiple interface inheritance). We formallydemonstrate the suitability of the phantom-types technique for capturingfirst-order subtyping by exhibiting a type-preserving translation from a simplecalculus with bounded polymorphism to a calculus embodying the type system ofSML.'}
2.85e+03 pattern=  0 task=  0 (u=12359)  {'docs': u'In evolutionary algorithms a critical parameter that must be tuned is that ofselection pressure. If it is set too low then the rate of convergence towardsthe optimum is likely to be slow. Alternatively if the selection pressure isset too high the system is likely to become stuck in a local optimum due to aloss of diversity in the population. The recent Fitness Uniform SelectionScheme (FUSS) is a conceptually simple but somewhat radical approach toaddressing this problem - rather than biasing the selection towards higherfitness, FUSS biases selection towards sparsely populated fitness levels. Inthis paper we compare the relative performance of FUSS with the well knowntournament selection scheme on a range of problems.'}
2.82e+03 pattern=  0 task=  0 (u=12456)  {'docs': u"An optimization problem that naturally arises in the study of swarm roboticsis the Freeze-Tag Problem (FTP) of how to awaken a set of ``asleep'' robots, byhaving an awakened robot move to their locations. Once a robot is awake, it canassist in awakening other slumbering robots.The objective is to have all robotsawake as early as possible. While the FTP bears some resemblance to problemsfrom areas in combinatorial optimization such as routing, broadcasting,scheduling, and covering, its algorithmic characteristics are surprisinglydifferent. We consider both scenarios on graphs and in geometricenvironments.In graphs, robots sleep at vertices and there is a length functionon the edges. Awake robots travel along edges, with time depending on edgelength. For most scenarios, we consider the offline version of the problem, inwhich each awake robot knows the position of all other robots. We prove thatthe problem is NP-hard, even for the special case of star graphs. We alsoestablish hardness of approximation, showing that it is NP-hard to obtain anapproximation factor better than 5/3, even for graphs of bounded degree.Theselower bounds are complemented with several positive algorithmic results,including: (1) We show that the natural greedy strategy on star graphs has atight worst-case performance of 7/3 and give a polynomial-time approximationscheme (PTAS) for star graphs. (2) We give a simple O(log D)-competitive onlinealgorithm for graphs with maximum degree D and locally bounded edge weights.(3) We give a PTAS, running in nearly linear time, for geometrically embeddedinstances."}
3.23e+03 pattern=  0 task=  0 (u=12465)  {'docs': u'A cycle cover of a graph is a set of cycles such that every vertex is part ofexactly one cycle. An L-cycle cover is a cycle cover in which the length ofevery cycle is in the set L. The weight of a cycle cover of an edge-weightedgraph is the sum of the weights of its edges.  We come close to settling the complexity and approximability of computingL-cycle covers. On the one hand, we show that for almost all L, computingL-cycle covers of maximum weight in directed and undirected graphs is APX-hardand NP-hard. Most of our hardness results hold even if the edge weights arerestricted to zero and one.  On the other hand, we show that the problem of computing L-cycle covers ofmaximum weight can be approximated within a factor of 2 for undirected graphsand within a factor of 8/3 in the case of directed graphs. This holds forarbitrary sets L.'}
2.26e+03 pattern=  0 task=  0 (u=12507)  {'docs': u'We demonstrate that the unbounded fan-out gate is very powerful.Constant-depth polynomial-size quantum circuits with bounded fan-in andunbounded fan-out over a fixed basis (denoted by QNCf^0) can approximate withpolynomially small error the following gates: parity, mod[q], And, Or,majority, threshold[t], exact[q], and Counting. Classically, we needlogarithmic depth even if we can use unbounded fan-in gates. If we allowarbitrary one-qubit gates instead of a fixed basis, then these circuits canalso be made exact in log-star depth. Sorting, arithmetical operations, phaseestimation, and the quantum Fourier transform with arbitrary moduli can also beapproximated in constant depth.'}
2.97e+03 pattern=  0 task=  0 (u=12815)  {'docs': u'We present two add-ons for Mathematica for teaching mathematics toundergraduate and high school students. These two applications, M@th Desktop(MD) and M@th Desktop Tools (MDTools), include several palettes and notebookscovering almost every field. The underlying didactic concept is so-called"blended learning", in which these tools are meant to be used as a complementto the professor or teacher rather than as a replacement, which othere-learning applications do. They enable students to avoid the usual problem ofcomputer-based learning, namely that too large an amount of time is wastedstruggling with computer and program errors instead of actually learning themathematical concepts.  M@th Desktop Tools is palette-based and provides easily accessible anduser-friendly templates for the most important functions in the fields ofAnalysis, Algebra, Linear Algebra and Statistics. M@th Desktop, in contrast, isa modern, interactive teaching and learning software package for mathematicsclasses. It is comprised of modules for Differentiation, Integration, andStatistics, and each module presents its topic with a combination ofinteractive notebooks and palettes.  Both packages can be obtained from Deltasoft\'s homepage athttp://www.deltasoft.at/ .'}
2.47e+03 pattern=  0 task=  0 (u=12903)  {'docs': u'This article aims to provide an introductory survey on quantum random walks.Starting from a physical effect to illustrate the main ideas we will introducequantum random walks, review some of their properties and outline theirstriking differences to classical walks. We will touch upon both physicaleffects and computer science applications, introducing some of the mainconcepts and language of present day quantum information science in thiscontext. We will mention recent developments in this new area and outline someopen questions.'}
2.94e+03 pattern=  0 task=  1 (u=12903)  {'docs': u'The k-local Hamiltonian problem is a natural complete problem for thecomplexity class QMA, the quantum analog of NP. It is similar in spirit toMAX-k-SAT, which is NP-complete for k<=2. It was known that the problem isQMA-complete for any k <= 3. On the other hand 1-local Hamiltonian is in P, andhence not believed to be QMA-complete. The complexity of the 2-localHamiltonian problem has long been outstanding. Here we settle the question andshow that it is QMA-complete. We provide two independent proofs; our firstproof uses only elementary linear algebra. Our second proof uses a powerfultechnique for analyzing the sum of two Hamiltonians; this technique is based onperturbation theory and we believe that it might prove useful elsewhere. Usingour techniques we also show that adiabatic computation with two-localinteractions on qubits is equivalent to standard quantum computation.'}
3.02e+03 pattern=  0 task=  0 (u=12952)  {'docs': u'CrocoPat is an efficient, powerful and easy-to-use tool for manipulatingrelations of arbitrary arity, including directed graphs. This manual providesan introduction to and a reference for CrocoPat and its programming languageRML. It includes several application examples, in particular from the analysisof structural models of software systems.'}
2.06e+03 pattern=  0 task=  0 (u=13028)  {'docs': u'We consider the question of whether collusion among bidders (a "biddingring") can be supported in equilibrium of unrepeated first-price auctions.Unlike previous work on the topic such as that by McAfee and McMillan [1992]and Marshall and Marx [2007], we do not assume that non-colluding agents haveperfect knowledge about the number of colluding agents whose bids aresuppressed by the bidding ring, and indeed even allow for the existence ofmultiple cartels. Furthermore, while we treat the association of bidders withbidding rings as exogenous, we allow bidders to make strategic decisions aboutwhether to join bidding rings when invited. We identify a bidding ring protocolthat results in an efficient allocation in Bayes{Nash equilibrium, under whichnon-colluding agents bid straightforwardly, and colluding agents join biddingrings when invited and truthfully declare their valuations to the ring center.We show that bidding rings benefit ring centers and all agents, both membersand non-members of bidding rings, at the auctioneer\'s expense. The techniqueswe introduce in this paper may also be useful for reasoning about otherproblems in which agents have asymmetric information about a setting.'}
2.89e+03 pattern=  0 task=  0 (u=13070)  {'docs': u'We propose a cipher similar to the One Time Pad and McEliece cipher based ona subband coding scheme. The encoding process is an approximation to the OneTime Pad encryption scheme. We present results of numerical experiments whichsuggest that a brute force attack to the proposed scheme does not result in allpossible plaintexts, as the One Time Pad does, but still the brute force attackdoes not compromise the system. However, we demonstrate that the cipher isvulnerable to a chosen-plaintext attack.'}
2.89e+03 pattern=  0 task=  0 (u=13083)  {'docs': u'In this paper, a new framework for one-dimensional contour extraction fromdiscrete two-dimensional data sets is presented. Contour extraction isimportant in many scientific fields such as digital image processing, computervision, pattern recognition, etc. This novel framework includes (but is notlimited to) algorithms for dilated contour extraction, contour displacement,shape skeleton extraction, contour continuation, shape feature based contourrefinement and contour simplification. Many of the new techniques dependstrongly on the application of a Delaunay tessellation. In order to demonstratethe versatility of this novel toolbox approach, the contour extractiontechniques presented here are applied to scientific problems in materialscience, biology and heavy ion physics.'}
1.77e+03 pattern=  0 task=  0 (u=13393)  {'docs': u'The design and implementation of two software systems introduced to improvethe efficiency of offline analysis of event data taken with the ZEUS Detectorat the HERA electron-proton collider at DESY are presented. Two differentapproaches were made, one using a set of event directories and the other usinga tag database based on a commercial object-oriented database managementsystem. These are described and compared. Both systems provide quick directaccess to individual collision events in a sequential data store of severalterabytes, and they both considerably improve the event analysis efficiency. Inparticular the tag database provides a very flexible selection mechanism andcan dramatically reduce the computing time needed to extract small subsamplesfrom the total event sample. Gains as large as a factor 20 have been obtained.'}
1.76e+03 pattern=  0 task=  0 (u=13407)  {'docs': u"The Globus Data Grid architecture provides a scalable infrastructure for themanagement of storage resources and data that are distributed across Gridenvironments. These services are designed to support a variety of scientificapplications, ranging from high-energy physics to computational genomics, thatrequire access to large amounts of data (terabytes or even petabytes) withvaried quality of service requirements. By layering on a set of core services,such as data transport, security, and replica cataloging, one can constructvarious higher-level services. In this paper, we discuss the design andimplementation of a high-level replica selection service that uses informationregarding replica location and user preferences to guide selection from amongstorage replica alternatives. We first present a basic replica selectionservice design, then show how dynamic information collected using Globusinformation service capabilities concerning storage system properties can helpimprove and optimize the selection process. We demonstrate the use of Condor'sClassAds resource description and matchmaking mechanism as an efficient toolfor representing and matching storage resource capabilities and policiesagainst application requirements."}
1.32e+03 pattern=  0 task=  0 (u=13489)  {'docs': u"We describe a framework and equations used to model and predict the behaviorof multi-agent systems (MASs) with learning agents. A difference equation isused for calculating the progression of an agent's error in its decisionfunction, thereby telling us how the agent is expected to fare in the MAS. Theequation relies on parameters which capture the agent's learning abilities,such as its change rate, learning rate and retention rate, as well as relevantaspects of the MAS such as the impact that agents have on each other. Wevalidate the framework with experimental results using reinforcement learningagents in a market system, as well as with other experimental results gatheredfrom the AI literature. Finally, we use PAC-theory to show how to calculatebounds on the values of the learning parameters."}
2.63e+03 pattern=  0 task=  1 (u=13489)  {'docs': u'We introduce the topic of learning in multiagent systems. We first provide aquick introduction to the field of game theory, focusing on the equilibriumconcepts of iterated dominance, and Nash equilibrium. We show some of the mostrelevant findings in the theory of learning in games, including theorems onfictitious play, replicator dynamics, and evolutionary stable strategies. TheCLRI theory and n-level learning agents are introduced as attempts to applysome of these findings to the problem of engineering multiagent systems withlearning agents. Finally, we summarize some of the remaining challenges in thefield of learning in multiagent systems.'}
2.94e+03 pattern=  0 task=  0 (u=13535)  {'docs': u"The term `hypermachine' denotes any data processing device (theoretical orthat can be implemented) capable of carrying out tasks that cannot be performedby a Turing machine. We present a possible quantum algorithm for a classicallynon-computable decision problem, Hilbert's tenth problem; more specifically, wepresent a possible hypercomputation model based on quantum computation. Ouralgorithm is inspired by the one proposed by Tien D. Kieu, but we have selectedthe infinite square well instead of the (one-dimensional) simple harmonicoscillator as the underlying physical system. Our model exploits the quantumadiabatic process and the characteristics of the representation of thedynamical Lie algebra su(1,1) associated to the infinite square well."}
3.23e+03 pattern=  0 task=  1 (u=13535)  {'docs': u"The hypercomputers compute functions or numbers, or more generally solveproblems or carry out tasks, that cannot be computed or solved by a Turingmachine. Several numerical simulations of a possible hypercomputationalalgorithm based on quantum computations previously constructed by the authorsare presented. The hypercomputability of our algorithm is based on the factthat this algorithm could solve a classically non-computable decision problem,Hilbert's tenth problem. The numerical simulations were realized for threetypes of Diophantine equations: with and without solutions in non-negativeintegers, and without solutions by way of various traditional mathematicalpackages."}
2.43e+03 pattern=  0 task=  0 (u=13549)  {'docs': u'A method is presented that reduces the number of terms of systems of linearequations (algebraic, ordinary and partial differential equations). As abyproduct these systems have a tendency to become partially decoupled and aremore likely to be factorizable or integrable. A variation of this method isapplicable to non-linear systems. Modifications to improve efficiency are givenand examples are shown. This procedure can be used in connection with thecomputation of the radical of a differential ideal (differential Groebnerbasis).'}
2.43e+03 pattern=  0 task=  1 (u=13549)  {'docs': u'A new integration technique is presented for systems of linear partialdifferential equations (PDEs) for which syzygies can be formulated that obeyconservation laws. These syzygies come for free as a by-product of thedifferential Groebner Basis computation. Compared with the more obvious way ofintegrating a single equation and substituting the result in other equationsthe new technique integrates more than one equation at once and thereforeintroduces temporarily fewer new functions of integration that in additiondepend on fewer variables. Especially for high order PDE systems in manyvariables the conventional integration technique may lead to an explosion ofthe number of functions of integration which is avoided with the new method. Afurther benefit is that redundant free functions in the solution are eitherprevented or that their number is at least reduced.'}
2.8e+03 pattern=  0 task=  0 (u=13588)  {'docs': u'Based on measurements of the Internet topology data, we found out that thereare two mechanisms which are necessary for the correct modeling of the Internettopology at the Autonomous Systems (AS) level: the Interactive Growth of newnodes and new internal links, and a nonlinear preferential attachment, wherethe preference probability is described by a positive-feedback mechanism. Basedon the above mechanisms, we introduce the Positive-Feedback Preference (PFP)model which accurately reproduces many topological properties of the AS-levelInternet, including: degree distribution, rich-club connectivity, the maximumdegree, shortest path length, short cycles, disassortative mixing andbetweenness centrality. The PFP model is a phenomenological model whichprovides a novel insight into the evolutionary dynamics of real complexnetworks.'}
2.96e+03 pattern=  0 task=  0 (u=13645)  {'docs': u'The classical propositional logic is known to be sound and complete withrespect to the set semantics that interprets connectives as set operations. Thepaper extends propositional language by a new binary modality that correspondsto partial recursive function type constructor under the above interpretation.The cases of deterministic and non-deterministic functions are considered andfor both of them semantically complete modal logics are described anddecidability of these logics is established.'}
1.93e+03 pattern=  0 task=  0 (u=13844)  {'docs': u'It is shown that a POPS network with g groups and d processors per group canefficiently route any permutation among the n=dg processors. The number ofslots used is optimal in the worst case, and is at most the double of theoptimum for all permutations p such that p(i)<>i for all i.'}
2.43e+03 pattern=  0 task=  0 (u=13917)  {'docs': u'We provide a semantic framework for preference handling in answer setprogramming. To this end, we introduce preference preserving consequenceoperators. The resulting fixpoint characterizations provide us with a uniformsemantic framework for characterizing preference handling in existingapproaches. Although our approach is extensible to other semantics by means ofan alternating fixpoint theory, we focus here on the elaboration of preferencesunder answer set semantics. Alternatively, we show how these approaches can becharacterized by the concept of order preservation. These uniform semanticcharacterizations provide us with new insights about interrelationships andmoreover about ways of implementation.'}
3.15e+03 pattern=  0 task=  0 (u=13924)  {'docs': u'We resolve the question of whether Fourier sampling can efficiently solve thehidden subgroup problem. Specifically, we show that the hidden subgroup problemover the symmetric group cannot be efficiently solved by strong Fouriersampling, even if one may perform an arbitrary POVM on the coset state. Ourresults apply to the special case relevant to the Graph Isomorphism problem.'}
3.05e+03 pattern=  0 task=  0 (u=13951)  {'docs': u"Quantum finite automata have been studied intensively since theirintroduction in late 1990s as a natural model of a quantum computer withfinite-dimensional quantum memory space. This paper seeks their directapplication to interactive proof systems in which a mighty quantum provercommunicates with a quantum-automaton verifier through a common communicationcell. Our quantum interactive proof systems are juxtaposed toDwork-Stockmeyer's classical interactive proof systems whose verifiers aretwo-way probabilistic automata. We demonstrate strengths and weaknesses of oursystems and further study how various restrictions on the behaviors ofquantum-automaton verifiers affect the power of quantum interactive proofsystems."}
3.18e+03 pattern=  0 task=  0 (u=13983)  {'docs': u'The achievable information rate of finite-state input two-dimensional (2-D)channels with memory is an open problem, which is relevant, e.g., forinter-symbol-interference (ISI) channels and cellular multiple-access channels.We propose a method for simulation-based computation of such information rates.We first draw a connection between the Shannon-theoretic information rate andthe statistical mechanics notion of free energy. Since the free energy of suchsystems is intractable, we approximate it using the cluster variation method,implemented via generalized belief propagation. The derived, fully tractable,algorithm is shown to provide a practically accurate estimate of theinformation rate. In our experimental study we calculate the information ratesof 2-D ISI channels and of hexagonal Wyner cellular networks with binaryinputs, for which formerly only bounds were known.'}
3.08e+03 pattern=  0 task=  0 (u=14194)  {'docs': u"The feedback capacity of the stationary Gaussian additive noise channel hasbeen open, except for the case where the noise is white. Here we find thefeedback capacity of the stationary first-order moving average additiveGaussian noise channel in closed form. Specifically, the channel is given by$Y_i = X_i + Z_i,$ $i = 1, 2, ...,$ where the input $\\{X_i\\}$ satisfies a powerconstraint and the noise $\\{Z_i\\}$ is a first-order moving average Gaussianprocess defined by $Z_i = \\alpha U_{i-1} + U_i,$ $|\\alpha| \\le 1,$ with whiteGaussian innovations $U_i,$ $i = 0,1,....$  We show that the feedback capacity of this channel is $-\\log x_0,$ where$x_0$ is the unique positive root of the equation $ \\rho x^2 = (1-x^2) (1 -|\\alpha|x)^2,$ and $\\rho$ is the ratio of the average input power pertransmission to the variance of the noise innovation $U_i$. The optimal codingscheme parallels the simple linear signalling scheme by Schalkwijk and Kailathfor the additive white Gaussian noise channel -- the transmitter sends areal-valued information-bearing signal at the beginning of communication andsubsequently refines the receiver's error by processing the feedback noisesignal through a linear stationary first-order autoregressive filter. Theresulting error probability of the maximum likelihood decoding decaysdoubly-exponentially in the duration of the communication. This feedbackcapacity of the first-order moving average Gaussian channel is very similar inform to the best known achievable rate for the first-order\\emph{autoregressive} Gaussian noise channel studied by Butman, Wolfowitz, andTiernan, although the optimality of the latter is yet to be established."}
2.36e+03 pattern=  0 task=  0 (u=14276)  {'docs': u"Internet traffic on a network link can be modeled as a stochastic process.After detecting and quantifying the properties of this process, usingstatistical tools, a series of mathematical models is developed, culminating inone that is able to generate ``traffic'' that exhibits --as a key feature-- thesame difference in behavior for different time scales, as observed in realtraffic, and is moreover indistinguishable from real traffic by otherstatistical tests as well. Tools inspired from the models are then used todetermine and calibrate the type of activity taking place in each of the timescales. Surprisingly, the above procedure does not require any detailedinformation originating from either the network dynamics, or the decompositionof the total traffic into its constituent user connections, but rather only thecompliance of these connections to very weak conditions."}
3.24e+03 pattern=  0 task=  0 (u=14335)  {'docs': u'An approach to compatibility analysis of systems of discrete relations isproposed. Unlike the Groebner basis technique, the proposed scheme is not basedon the polynomial ring structure. It uses more primitive set-theoretic andtopological concepts and constructions. We illustrate the approach byapplication to some two-state cellular automata. In the two-state case theGroebner basis method is also applicable, and we compare both approaches.'}
2.95e+03 pattern=  0 task=  0 (u=14398)  {'docs': u'We report on a recently initiated project which aims at building amulti-layered parallel treebank of English and German. Particular attention isdevoted to a dedicated predicate-argument layer which is used for aligningtranslationally equivalent sentences of the two languages. We describe both ourconceptual decisions and aspects of their technical realisation. We discusssome selected problems and conclude with a few remarks on how this projectrelates to similar projects in the field.'}
3.02e+03 pattern=  0 task=  1 (u=14398)  {'docs': u'While alignment of texts on the sentential level is often seen as being toocoarse, and word alignment as being too fine-grained, bi- or multilingual textswhich are aligned on a level in-between are a useful resource for manypurposes. Starting from a number of examples of non-literal translations, whichtend to make alignment difficult, we describe an alignment model which copeswith these cases by explicitly coding them. The model is based onpredicate-argument structures and thus covers the middle ground betweensentence and word alignment. The model is currently used in a recentlyinitiated project of a parallel English-German treebank (FuSe), which can inprinciple be extended with additional languages.'}
1.42e+03 pattern=  0 task=  0 (u=14408)  {'docs': u'The traditional split-up into a low level language and a high level languagein the design of computer algebra systems may become obsolete with the adventof more versatile computer languages. We describe GiNaC, a special-purposesystem that deliberately denies the need for such a distinction. It is entirelywritten in C++ and the user can interact with it directly in that language. Itwas designed to provide efficient handling of multivariate polynomials,algebras and special functions that are needed for loop calculations intheoretical quantum field theory. It also bears some potential to become a moregeneral purpose symbolic package.'}
2.43e+03 pattern=  0 task=  0 (u=14426)  {'docs': u'This paper considers an example of Object-Oriented Programming (OOP) leadingto subtle errors that break separation of interface and implementations. Acomprehensive principle that guards against such errors is undecidable. Thepaper introduces a set of mechanically verifiable rules that prevent theseinsidious problems. Although the rules seem restrictive, they are powerful andexpressive, as we show on several familiar examples. The rules contradict boththe spirit and the letter of the OOP. The present examples as well as availabletheoretical and experimental results pose a question if OOP is conducive tosoftware development at all.'}
2.56e+03 pattern=  0 task=  0 (u=14444)  {'docs': u'The system PL permits the translation of abstract proofs of programcorrectness into programs in a variety of programming languages. A programminglanguage satisfying certain axioms may be the target of such a translation. Thesystem PL also permits the construction and proof of correctness of programs inan abstract programming language, and permits the translation of these programsinto correct programs in a variety of languages. The abstract programminglanguage has an imperative style of programming with assignment statements andside-effects, to allow the efficient generation of code. The abstract programsmay be written by humans and then translated, avoiding the need to write thesame program repeatedly in different languages or even the same language. Thissystem uses classical logic, is conceptually simple, and permits reasoningabout nonterminating programs using Scott-Strachey style denotationalsemantics.'}
2.67e+03 pattern=  0 task=  1 (u=14444)  {'docs': u'A situation calculus is presented that provides a solution to the frameproblem for hierarchical situations, that is, situations that have a modularstructure in which parts of the situation behave in a relatively independentmanner. This situation calculus is given in a relational, functional, and modallogic form. Each form permits both a single level hierarchy or a multiple levelhierarchy, giving six versions of the formalism in all, and a number ofsub-versions of these. For multiple level hierarchies, it is possible to giveequations between parts of the situation to impose additional structure on theproblem. This approach is compared to others in the literature.'}
1.61e+03 pattern=  0 task=  0 (u=14473)  {'docs': u"In fuzzy propositional logic, to a proposition a partial truth in [0,1] isassigned. It is well known that under certain circumstances, fuzzy logiccollapses to classical logic. In this paper, we will show that under dualconditions, fuzzy logic collapses to four-valued (relevance) logic, wherepropositions have truth-value true, false, unknown, or contradiction. As aconsequence, fuzzy entailment may be considered as ``in between'' four-valued(relevance) entailment and classical entailment."}
2.7e+03 pattern=  0 task=  0 (u=14610)  {'docs': u'Abduction, first proposed in the setting of classical logics, has beenstudied with growing interest in the logic programming area during the lastyears.  In this paper we study abduction with penalization in the logic programmingframework. This form of abductive reasoning, which has not been previouslyanalyzed in logic programming, turns out to represent several relevantproblems, including optimization problems, very naturally. We define a formalmodel for abduction with penalization over logic programs, which extends theabductive framework proposed by Kakas and Mancarella. We address knowledgerepresentation issues, encoding a number of problems in our abductiveframework. In particular, we consider some relevant problems, taken fromdifferent domains, ranging from optimization theory to diagnosis and planning;their encodings turn out to be simple and elegant in our formalism. Wethoroughly analyze the computational complexity of the main problems arising inthe context of abduction with penalization from logic programs. Finally, weimplement a system supporting the proposed abductive framework on top of theDLV engine. To this end, we design a translation from abduction problems withpenalties into logic programs with weak constraints. We prove that thisapproach is sound and complete.'}
3.08e+03 pattern=  0 task=  0 (u=14802)  {'docs': u'In this article, we study directed graphs (digraphs) with a coloringconstraint due to Von Neumann and related to Nim-type games. This is equivalentto the notion of kernels of digraphs, which appears in numerous fields ofresearch such as game theory, complexity theory, artificial intelligence(default logic, argumentation in multi-agent systems), 0-1 laws in monadicsecond order logic, combinatorics (perfect graphs)... Kernels of digraphs leadto numerous difficult questions (in the sense of NP-completeness,#P-completeness). However, we show here that it is possible to use a generatingfunction approach to get new informations: we use technique of symbolic andanalytic combinatorics (generating functions and their singularities) in orderto get exact and asymptotic results, e.g. for the existence of a kernel in acircuit or in a unicircuit digraph. This is a first step toward ageneratingfunctionology treatment of kernels, while using, e.g., an approach "ala Wright". Our method could be applied to more general "local coloringconstraints" in decomposable combinatorial structures.'}
3.1e+03 pattern=  0 task=  0 (u=14804)  {'docs': u"Given a set $\\xi=\\{H_1,H_2,...\\}$ of connected non acyclic graphs, a$\\xi$-free graph is one which does not contain any member of $% \\xi$ as copy.Define the excess of a graph as the difference between its number of edges andits number of vertices. Let ${\\gr{W}}_{k,\\xi}$ be theexponential generatingfunction (EGF for brief) of connected $\\xi$-free graphs of excess equal to $k$($k \\geq 1$). For each fixed $\\xi$, a fundamental differential recurrencesatisfied by the EGFs ${\\gr{W}}_{k,\\xi}$ is derived. We give methods on how tosolve this nonlinear recurrence for the first few values of $k$ by means ofgraph surgery. We also show that for any finite collection $\\xi$ of non-acyclicgraphs, the EGFs ${\\gr{W}}_{k,\\xi}$ are always rational functions of thegenerating function, $T$, of Cayley's rooted (non-planar) labelled trees. Fromthis, we prove that almost all connected graphs with $n$ nodes and $n+k$ edgesare $\\xi$-free, whenever $k=o(n^{1/3})$ and $|\\xi| < \\infty$ by means ofWright's inequalities and saddle point method. Limiting distributions arederived for sparse connected $\\xi$-free components that are present when arandom graph on $n$ nodes has approximately $\\frac{n}{2}$ edges. In particular,the probability distribution that it consists of trees, unicyclic components,$...$, $(q+1)$-cyclic components all $\\xi$-free is derived. Similar results arealso obtained for multigraphs, which are graphs where self-loops andmultiple-edges are allowed."}
2.44e+03 pattern=  0 task=  0 (u=14819)  {'docs': u'Regular expression patterns are a key feature of document processinglanguages like Perl and XDuce. It is in this context that the first and longestmatch policies have been proposed to disambiguate the pattern matching process.We formally define a matching semantics with these policies and show that thegenerally accepted method of simulating longest match by first match andrecursion is incorrect. We continue by solving the associated type inferenceproblem, which consists in calculating for every subexpression the set of wordsthe subexpression can still match when these policies are in effect, and showhow this algorithm can be used to efficiently implement the matching process.'}
2.7e+03 pattern=  0 task=  0 (u=14825)  {'docs': u'We give a gentle introduction to using various software tools for automaticdifferentiation (AD). Ready-to-use examples are discussed, and links to furtherinformation are presented. Our target audience includes all those who arelooking for a straightforward way to get started using the available ADtechnology. The document is dynamic in the sense that its content will beupdated as the AD software evolves.'}
2.87e+03 pattern=  0 task=  0 (u=14844)  {'docs': u'The logic programming paradigm provides the basis for a new intensional viewof higher-order notions. This view is realized primarily by employing the termsof a typed lambda calculus as representational devices and by using a richerform of unification for probing their structures. These additions haveimportant meta-programming applications but they also pose non-trivialimplementation problems. One issue concerns the machine representation oflambda terms suitable to their intended use: an adequate encoding mustfacilitate comparison operations over terms in addition to supporting the usualreduction computation. Another aspect relates to the treatment of a unificationoperation that has a branching character and that sometimes calls for thedelaying of the solution of unification problems. A final issue concerns theexecution of goals whose structures become apparent only in the course ofcomputation. These various problems are exposed in this paper and solutions tothem are described. A satisfactory representation for lambda terms is developedby exploiting the nameless notation of de Bruijn as well as explicit encodingsof substitutions. Special mechanisms are molded into the structure oftraditional Prolog implementations to support branching in unification andcarrying of unification problems over other computation steps; a premium isplaced in this context on exploiting determinism and on emulating usualfirst-order behaviour. An extended compilation model is presented that treatshigher-order unification and also handles dynamically emergent goals. The ideasdescribed here have been employed in the Teyjus implementation of the LambdaProlog language, a fact that is used to obtain a preliminary assessment oftheir efficacy.'}
2.09e+03 pattern=  0 task=  0 (u=15244)  {'docs': u'Many problems in engineering, chemistry and physics require therepresentation of solutions in complex geometries. In the paper we deal with aproblem of unstructured mesh generation for the control volume method. Wepropose an algorithm which bases on the spheres generation in central points ofthe control volumes.'}
2.09e+03 pattern=  0 task=  0 (u=15244)  {'docs': u'In this paper we propose an algorithm for the numerical solution of arbitrarydifferential equations of fractional order. The algorithm is obtained by usingthe following decomposition of the differential equation into a system ofdifferential equation of integer order connected with inverse forms ofAbel-integral equations. The algorithm is used for solution of the linear andnon-linear equations.'}
2.09e+03 pattern=  0 task=  0 (u=15246)  {'docs': u'A Boolean constraint satisfaction instance is a conjunction of constraintapplications, where the allowed constraints are drawn from a fixed set B ofBoolean functions. We consider the problem of determining whether two givenconstraint satisfaction instances are equivalent and prove a Dichotomy Theoremby showing that for all sets C of allowed constraints, this problem is eitherpolynomial-time solvable or coNP-complete, and we give a simple criterion todetermine which case holds.  A more general problem addressed in this paper is the isomorphism problem,the problem of determining whether there exists a renaming of the variablesthat makes two given constraint satisfaction instances equivalent in the abovesense. We prove that this problem is coNP-hard if the corresponding equivalenceproblem is coNP-hard, and polynomial-time many-one reducible to the graphisomorphism problem in all other cases.'}
3.29e+03 pattern=  0 task=  0 (u=15339)  {'docs': u'In this paper, we propose a new language, called AR ({\\it Action Rules}), anddescribe how various propagators for finite-domain constraints can beimplemented in it. An action rule specifies a pattern for agents, an actionthat the agents can carry out, and an event pattern for events that canactivate the agents. AR combines the goal-oriented execution model of logicprogramming with the event-driven execution model. This hybrid execution modelfacilitates programming constraint propagators. A propagator for a constraintis an agent that maintains the consistency of the constraint and is activatedby the updates of the domain variables in the constraint. AR has a muchstronger descriptive power than {\\it indexicals}, the language widely used inthe current finite-domain constraint systems, and is flexible for implementingnot only interval-consistency but also arc-consistency algorithms. As examples,we present a weak arc-consistency propagator for the {\\tt all\\_distinct}constraint and a hybrid algorithm for n-ary linear equality constraints.B-Prolog has been extended to accommodate action rules. Benchmarking shows thatB-Prolog as a CLP(FD) system significantly outperforms other CLP(FD) systems.'}
3.06e+03 pattern=  0 task=  0 (u=15340)  {'docs': u"In the Boehm theorem workshop on Crete island, Zoran Petric called Statman's``Typical Ambiguity theorem'' typed Boehm theorem. Moreover, he gave a newproof of the theorem based on set-theoretical models of the simply typed lambdacalculus. In this paper, we study the linear version of the typed Boehm theoremon a fragment of Intuitionistic Linear Logic. We show that in themultiplicative fragment of intuitionistic linear logic without themultiplicative unit 1 (for short IMLL) weak typed Boehm theorem holds. Thesystem IMLL exactly corresponds to the linear lambda calculus withoutexponentials, additives and logical constants. The system IMLL also exactlycorresponds to the free symmetric monoidal closed category without the unitobject. As far as we know, our separation result is the first one with regardto these systems in a purely syntactical manner."}
3.06e+03 pattern=  0 task=  1 (u=15340)  {'docs': u'In this paper, we introduce Linear Logic with a nondeterministic facility,which has a self-dual additive connective. In the system the proof nettechnology is available in a natural way. The important point is thatnondeterminism in the system is expressed by the process of normalization, notby proof search. Moreover we can incorporate the system into Light Linear Logicand Elementary Linear Logic developed by J.-Y.Girard recently: NondeterministicLight Linear Logic and Nondeterministic Elementary Linear Logic are defined ina very natural way.'}
3.06e+03 pattern=  0 task=  2 (u=15340)  {'docs': u'In CSL\'99 Roversi pointed out that the Turing machine encoding of Girard\'sseminal paper "Light Linear Logic" has a flaw. Moreover he presented a workingversion of the encoding in Light Affine Logic, but not in Light Linear Logic.In this paper we present a working version of the encoding in Light LinearLogic. The idea of the encoding is based on a remark of Girard\'s tutorial paperon Linear Logic. The encoding is also an example which shows usefulness ofadditive connectives. Moreover we also consider a nondeterministic extension ofLight Linear Logic. We show that the extended system is NP-complete in the samemeaning as P-completeness of Light Linear Logic.'}
3.04e+03 pattern=  0 task=  0 (u=15544)  {'docs': u'We present a near linear time algorithm for constructing hierarchical nets infinite metric spaces with constant doubling dimension. This data-structure isthen applied to obtain improved algorithms for the following problems:Approximate nearest neighbor search, well-separated pair decomposition, compactrepresentation scheme, doubling measure, and computation of the (approximate)Lipschitz constant of a function. In all cases, the running (preprocessing)time is near-linear and the space being used is linear.'}
2.9e+03 pattern=  0 task=  0 (u=15752)  {'docs': u'This paper focuses on an interesting phenomenon when chaos meets computers.It is found that digital computers are absolutely incapable of showing truelong-time dynamics of some chaotic systems, including the tent map, theBernoulli shift map and their analogues, even in a high-precisionfloating-point arithmetic. Although the results cannot directly generalized tomost chaotic systems, the risk of using digital computers to numerically studycontinuous dynamical systems is shown clearly. As a result, we reach the oldsaying that "it is impossible to do everything with computers only".'}
2.97e+03 pattern=  0 task=  1 (u=15752)  {'docs': u'This paper studies the security of a secure communication scheme based on twodiscrete-time intermittently-chaotic systems synchronized via a common randomdriving signal. Some security defects of the scheme are revealed: 1) the keyspace can be remarkably reduced; 2) the decryption is insensitive to themismatch of the secret key; 3) the key-generation process is insecure againstknown/chosen-plaintext attacks. The first two defects mean that the scheme isnot secure enough against brute-force attacks, and the third one means that anattacker can easily break the cryptosystem by approximately estimating thesecret key once he has a chance to access a fragment of the generatedkeystream. Yet it remains to be clarified if intermittent chaos could be usedfor designing secure chaotic cryptosystems.'}
3.14e+03 pattern=  0 task=  2 (u=15752)  {'docs': u'As a powerful cryptanalysis tool, the method of return-map attacks can beused to extract secret messages masked by chaos in secure communicationschemes. Recently, a simple defensive mechanism was presented to enhance thesecurity of chaotic parameter modulation schemes against return-map attacks.Two techniques are combined in the proposed defensive mechanism: multistepparameter modulation and alternative driving of two different transmittervariables. This paper re-studies the security of this proposed defensivemechanism against return-map attacks, and points out that the security was muchover-estimated in the original publication for both ciphertext-only attack andknown/chosen-plaintext attacks. It is found that a deterministic relationshipexists between the shape of the return map and the modulated parameter, andthat such a relationship can be used to dramatically enhance return-map attacksthereby making them quite easy to break the defensive mechanism.'}
1.84e+03 pattern=  0 task=  0 (u=16069)  {'docs': u'This paper deals with an extended model of computations which uses theparameterized families of entities for data objects and reflects a preliminaryoutline of this problem. Some topics are selected out, briefly analyzed andarranged to cover a general problem. The authors intended more to discuss theparticular topics, their interconnection and computational meaning as a panelproposal, so that this paper is not yet to be evaluated as a closed journalpaper. To save space all the technical and implementation features are left forthe future paper.  Data object is a schematic entity and modelled by the partial function. Anotion of type is extended by the variable domains which depend on events andtypes. A variable domain is built from the potential and schematic individualsand generates the valid families of types depending on a sequence of events.Each valid type consists of the actual individuals which are actual relativelythe event or script. In case when a type depends on the script thencorresponding view for data objects is attached, otherwise a snapshot isgenerated. The type thus determined gives an upper range for typed variables sothat the local ranges are event driven resulting is the families of actualindividuals. An expressive power of the query language is extended using theextensional and intentional relations.'}
1.49e+03 pattern=  0 task=  0 (u=16245)  {'docs': u"Many problems in robust control and motion planning can be reduced to eitherfind a sound approximation of the solution space determined by a set ofnonlinear inequalities, or to the ``guaranteed tuning problem'' as defined byJaulin and Walter, which amounts to finding a value for some tuning parametersuch that a set of inequalities be verified for all the possible values of someperturbation vector. A classical approach to solve these problems, whichsatisfies the strong soundness requirement, involves some quantifierelimination procedure such as Collins' Cylindrical Algebraic Decompositionsymbolic method. Sound numerical methods using interval arithmetic and localconsistency enforcement to prune the search space are presented in this paperas much faster alternatives for both soundly solving systems of nonlinearinequalities, and addressing the guaranteed tuning problem whenever theperturbation vector has dimension one. The use of these methods in cameracontrol is investigated, and experiments with the prototype of a declarativemodeller to express camera motion using a cinematic language are reported andcommented."}
1.6e+03 pattern=  0 task=  0 (u=16608)  {'docs': u'In this paper we present a numerical solution for the mathematical modelingof the hot-pressing process applied to medium density fiberboard. The model isbased in the work of Humphrey[82], Humphrey and Bolton[89] and Carvalho andCosta[98], with some modifications and extensions in order to take into accountmainly the convective effects on the phase change term and also a conservativenumerical treatment of the resulting system of partial differential equations.'}
1.46e+03 pattern=  6 task=  0 (u=16645)  {'docs': u"This paper is a structured introduction to Light Affine Logic, and to itsintuitionistic fragment. Light Affine Logic has a polynomially costing cutelimination (P-Time correctness), and encodes all P-Time Turing machines(P-Time completeness). P-Time correctness is proved by introducing the Proofnets for Intuitionistic Light Affine Logic. P-Time completeness is demonstratedin full details thanks to a very compact program notation. On one side, theproof of P-Time correctness describes how the complexity of cut elimination iscontrolled, thanks to a suitable cut elimination strategy that exploitsstructural properties of the Proof nets. This allows to have a good catch onthe meaning of the ``paragraph'' modality, which is a peculiarity of lightlogics. On the other side, the proof of P-Time completeness, together with alot of programming examples, gives a flavor of the non trivial task ofprogramming with resource limitations, using Intuitionistic Light Affine Logicderivations as programs."}
2.17e+03 pattern=  0 task=  0 (u=16694)  {'docs': u"In evaluating an algorithm, worst-case analysis can be overly pessimistic.Average-case analysis can be overly optimistic. An intermediate approach is toshow that an algorithm does well on a broad class of input distributions.Koutsoupias and Papadimitriou recently analyzed the least-recently-used (LRU)paging strategy in this manner, analyzing its performance on an input sequencegenerated by a so-called diffuse adversary -- one that must choose each requestprobabilitistically so that no page is chosen with probability more than somefixed epsilon>0. They showed that LRU achieves the optimal competitive ratio(for deterministic on-line algorithms), but they didn't determine the actualratio.  In this paper we estimate the optimal ratios within roughly a factor of twofor both deterministic strategies (e.g. least-recently-used andfirst-in-first-out) and randomized strategies. Around the threshold epsilon ~1/k (where k is the cache size), the optimal ratios are both Theta(ln k). Belowthe threshold the ratios tend rapidly to O(1). Above the threshold the ratio isunchanged for randomized strategies but tends rapidly to Theta(k) fordeterministic ones.  We also give an alternate proof of the optimality of LRU."}
2.17e+03 pattern=  0 task=  1 (u=16694)  {'docs': u"In the on-line file-caching problem problem, the input is a sequence ofrequests for files, given on-line (one at a time). Each file has a non-negativesize and a non-negative retrieval cost. The problem is to decide which files tokeep in a fixed-size cache so as to minimize the sum of the retrieval costs forfiles that are not in the cache when requested. The problem arises in webcaching by browsers and by proxies. This paper describes a naturalgeneralization of LRU called Landlord and gives an analysis showing that it hasan optimal performance guarantee (among deterministic on-line algorithms).  The paper also gives an analysis of the algorithm in a so-called ``loosely''competitive model, showing that on a ``typical'' cache size, either theperformance guarantee is O(1) or the total retrieval cost is insignificant."}
2.18e+03 pattern=  0 task=  2 (u=16694)  {'docs': u'Mixed packing and covering problems are problems that can be formulated aslinear programs using only non-negative coefficients. Examples includemulticommodity network flow, the Held-Karp lower bound on TSP, fractionalrelaxations of set cover, bin-packing, knapsack, scheduling problems,minimum-weight triangulation, etc. This paper gives approximation algorithmsfor the general class of problems. The sequential algorithm is a simple greedyalgorithm that can be implemented to find an epsilon-approximate solution inO(epsilon^-2 log m) linear-time iterations. The parallel algorithm doescomparable work but finishes in polylogarithmic time.  The results generalize previous work on pure packing and covering (thespecial case when the constraints are all "less-than" or all "greater-than") byMichael Luby and Noam Nisan (1993) and Naveen Garg and Jochen Konemann (1998).'}
2.18e+03 pattern=  0 task=  3 (u=16694)  {'docs': u'Randomized rounding is a standard method, based on the probabilistic method,for designing combinatorial approximation algorithms. In Raghavan\'s seminalpaper introducing the method (1988), he writes: "The time taken to solve thelinear program relaxations of the integer programs dominates the net runningtime theoretically (and, most likely, in practice as well)."  This paper explores how this bottleneck can be avoided for randomizedrounding algorithms for packing and covering problems (linear programs, ormixed integer linear programs, having no negative coefficients). The resultingalgorithms are greedy algorithms, and are faster and simpler to implement thanstandard randomized-rounding algorithms.  This approach can also be used to understand Lagrangian-relaxation algorithmsfor packing/covering linear programs: such algorithms can be viewed as as(derandomized) randomized-rounding schemes.'}
2.18e+03 pattern=  0 task=  4 (u=16694)  {'docs': u'The paper gives approximation algorithms for the k-medians andfacility-location problems (both NP-hard). For k-medians, the algorithm returnsa solution using at most ln(n+n/epsilon)k medians and having cost at most(1+epsilon) times the cost of the best solution that uses at most k medians.Here epsilon > 0 is an input to the algorithm. In comparison, the best previousalgorithm (Jyh-Han Lin and Jeff Vitter, 1992) had a (1+1/epsilon)ln(n) terminstead of the ln(n+n/epsilon) term in the performance guarantee. For facilitylocation, the algorithm returns a solution of cost at most d+ln(n) k, providedthere exists a solution of cost d+k where d is the assignment cost and k is thefacility cost. In comparison, the best previous algorithm (Dorit Hochbaum,1982) returned a solution of cost at most ln(n)(d+k). For both problems, thealgorithms currently provide the best performance guarantee known for thegeneral (non-metric) problems.  The paper also introduces a new probabilistic bound (called "Chernoff-Waldbound") for bounding the expectation of the maximum of a collection of sums ofrandom variables, when each sum contains a random number of terms. The bound isused to analyze the randomized rounding scheme that underlies the algorithms.'}
2.2e+03 pattern=  0 task=  0 (u=16878)  {'docs': u"It is well known that the consensus problem cannot be solveddeterministically in an asynchronous environment, but that randomized solutionsare possible. We propose a new model, called noisy scheduling, in which anadversarial schedule is perturbed randomly, and show that in this modelrandomness in the environment can substitute for randomness in the algorithm.In particular, we show that a simplified, deterministic version of Chandra'swait-free shared-memory consensus algorithm (PODC, 1996, pp. 166-175) solvesconsensus in time at most logarithmic in the number of active processes. Theproof of termination is based on showing that a race between independentdelayed renewal processes produces a winner quickly. In addition, we show thatthe protocol finishes in constant time using quantum and priority-basedscheduling on a uniprocessor, suggesting that it is robust against the choiceof model over a wide range."}
2.29e+03 pattern=  0 task=  1 (u=16878)  {'docs': u'The famous Fischer, Lynch, and Paterson impossibility proof shows that it isimpossible to solve the consensus problem in a natural model of an asynchronousdistributed system if even a single process can fail. Since its publication,two decades of work on fault-tolerant asynchronous consensus algorithms haveevaded this impossibility result by using extended models that provide (a)randomization, (b) additional timing assumptions, (c) failure detectors, or (d)stronger synchronization mechanisms than are available in the basic model.Concentrating on the first of these approaches, we illustrate the history andstructure of randomized asynchronous consensus protocols by giving detaileddescriptions of several such protocols.'}
2.29e+03 pattern=  0 task=  0 (u=16880)  {'docs': u'A fork stack is a generalised stack which allows pushes and pops of severalitems at a time. We consider the problem of determining which input streams canbe sorted using a single forkstack, or dually, which permutations of a fixedinput stream can be produced using a single forkstack. An algorithm is given tosolve the sorting problem and the minimal unsortable sequences are found. Theresults are extended to fork stacks where there are bounds on how many itemscan be pushed and popped at one time. In this context we also establish how toenumerate the collection of sortable sequences.'}
2.93e+03 pattern=  0 task=  0 (u=16890)  {'docs': u'We consider the class F of 2-connected non-planar K_{3,3}-subdivision-freegraphs that are embeddable in the projective plane. We show that these graphsadmit a unique decomposition as a graph K_5 (the core) where the edges arereplaced by two-pole networks constructed from 2-connected planar graphs. Amethod to enumerate these graphs in the labelled case is described. Moreover,we enumerate the homeomorphically irreducible graphs in F and homeomorphicallyirreducible 2-connected planar graphs. Particular use is made of two-poledirected series-parallel networks. We also show that the number m of edges ofgraphs in F with n vertices satisfies the bound m <=3n-6, for n >= 6.'}
3.09e+03 pattern= 11 task=  1 (u=16890)  {'docs': u"Forbidden minors and subdivisions for toroidal graphs are numerous. Weconsider the toroidal graphs with no $K_{3,3}$-subdivisions that coincide withthe toroidal graphs with no $K_{3,3}$-minors. These graphs admit a uniquedecomposition into planar components and have short lists of obstructions. Weprovide the complete lists of four forbidden minors and eleven forbiddensubdivisions for the toroidal graphs with no $K_{3,3}$'s and prove that thelists are sufficient."}
1.61e+03 pattern=  0 task=  0 (u=17070)  {'docs': u'This paper is devoted to the study of the dynamics of a discrete systemrelated to some self stabilizing protocol on a ring of processors.'}
2.95e+03 pattern=  0 task=  0 (u=17119)  {'docs': u"We describe quantum-octave package of functions useful for simulations ofquantum algorithms and protocols. Presented package allows to performsimulations with mixed states. We present numerical implementation of importantquantum mechanical operations - partial trace and partial transpose. Thoseoperations are used as building blocks of algorithms for analysis ofentanglement and quantum error correction codes. Simulation of Shor's algorithmis presented as an example of package capabilities."}
2.76e+03 pattern=  0 task=  0 (u=17126)  {'docs': u"(We apologize for pidgin LaTeX) Schlipf \\cite{sch91} proved that Stable LogicProgramming (SLP) solves all $\\mathit{NP}$ decision problems. We extendSchlipf's result to prove that SLP solves all search problems in the class$\\mathit{NP}$. Moreover, we do this in a uniform way as defined in \\cite{mt99}.Specifically, we show that there is a single $\\mathrm{DATALOG}^{\\neg}$ program$P_{\\mathit{Trg}}$ such that given any Turing machine $M$, any polynomial $p$with non-negative integer coefficients and any input $\\sigma$ of size $n$ overa fixed alphabet $\\Sigma$, there is an extensional database$\\mathit{edb}_{M,p,\\sigma}$ such that there is a one-to-one correspondencebetween the stable models of $\\mathit{edb}_{M,p,\\sigma} \\cup P_{\\mathit{Trg}}$and the accepting computations of the machine $M$ that reach the final state inat most $p(n)$ steps. Moreover, $\\mathit{edb}_{M,p,\\sigma}$ can be computed inpolynomial time from $p$, $\\sigma$ and the description of $M$ and the decodingof such accepting computations from its corresponding stable model of$\\mathit{edb}_{M,p,\\sigma} \\cup P_{\\mathit{Trg}}$ can be computed in lineartime. A similar statement holds for Default Logic with respect to$\\Sigma_2^\\mathrm{P}$-search problems\\footnote{The proof of this resultinvolves additional technical complications and will be a subject of anotherpublication.}."}
2.76e+03 pattern=  0 task=  0 (u=17128)  {'docs': u'Abductive logic programming offers a formalism to declaratively express andsolve problems in areas such as diagnosis, planning, belief revision andhypothetical reasoning. Tabled logic programming offers a computationalmechanism that provides a level of declarativity superior to that of Prolog,and which has supported successful applications in fields such as parsing,program analysis, and model checking. In this paper we show how to use tabledlogic programming to evaluate queries to abductive frameworks with integrityconstraints when these frameworks contain both default and explicit negation.The result is the ability to compute abduction over well-founded semantics withexplicit negation and answer sets. Our approach consists of a transformationand an evaluation method. The transformation adjoins to each objective literal$O$ in a program, an objective literal $not(O)$ along with rules that ensurethat $not(O)$ will be true if and only if $O$ is false. We call the resultingprogram a {\\em dual} program. The evaluation method, \\wfsmeth, then operates onthe dual program. \\wfsmeth{} is sound and complete for evaluating queries toabductive frameworks whose entailment method is based on either thewell-founded semantics with explicit negation, or on answer sets. Further,\\wfsmeth{} is asymptotically as efficient as any known method for either classof problems. In addition, when abduction is not desired, \\wfsmeth{} operatingon a dual program provides a novel tabling method for evaluating queries toground extended programs whose complexity and termination properties aresimilar to those of the best tabling methods for the well-founded semantics. Apublicly available meta-interpreter has been developed for \\wfsmeth{} using theXSB system.'}
2.34e+03 pattern=  0 task=  0 (u=17138)  {'docs': u"Part of the theory of logic programming and nonmonotonic reasoning concernsthe study of fixed-point semantics for these paradigms. Several differentsemantics have been proposed during the last two decades, and some have beenmore successful and acknowledged than others. The rationales behind thosevarious semantics have been manifold, depending on one's point of view, whichmay be that of a programmer or inspired by commonsense reasoning, andconsequently the constructions which lead to these semantics are technicallyvery diverse, and the exact relationships between them have not yet been fullyunderstood. In this paper, we present a conceptually new method, based on levelmappings, which allows to provide uniform characterizations of differentsemantics for logic programs. We will display our approach by giving new anduniform characterizations of some of the major semantics, more particular ofthe least model semantics for definite programs, of the Fitting semantics, andof the well-founded semantics. A novel characterization of the weakly perfectmodel semantics will also be provided."}
2.34e+03 pattern=  0 task=  0 (u=17140)  {'docs': u'Web attacks, i.e. attacks exclusively using the HTTP protocol, are rapidlybecoming one of the fundamental threats for information systems connected tothe Internet. When the attacks suffered by web servers through the years areanalyzed, it is observed that most of them are very similar, using a reducednumber of attacking techniques. It is generally agreed that classification canhelp designers and programmers to better understand attacks and build moresecure applications. As an effort in this direction, a new taxonomy of webattacks is proposed in this paper, with the objective of obtaining apractically useful reference framework for security applications. The use ofthe taxonomy is illustrated by means of multiplatform real world web attackexamples. Along with this taxonomy, important features of each attack categoryare discussed. A suitable semantic-dependent web attack encoding scheme isdefined that uses different-length vectors. Possible applications aredescribed, which might benefit from this taxonomy and encoding scheme, such asintrusion detection systems and application firewalls.'}
2.75e+03 pattern=  0 task=  1 (u=17140)  {'docs': u'A {k,n}-threshold scheme based on two-dimensional memory cellular automata isproposed to share images in a secret way. This method allows to encode an imageinto n shared images so that only qualified subsets of k or more shares canrecover the secret image, but any k-1 or fewer of them gain no informationabout the original image. The main characteristics of this new scheme are: eachshared image has the same size that the original one, and the recovered imageis exactly the same than the secret image; i.e., there is no loss ofresolution.'}
2.5e+03 pattern=  0 task=  0 (u=17141)  {'docs': u'Cluster analysis often serves as the initial step in the process of dataclassification. In this paper, the problem of clustering different length inputdata is considered. The edit distance as the minimum number of elementary editoperations needed to transform one vector into another is used. A heuristic forclustering unequal length vectors, analogue to the well known k-means algorithmis described and analyzed. This heuristic determines cluster centroidsexpanding shorter vectors to the lengths of the longest ones in each cluster ina specific way. It is shown that the time and space complexities of theheuristic are linear in the number of input vectors. Experimental results onreal data originating from a system for classification of Web attacks aregiven.'}
2.34e+03 pattern=  0 task=  0 (u=17142)  {'docs': u"We present a new algorithm for discovering patterns in time series and othersequential data. We exhibit a reliable procedure for building the minimal setof hidden, Markovian states that is statistically capable of producing thebehavior exhibited in the data -- the underlying process's causal states.Unlike conventional methods for fitting hidden Markov models (HMMs) to data,our algorithm makes no assumptions about the process's causal architecture (thenumber of hidden states and their transition structure), but rather infers itfrom the data. It starts with assumptions of minimal structure and introducescomplexity only when the data demand it. Moreover, the causal states it infershave important predictive optimality properties that conventional HMM stateslack. We introduce the algorithm, review the theory behind it, prove itsasymptotic reliability, use large deviation theory to estimate its rate ofconvergence, and compare it to other algorithms which also construct HMMs fromdata. We also illustrate its behavior on an example process, and reportselected numerical results from an implementation."}
1.14e+03 pattern=  0 task=  0 (u=17249)  {'docs': u"This paper presents how the space of spheres and shelling may be used todelete a point from a $d$-dimensional triangulation efficiently. In dimensiontwo, if k is the degree of the deleted vertex, the complexity is O(k log k),but we notice that this number only applies to low cost operations, while timeconsuming computations are only done a linear number of times.  This algorithm may be viewed as a variation of Heller's algorithm, which ispopular in the geographic information system community. Unfortunately, Helleralgorithm is false, as explained in this paper."}
1.14e+03 pattern=  0 task=  1 (u=17249)  {'docs': u'We propose a new data structure to compute the Delaunay triangulation of aset of points in the plane. It combines good worst case complexity, fastbehavior on real data, and small memory occupation.  The location structure is organized into several levels. The lowest leveljust consists of the triangulation, then each level contains the triangulationof a small sample of the levels below. Point location is done by marching in atriangulation to determine the nearest neighbor of the query at that level,then the march restarts from that neighbor at the level below. Using a smallsample (3%) allows a small memory occupation; the march and the use of thenearest neighbor to change levels quickly locate the query.'}
1.14e+03 pattern=  0 task=  2 (u=17249)  {'docs': u'An efficient technique to solve precision problems consists in using exactcomputations. For geometric predicates, using systematically expensive exactcomputations can be avoided by the use of filters. The predicate is firstevaluated using rounding computations, and an error estimation gives acertificate of the validity of the result. In this note, we studies thestatistical efficiency of filters for cosphericity predicate with an assumptionof regular distribution of the points. We prove that the expected value of thepolynomial corresponding to the in sphere test is greater than epsilon withprobability O(epsilon log 1/epsilon) improving the results of a previous paperby the same authors.'}
1.14e+03 pattern=  0 task=  3 (u=17249)  {'docs': u'The assumption of real-number arithmetic, which is at the basis ofconventional geometric algorithms, has been seriously challenged in recentyears, since digital computers do not exhibit such capability.  A geometric predicate usually consists of evaluating the sign of somealgebraic expression. In most cases, rounded computations yield a reliableresult, but sometimes rounded arithmetic introduces errors which may invalidatethe algorithms. The rounded arithmetic may produce an incorrect result only ifthe exact absolute value of the algebraic expression is smaller than some(small) varepsilon, which represents the largest error that may arise in theevaluation of the expression. The threshold varepsilon depends on the structureof the expression and on the adopted computer arithmetic, assuming that theinput operands are error-free.  A pair (arithmetic engine,threshold) is an "arithmetic filter". In this paperwe develop a general technique for assessing the efficacy of an arithmeticfilter. The analysis consists of evaluating both the threshold and theprobability of failure of the filter.  To exemplify the approach, under the assumption that the input points bechosen randomly in a unit ball or unit cube with uniform density, we analyzethe two important predicates "which-side" and "insphere". We show that theprobability that the absolute values of the corresponding determinants be nolarger than some positive value V, with emphasis on small V, is Theta(V) forthe which-side predicate, while for the insphere predicate it is Theta(V^(2/3))in dimension 1, O(sqrt(V)) in dimension 2, and O(sqrt(V) ln(1/V)) in higherdimensions. Constants are small, and are given in the paper.'}
1.21e+03 pattern=  0 task=  4 (u=17249)  {'docs': u'Given a finite set of non-collinear points in the plane, there exists a linethat passes through exactly two points. Such a line is called an ordinary line.An efficient algorithm for computing such a line was proposed by Mukhopadhyayet al. In this note we extend this result in two directions. We first show howto use this algorithm to compute an ordinary conic, that is, a conic passingthrough exactly five points, assuming that all the points do not lie on thesame conic. Both our proofs of existence and the consequent algorithms aresimpler than previous ones. We next show how to compute an ordinary hyperplanein three and higher dimensions.'}
1.14e+03 pattern=  0 task=  0 (u=17278)  {'docs': u'Natural language generation systems (NLG) map non-linguistic representationsinto strings of words through a number of steps using intermediaterepresentations of various levels of abstraction. Template based systems, bycontrast, tend to use only one representation level, i.e. fixed strings, whichare combined, possibly in a sophisticated way, to generate the final text.  In some circumstances, it may be profitable to combine NLG and template basedtechniques. The issue of combining generation techniques can be seen in moreabstract terms as the issue of mixing levels of representation of differentdegrees of linguistic abstraction. This paper aims at defining a referencearchitecture for systems using mixed representations. We argue that mixedrepresentations can be used without abandoning a linguistically groundedapproach to language generation.'}
1.14e+03 pattern=  0 task=  0 (u=17281)  {'docs': u'We provide a lower bound construction showing that the union of unit balls inthree-dimensional space has quadratic complexity, even if they all contain theorigin. This settles a conjecture of Sharir.'}
2.59e+03 pattern=  0 task=  0 (u=17314)  {'docs': u'When implementing regular enough functions (e.g., elementary or specialfunctions) on a computing system, we frequently use polynomial approximations.In most cases, the polynomial that best approximates (for a given distance andin a given interval) a function has coefficients that are not exactlyrepresentable with a finite number of bits. And yet, the polynomialapproximations that are actually implemented do have coefficients that arerepresented with a finite - and sometimes small - number of bits: this is dueto the finiteness of the floating-point representations (for softwareimplementations), and to the need to have small, hence fast and/or inexpensive,multipliers (for hardware implementations). We then have to consider polynomialapproximations for which the degree-$i$ coefficient has at most $m_i$fractional bits (in other words, it is a rational number with denominator$2^{m_i}$). We provide a general method for finding the best polynomialapproximation under this constraint. Then, we suggest refinements than can beused to accelerate our method.'}
2.82e+03 pattern=  0 task=  0 (u=17316)  {'docs': u'In this paper we present a general method for information extraction thatexploits the features of data compression techniques. We first define and focusour attention on the so-called "dictionary" of a sequence. Dictionaries areintrinsically interesting and a study of their features can be of greatusefulness to investigate the properties of the sequences they have beenextracted from (e.g. DNA strings). We then describe a procedure of stringcomparison between dictionary-created sequences (or "artificial texts") thatgives very good results in several contexts. We finally present some results onself-consistent classification problems.'}
2.05e+03 pattern=  0 task=  0 (u=17470)  {'docs': u'To study mechanisms that cause the non-Gaussian nature of network traffic, weanalyzed IP flow statistics. For greedy flows in particular, we investigatedthe hop counts between source and destination nodes, and classifiedapplications by the port number. We found that the main flows contributing tothe non-Gaussian nature of network traffic were HTTP flows with relativelysmall hop counts compared with the average hop counts of all flows.'}
2.28e+03 pattern=  0 task=  1 (u=17470)  {'docs': u"We analyzed the non-Gaussian nature of network traffic using some Internettraffic data. We found that (1) the non-Gaussian nature degrades networkperformance, (2) it is caused by `greedy flows' that exist with non-negligibleprobability, and (3) a large majority of `greedy flows' are TCP flows havingrelatively small hop counts, which correspond to small round-trip times. Weconclude that in a network hat has greedy flows with non-negligibleprobability, a traffic controlling scheme or bandwidth design that considersnon-Gaussian nature is essential."}
3.16e+03 pattern=  0 task=  0 (u=17575)  {'docs': u"Convex relaxations of the optimal finger selection algorithm are proposed fora minimum mean square error (MMSE) Rake receiver in an impulse radioultra-wideband system. First, the optimal finger selection problem isformulated as an integer programming problem with a non-convex objectivefunction. Then, the objective function is approximated by a convex function andthe integer programming problem is solved by means of constraint relaxationtechniques. The proposed algorithms are suboptimal due to the approximateobjective function and the constraint relaxation steps. However, they can beused in conjunction with the conventional finger selection algorithm, which issuboptimal on its own since it ignores the correlation between multipathcomponents, to obtain performances reasonably close to that of the optimalscheme that cannot be implemented in practice due to its complexity. Theproposed algorithms leverage convexity of the optimization problemformulations, which is the watershed between `easy' and `difficult'optimization problems."}
2.06e+03 pattern=  0 task=  0 (u=17591)  {'docs': u'A valuation for a player in a game in extensive form is an assignment ofnumeric values to the players moves. The valuation reflects the desirabilitymoves. We assume a myopic player, who chooses a move with the highestvaluation. Valuations can also be revised, and hopefully improved, after eachplay of the game. Here, a very simple valuation revision is considered, inwhich the moves made in a play are assigned the payoff obtained in the play. Weshow that by adopting such a learning process a player who has a winningstrategy in a win-lose game can almost surely guarantee a win in a repeatedgame. When a player has more than two payoffs, a more elaborate learningprocedure is required. We consider one that associates with each move theaverage payoff in the rounds in which this move was made. When all playersadopt this learning procedure, with some perturbations, then, with probability1, strategies that are close to subgame perfect equilibrium are played aftersome time. A single player who adopts this procedure can guarantee only herindividually rational payoff.'}
2.07e+03 pattern=  0 task=  0 (u=17793)  {'docs': u"The semantics of the Prolog ``cut'' construct is explored in the context ofsome desirable properties of logic programming systems, referred to as thewitness properties. The witness properties concern the operational consistencyof responses to queries. A generalization of Prolog with negation as failureand cut is described, and shown not to have the witness properties. Arestriction of the system is then described, which preserves the choice andfirst-solution behaviour of cut but allows the system to have the witnessproperties.  The notion of cut in the restricted system is more restricted than the Prologhard cut, but retains the useful first-solution behaviour of hard cut, notretained by other proposed cuts such as the ``soft cut''. It is argued that therestricted system achieves a good compromise between the power and utility ofthe Prolog cut and the need for internal consistency in logic programmingsystems. The restricted system is given an abstract semantics, which depends onthe witness properties; this semantics suggests that the restricted system hasa deeper connection to logic than simply permitting some computations which arelogical.  Parts of this paper appeared previously in a different form in theProceedings of the 1995 International Logic Programming Symposium."}
1.3e+03 pattern=  0 task=  0 (u=18003)  {'docs': u'An approach to the solution of NP-complete problems based on quantumcomputing and chaotic dynamics is proposed. We consider the satisfiabilityproblem and argue that the problem, in principle, can be solved in polynomialtime if we combine the quantum computer with the chaotic dynamics amplifierbased on the logistic map. We discuss a possible implementation of such achaotic quantum computation by using the atomic quantum computer with quantumgates described by the Hartree-Fock equations. In this case, in principle, onecan build not only standard linear quantum gates but also nonlinear gates andmoreover they obey to Fermi statistics. This new type of entaglement relatedwith Fermi statistics can be interesting also for quantum communication theory.'}
2.38e+03 pattern=  0 task=  0 (u=18115)  {'docs': u'This paper solves the problem of computing conformal structures of general2-manifolds represented as triangle meshes. We compute conformal structures inthe following way: first compute homology bases from simplicial complexstructures, then construct dual cohomology bases and diffuse them to harmonic1-forms. Next, we construct bases of holomorphic differentials. We then obtainperiod matrices by integrating holomorphic differentials along homology bases.We also study the global conformal mapping between genus zero surfaces andspheres, and between general meshes and planes. Our method of computingconformal structures can be applied to tackle fundamental problems in computeraid design and computer graphics, such as geometry classification andidentification, and surface global parametrization.'}
2.5e+03 pattern=  0 task=  0 (u=18126)  {'docs': u"We introduce and analyze an efficient family of linear feedback shiftregisters (LFSR's) with maximal period. This family is word-oriented and issuitable for implementation in software, thus provides a solution to a recentchallenge posed in FSE '94. The classical theory of LFSR's is extended toprovide efficient algorithms for generation of irreducible and primitive LFSR'sof this new type."}
2.5e+03 pattern=  0 task=  0 (u=18128)  {'docs': u'Environment maps are used to simulate reflections off curved objects. Wepresent a technique to reflect a user, or a group of users, in a realenvironment, onto a virtual object, in a virtual reality application, using thelive video feeds from a set of cameras, in real-time. Our setup can be used ina variety of environments ranging from outdoor or indoor scenes.'}
2.92e+03 pattern=  0 task=  0 (u=18258)  {'docs': u'During 2003 test beam session for ATLAS Tile Calorimeter a monitoring programhas been developed to ease the setup of correct running condition and theassessment of data quality. The program has been built using the OnlineSoftware services provided by the ATLAS Online Software group. The first partof this note contains a brief overview of these services followed by the fulldescription of Tile Calorimeter monitoring program architecture and features.Performances and future upgrades are discussed in the final part of this note.'}
2.07e+03 pattern=  0 task=  0 (u=18387)  {'docs': u'Capital accumulation has been a major issue in fisheries economics over thelast two decades, whereby the interaction of the fish and capital stocks wereof particular interest. Because bio-economic systems are intrinsically complex,previous efforts in this field have relied on a variety of simplifyingassumptions. The model presented here relaxes some of these simplifications.Problems of tractability are surmounted by using the methodology of qualitativedifferential equations (QDE). The theory of QDEs takes into account thatscientific knowledge about particular fisheries is usually limited, andfacilitates an analysis of the global dynamics of systems with more than twoordinary differential equations. The model is able to trace the evolution ofcapital and fish stock in good agreement with observed patterns, and shows thatover-capitalization is unavoidable in unregulated fisheries.'}
2.07e+03 pattern=  0 task=  0 (u=18405)  {'docs': u'Existing refinement calculi provide frameworks for the stepwise developmentof imperative programs from specifications. This paper presents a refinementcalculus for deriving logic programs. The calculus contains a wide-spectrumlogic programming language, including executable constructs such as sequentialconjunction, disjunction, and existential quantification, as well asspecification constructs such as general predicates, assumptions and universalquantification. A declarative semantics is defined for this wide-spectrumlanguage based on executions. Executions are partial functions from states tostates, where a state is represented as a set of bindings. The semantics isused to define the meaning of programs and specifications, including parametersand recursion. To complete the calculus, a notion of correctness-preservingrefinement over programs in the wide-spectrum language is defined andrefinement laws for developing programs are introduced. The refinement calculusis illustrated using example derivations and prototype tool support isdiscussed.'}
2.23e+03 pattern=  0 task=  0 (u=18462)  {'docs': u'Previous work in the area of tracing CLP(FD) programs mainly focuses onproviding information about control of execution and domain modification. Inthis paper, we present a trace structure that provides information aboutadditional important aspects. We incorporate explanations in the tracestructure, i.e. reasons for why certain solver actions occur. Furthermore, wecome up with a format for describing the execution of the filtering algorithmsof global constraints. Some new ideas about the design of the trace are alsopresented. For example, we have modeled our trace as a nested block structurein order to achieve a hierarchical view. Also, new ways about how to representand identify different entities such as constraints and domain variables arepresented.'}
3.15e+03 pattern=  0 task=  0 (u=18504)  {'docs': u'The effect of Rician-ness on the capacity of multiple antenna systems isinvestigated under the assumption that channel state information (CSI) isavailable only at the receiver. The average-power-constrained capacity of suchsystems is considered under two different assumptions on the knowledge aboutthe fading available at the transmitter: the case in which the transmitter hasno knowledge of fading at all, and the case in which the transmitter hasknowledge of the distribution of the fading process but not the instantaneousCSI. The exact capacity is given for the former case while capacity bounds arederived for the latter case. A new signalling scheme is also proposed for thelatter case and it is shown that by exploiting the knowledge of Rician-ness atthe transmitter via this signalling scheme, significant capacity gain can beachieved. The derived capacity bounds are evaluated explicitly to providenumerical results in some representative situations.'}
1.64e+03 pattern=  0 task=  0 (u=18510)  {'docs': u"Transient faults corrupt the content and organization of data structures. Arecovery technique dealing with such faults is stabilization, which guarantees,following some number of operations on the data structure, that content of thedata structure is legitimate. Another notion of fault tolerance isavailability, which is the property that operations continue to be appliedduring the period of recovery after a fault, and successful updates are notlost while the data structure stabilizes to a legitimate state. The available,stabilizing 2-3 tree supports find, insert, and delete operations, each withO(lg n) complexity when the tree's state is legitimate and contains n items.For an illegitimate state, these operations have O(lg K) complexity where K isthe maximum capacity of the tree. Within O(t) operations, the state of the treeis guaranteed to be legitimate, where t is the number of nodes accessible viasome path from the tree's root at the initial state. This paper resolves, forthe first time, issues of dynamic allocation and pointer organization in astabilizing data structure."}
2.24e+03 pattern=  0 task=  0 (u=18610)  {'docs': u'Classical logic predicts that everything (thus nothing useful at all) followsfrom inconsistency. A paraconsistent logic is a logic where an inconsistencydoes not lead to such an explosion, and since in practice consistency isdifficult to achieve there are many potential applications of paraconsistentlogics in knowledge-based systems, logical semantics of natural language, etc.Higher order logics have the advantages of being expressive and with severalautomated theorem provers available. Also the type system can be helpful. Wepresent a concise description of a paraconsistent higher order logic withcountable infinite indeterminacy, where each basic formula can get its ownindeterminate truth value (or as we prefer: truth code). The meaning of thelogical operators is new and rather different from traditional many-valuedlogics as well as from logics based on bilattices. The adequacy of the logic isexamined by a case study in the domain of medicine. Thus we try to build abridge between the HOL and MVL communities. A sequent calculus is proposedbased on recent work by Muskens.'}
2.78e+03 pattern=  0 task=  0 (u=18753)  {'docs': u"In this paper, we model the cost incurred by each peer participating in apeer-to-peer network. Such a cost model allows to gauge potential disincentivesfor peers to collaborate, and provides a measure of the ``total cost'' of anetwork, which is a possible benchmark to distinguish between proposals. Wecharacterize the cost imposed on a node as a function of the experienced loadand the node connectivity, and show how our model applies to a few proposedrouting geometries for distributed hash tables (DHTs). We further outline anumber of open questions this research has raised."}
2.53e+03 pattern=  0 task=  0 (u=18787)  {'docs': u'We present the first in-place algorithm for sorting an array of size n thatperforms, in the worst case, at most O(n log n) element comparisons and O(n)element transports.  This solves a long-standing open problem, stated explicitly, e.g., in [J.I.Munro and V. Raman, Sorting with minimum data movement, J. Algorithms, 13,374-93, 1992], of whether there exists a sorting algorithm that matches theasymptotic lower bounds on all computational resources simultaneously.'}
3e+03 pattern=  0 task=  0 (u=19180)  {'docs': u'XSLT is an increasingly popular language for processing XML data. It iswidely supported by application platform software. However, little optimizationeffort has been made inside the current XSLT processing engines. Evaluating avery simple XSLT program on a large XML document with a simple schema mayresult in extensive usage of memory. In this paper, we present a novel notionof \\emph{Streaming Processing Model} (\\emph{SPM}) to evaluate a subset of XSLTprograms on XML documents, especially large ones. With SPM, an XSLT processorcan transform an XML source document to other formats without extra memorybuffers required. Therefore, our approach can not only tackle large sourcedocuments, but also produce large results. We demonstrate with a performancestudy the advantages of the SPM approach. Experimental results clearly confirmthat SPM improves XSLT evaluation typically 2 to 10 times better than theexisting approaches. Moreover, the SPM approach also features high scalability.'}
3e+03 pattern=  0 task=  0 (u=19185)  {'docs': u"We discuss long-term preservation of and access to relational databases. Thefocus is on national archives and science data archives which have to ingestand integrate data from a broad spectrum of vendor-specific relational databasemanagement systems (RDBMS). Furthermore, we present our solution SIARD whichanalyzes and extracts data and data logic from almost any RDBMS. It enables, toa reasonable level of authenticity, complete detachment of databases from theirvendor-specific environment. The user can add archival descriptive metadataaccording to a customizable schema. A SIARD database archive integrates data,data logic, technical metadata, and archival descriptive information in onearchival information package, independent of any specific software andhardware, based upon plain text files and the standardized languages SQL andXML. For usage purposes, a SIARD archive can be reloaded into any current orfuture RDBMS which supports standard SQL. In addition, SIARD contains a clientthat enables 'on demand' reload of archives into a target RDBMS, and multi-userremote access for querying and browsing the data together with its technicaland descriptive metadata in one graphical user interface."}
2.14e+03 pattern=  0 task=  0 (u=19224)  {'docs': u'We consider the traveling salesman problem when the cities are points in R^dfor some fixed d and distances are computed according to geometric distances,determined by some norm. We show that for any polyhedral norm, the problem offinding a tour of maximum length can be solved in polynomial time. Ifarithmetic operations are assumed to take unit time, our algorithms run in timeO(n^{f-2} log n), where f is the number of facets of the polyhedron determiningthe polyhedral norm. Thus for example we have O(n^2 log n) algorithms for thecases of points in the plane under the Rectilinear and Sup norms. This is incontrast to the fact that finding a minimum length tour in each case isNP-hard. Our approach can be extended to the more general case of quasi-normswith not necessarily symmetric unit ball, where we get a complexity ofO(n^{2f-2} log n).  For the special case of two-dimensional metrics with f=4 (which includes theRectilinear and Sup norms), we present a simple algorithm with O(n) runningtime. The algorithm does not use any indirect addressing, so its running timeremains valid even in comparison based models in which sorting requires Omega(n\\log n) time. The basic mechanism of the algorithm provides some intuition onwhy polyhedral norms allow fast algorithms.  Complementing the results on simplicity for polyhedral norms, we prove thatfor the case of Euclidean distances in R^d for d>2, the Maximum TSP is NP-hard.This sheds new light on the well-studied difficulties of Euclidean distances.'}
1.72e+03 pattern=  0 task=  0 (u=19238)  {'docs': u"We develop a theory of the algorithmic information in bits contained in anindividual pure quantum state. This extends classical Kolmogorov complexity tothe quantum domain retaining classical descriptions. Quantum Kolmogorovcomplexity coincides with the classical Kolmogorov complexity on the classicaldomain. Quantum Kolmogorov complexity is upper bounded and can be effectivelyapproximated from above under certain conditions. With high probability aquantum object is incompressible. Upper- and lower bounds of the quantumcomplexity of multiple copies of individual pure quantum states are derived andmay shed some light on the no-cloning properties of quantum states. In thequantum situation complexity is not sub-additive. We discuss some relationswith ``no-cloning'' and ``approximate cloning'' properties."}
3.02e+03 pattern=  0 task=  0 (u=19294)  {'docs': u"This paper provides a new conceptual perspective on survey propagation, whichis an iterative algorithm recently introduced by the statistical physicscommunity that is very effective in solving random k-SAT problems even withdensities close to the satisfiability threshold. We first describe how any SATformula can be associated with a novel family of Markov random fields (MRFs),parameterized by a real number \\rho \\in [0,1]. We then show that applyingbelief propagation--a well-known ``message-passing'' technique for estimatingmarginal probabilities--to this family of MRFs recovers a known family ofalgorithms, ranging from pure survey propagation at one extreme (\\rho = 1) tostandard belief propagation on the uniform distribution over SAT assignments atthe other extreme (\\rho = 0). Configurations in these MRFs have a naturalinterpretation as partial satisfiability assignments, on which a partial ordercan be defined. We isolate cores as minimal elements in this partial ordering,which are also fixed points of survey propagation and the only assignments withpositive probability in the MRF for \\rho=1. Our experimental results for k=3suggest that solutions of random formulas typically do not possess non-trivialcores. This makes it necessary to study the structure of the space of partialassignments for \\rho<1 and investigate the role of assignments that are veryclose to being cores. To that end, we investigate the associated latticestructure, and prove a weight-preserving identity that shows how any MRF with\\rho>0 can be viewed as a ``smoothed'' version of the uniform distribution oversatisfying assignments (\\rho=0). Finally, we isolate properties of Gibbssampling and message-passing algorithms that are typical for an ensemble ofk-SAT problems."}
3.02e+03 pattern=  0 task=  0 (u=19296)  {'docs': u"In order to investigate the routing aspects of small-world networks,Kleinberg proposes a network model based on a $d$-dimensional lattice withlong-range links chosen at random according to the $d$-harmonic distribution.Kleinberg shows that the greedy routing algorithm by using only localinformation performs in $O(\\log^2 n)$ expected number of hops, where $n$denotes the number of nodes in the network. Martel and Nguyen have found thatthe expected diameter of Kleinberg's small-world networks is $\\Theta(\\log n)$.Thus a question arises naturally: Can we improve the routing algorithms tomatch the diameter of the networks while keeping the amount of informationstored on each node as small as possible? We extend Kleinberg's model and addthree augmented local links for each node: two of which are connected to nodeschosen randomly and uniformly within $\\log^2 n$ Mahattan distance, and thethird one is connected to a node chosen randomly and uniformly within $\\log n$Mahattan distance. We show that if each node is aware of $O(\\log n)$ number ofneighbors via the augmented local links, there exist both non-oblivious andoblivious algorithms that can route messages between any pair of nodes in$O(\\log n \\log \\log n)$ expected number of hops, which is a near optimalrouting complexity and outperforms the other related results for routing inKleinberg's small-world networks. Our schemes keep only $O(\\log^2 n)$ bits ofrouting information on each node, thus they are scalable with the network size.Besides adding new light to the studies of social networks, our results mayalso find applications in the design of large-scale distributed networks, suchas peer-to-peer systems, in the same spirit of Symphony."}
3.07e+03 pattern=  0 task=  1 (u=19296)  {'docs': u'We propose a simple distributed hash table called ReCord, which is ageneralized version of Randomized-Chord and offers improved tradeoffs inperformance and topology maintenance over existing P2P systems. ReCord isscalable and can be easily implemented as an overlay network, and offers a goodtradeoff between the node degree and query latency. For instance, an $n$-nodeReCord with $O(\\log n)$ node degree has an expected latency of $\\Theta(\\log n)$hops. Alternatively, it can also offer $\\Theta(\\frac{\\log n}{\\log \\log n})$hops latency at a higher cost of $O(\\frac{\\log^2 n}{\\log  \\log n})$ node degree. Meanwhile, simulations of the dynamic behaviors ofReCord are studied.'}
3.15e+03 pattern=  0 task=  0 (u=19370)  {'docs': u'Estimating the number of sources impinging on an array of sensors is a wellknown and well investigated problem. A common approach for solving this problemis to use an information theoretic criterion, such as Minimum DescriptionLength (MDL) or the Akaike Information Criterion (AIC). The MDL estimator isknown to be a consistent estimator, robust against deviations from the Gaussianassumption, and non-robust against deviations from the point source and/ortemporally or spatially white additive noise assumptions. Over the yearsseveral alternative estimation algorithms have been proposed and tested.Usually, these algorithms are shown, using computer simulations, to haveimproved performance over the MDL estimator, and to be robust againstdeviations from the assumed spatial model. Nevertheless, these robustalgorithms have high computational complexity, requiring severalmulti-dimensional searches.  In this paper, motivated by real life problems, a systematic approach towardthe problem of robust estimation of the number of sources using informationtheoretic criteria is taken. An MDL type estimator that is robust againstdeviation from assumption of equal noise level across the array is studied. Theconsistency of this estimator, even when deviations from the equal noise levelassumption occur, is proven. A novel low-complexity implementation methodavoiding the need for multi-dimensional searches is presented as well, makingthis estimator a favorable choice for practical applications.'}
3.16e+03 pattern=  0 task=  1 (u=19370)  {'docs': u'One of the features characterizing almost every multiple access (MA)communication system is the processing gain. Through the use of spreadingsequences, the processing gain of Random CDMA systems (RCDMA), is devoted toboth bandwidth expansion and orthogonalization of the signals transmitted bydifferent users. Another type of multiple access system is Impulse Radio (IR).In many aspects, IR systems are similar to time division multiple access (TDMA)systems, and the processing gain of IR systems represents the ratio between theactual transmission time and the total time between two consecutiveransmissions (on-plus-off to on ratio). While CDMA systems, which constantlyexcite the channel, rely on spreading sequences to orthogonalize the signalstransmitted by different users, IR systems transmit a series of short pulsesand the orthogonalization between the signals transmitted by different users isachieved by the fact that most of the pulses do not collide with each other atthe receiver.  In this paper, a general class of MA communication systems that use bothtypes of processing gain is presented, and both IR and RCDMA systems aredemonstrated to be two special cases of this more general class of systems. Thebit error rate (BER) of several receivers as a function of the ratio betweenthe two types of processing gain is analyzed and compared under the constraintthat the total processing gain of the system is large and fixed. It isdemonstrated that in non inter-symbol interference (ISI) channels there is notradeoff between the two types of processing gain. However, in ISI channels atradeoff between the two types of processing gain exists. In addition, thesub-optimality of RCDMA systems in frequency selective channels is established.'}
3.16e+03 pattern=  0 task=  0 (u=19378)  {'docs': u'In this paper, we propose a new method based on Hidden Markov Models tointerpret temporal sequences of sensor data from mobile robots to automaticallydetect features. Hidden Markov Models have been used for a long time in patternrecognition, especially in speech recognition. Their main advantages over othermethods (such as neural networks) are their ability to model noisy temporalsignals of variable length. We show in this paper that this approach is wellsuited for interpretation of temporal sequences of mobile-robot sensor data. Wepresent two distinct experiments and results: the first one in an indoorenvironment where a mobile robot learns to detect features like open doors orT-intersections, the second one in an outdoor environment where a differentmobile robot has to identify situations like climbing a hill or crossing arock.'}
3.26e+03 pattern=  0 task=  0 (u=19379)  {'docs': u'In the frame of designing a knowledge discovery system, we have developedstochastic models based on high-order hidden Markov models. These models arecapable to map sequences of data into a Markov chain in which the transitionsbetween the states depend on the \\texttt{n} previous states according to theorder of the model. We study the process of achieving information extractionfromspatial and temporal data by means of an unsupervised classification. Weuse therefore a French national database related to the land use of a region,named Teruti, which describes the land use both in the spatial and temporaldomain. Land-use categories (wheat, corn, forest, ...) are logged every year oneach site regularly spaced in the region. They constitute a temporal sequenceof images in which we look for spatial and temporal dependencies. The temporalsegmentation of the data is done by means of a second-order Hidden Markov Model(\\hmmd) that appears to have very good capabilities to locate stationarysegments, as shown in our previous work in speech recognition. Thespatialclassification is performed by defining a fractal scanning ofthe images withthe help of a Hilbert-Peano curve that introduces atotal order on the sites,preserving the relation ofneighborhood between the sites. We show that the\\hmmd performs aclassification that is meaningful for the agronomists.Spatialand temporal classification may be achieved simultaneously by means of a 2levels \\hmmd that measures the \\aposteriori probability to map a temporalsequence of images onto a set of hidden classes.'}
1.26e+03 pattern=  5 task=  0 (u=19513)  {'docs': u'Recent developments in theoretical linguistics have lead to a widespreadacceptance of constraint-based analyses of prosodic morphology phenomena suchas truncation, infixation, floating morphemes and reduplication. Of these,reduplication is particularly challenging for state-of-the-art computationalmorphology, since it involves copying of some part of a phonological string. Inthis paper I argue for certain extensions to the one-level model of phonologyand morphology (Bird & Ellison 1994) to cover the computational aspects ofprosodic morphology using finite-state methods. In a nutshell, enriched lexicalrepresentations provide additional automaton arcs to repeat or skip sounds andalso to allow insertion of additional material. A kind of resourceconsciousness is introduced to control this additional freedom, distinguishingbetween producer and consumer arcs. The non-finite-state copying aspect ofreduplication is mapped to automata intersection, itself a non-finite-stateoperation. Bounded local optimization prunes certain automaton arcs that failto contribute to linguistic optimisation criteria. The paper then presentsimplemented case studies of Ulwa construct state infixation, Germanhypocoristic truncation and Tagalog over-applying reduplication that illustratethe expressive power of this approach, before its merits and limitations arediscussed and possible extensions are sketched. I conclude that the one-levelapproach to prosodic morphology presents an attractive way of extendingfinite-state techniques to difficult phenomena that hitherto resisted elegantcomputational analyses.'}
1.59e+03 pattern=  0 task=  0 (u=19532)  {'docs': u'Mobile code based computing requires development of protection schemes thatallow digital signature and encryption of data collected by the agents inuntrusted hosts. These algorithms could not rely on carrying encryption keys ifthese keys could be stolen or used to counterfeit data by hostile hosts andagents. As a consequence, both information and keys must be protected in a waythat only authorized hosts, that is the host that provides information and theserver that has sent the mobile agent, could modify (by changing or removing)retrieved data. The data management model proposed in this work allows theinformation collected by the agents to be protected against handling by otherhosts in the information network. It has been done by using standard public-keycryptography modified to support protection of data in distributed environmentswithout requiring an interactive protocol with the host that has dropped theagent. Their significance stands on the fact that it is the first model thatsupports a full-featured protection of mobile agents allowing remote hosts tochange its own information if required before agent returns to its originatingserver.'}
1.45e+03 pattern=  0 task=  0 (u=19533)  {'docs': u'A one-time pad (OTP) based cipher to insure both data protection andintegrity when mobile code arrives to a remote host is presented. Dataprotection is required when a mobile agent could retrieve confidentialinformation that would be encrypted in untrusted nodes of the network; in thiscase, information management could not rely on carrying an encryption key. Dataintegrity is a prerequisite because mobile code must be protected againstmalicious hosts that, by counterfeiting or removing collected data, could coverinformation to the server that has sent the agent. The algorithm described inthis article seems to be simple enough, so as to be easily implemented. Thisscheme is based on a non-interactive protocol and allows a remote host tochange its own data on-the-fly and, at the same time, protecting informationagainst handling by other hosts.'}
1.44e+03 pattern=  0 task=  0 (u=19543)  {'docs': u'When reasoning in description, modal or temporal logics it is often useful toconsider axioms representing universal truths in the domain of discourse.Reasoning with respect to an arbitrary set of axioms is hard, even forrelatively inexpressive logics, and it is essential to deal with such axioms inan efficient manner if implemented systems are to be effective in realapplications. This is particularly relevant to Description Logics, wheresubsumption reasoning with respect to a terminology is a fundamental problem.Two optimisation techniques that have proved to be particularly effective indealing with terminologies are lazy unfolding and absorption. In this paper weseek to improve our theoretical understanding of these important techniques. Wedefine a formal framework that allows the techniques to be precisely described,establish conditions under which they can be safely applied, and prove that,provided these conditions are respected, subsumption testing algorithms willstill function correctly. These results are used to show that the proceduresused in the FaCT system are correct and, moreover, to show how efficiency canbe significantly improved, while still retaining the guarantee of correctness,by relaxing the safety conditions for absorption.'}
2.55e+03 pattern=  0 task=  0 (u=19558)  {'docs': u'The relationship between the length of a word and the maximum length of itsunbordered factors is investigated in this paper. Consider a finite word w oflength n. We call a word bordered, if it has a proper prefix which is also asuffix of that word. Let f(w) denote the maximum length of all unborderedfactors of w, and let p(w) denote the period of w. Clearly, f(w) < p(w)+1.  We establish that f(w) = p(w), if w has an unbordered prefix of length f(w)and n > 2f(w)-2. This bound is tight and solves the stronger version of a 21years old conjecture by Duval. It follows from this result that, in general, n> 3f(w)-3 implies f(w) = p(w) which gives an improved bound for the questionasked by Ehrenfeucht and Silberger in 1979.'}
3.26e+03 pattern=  0 task=  0 (u=19607)  {'docs': u'We present a general method for introducing finitely axiomatizable "minimal"two-sorted theories for various subclasses of P (problems solvable inpolynomial time). The two sorts are natural numbers and finite sets of naturalnumbers. The latter are essentially the finite binary strings, which provide anatural domain for defining the functions and sets in small complexity classes.We concentrate on the complexity class TC^0, whose problems are defined byuniform polynomial-size families of bounded-depth Boolean circuits withmajority gates. We present an elegant theory VTC^0 in which the provably-totalfunctions are those associated with TC^0, and then prove that VTC^0 is"isomorphic" to a different-looking single-sorted theory introduced byJohannsen and Pollet. The most technical part of the isomorphism proof isdefining binary number multiplication in terms a bit-counting function, andshowing how to formalize the proofs of its algebraic properties.'}
1.78e+03 pattern=  0 task=  0 (u=19864)  {'docs': u'We consider the problem of selecting a portfolio of assets that provides theinvestor a suitable balance of expected return and risk. With respect to theseminal mean-variance model of Markowitz, we consider additional constraints onthe cardinality of the portfolio and on the quantity of individual shares. Suchconstraints better capture the real-world trading system, but make the problemmore difficult to be solved with exact methods. We explore the use of localsearch techniques, mainly tabu search, for the portfolio selection problem. Wecompare and combine previous work on portfolio selection that makes use of thelocal search approach and we propose new algorithms that combine differentneighborhood relations. In addition, we show how the use of randomization andof a simple form of adaptiveness simplifies the setting of a large number ofcritical parameters. Finally, we show how our techniques perform on publicbenchmarks.'}
2.42e+03 pattern=  0 task=  0 (u=19890)  {'docs': u'In this paper we propose a simple and efficient data structure yielding aperfect hashing of quite general arrays. The data structure is named phorma,which is an acronym for perfectly hashable order restricted multidimensionalarray.  Keywords: Perfect hash function, Digraph, Implicit enumeration,Nijenhuis-Wilf combinatorial family.'}
2.43e+03 pattern=  0 task=  1 (u=19890)  {'docs': u'In this paper we propose a simple and efficient strategy to obtain a datastructure generator to accomplish a perfect hash of quite general orderrestricted multidimensional arrays named {\\em phormas}. The constructor of suchobjects gets two parameters as input: an n-vector a of non negative integersand a boolean function B on the types of order restrictions on the coordinatesof the valid n-vectors bounded by a. At compiler time, the phorma constructorbuilds, from the pair a,B, a digraph G(a,B) with a single source s and a singlesink t such that the st-paths are in 1-1 correspondence with the members of theB-restricted a-bounded array A(a,B). Besides perfectly hashing A(a,B), G(a,B)is an instance of an NW-family. This permits other useful computational taskson it.'}
2.35e+03 pattern=  0 task=  0 (u=19945)  {'docs': u'We propose a novel criterion for support vector machine learning: maximizingthe margin in the input space, not in the feature (Hilbert) space. Thiscriterion is a discriminative version of the principal curve proposed by Hastieet al. The criterion is appropriate in particular when the input space isalready a well-designed feature space with rather small dimensionality. Thedefinition of the margin is generalized in order to represent prior knowledge.The derived algorithm consists of two alternating steps to estimate the dualparameters. Firstly, the parameters are initialized by the original SVM. Thenone set of parameters is updated by Newton-like procedure, and the other set isupdated by solving a quadratic programming problem. The algorithm converges ina few steps to a local optimum under mild conditions and it preserves thesparsity of support vectors. Although the complexity to calculate temporalvariables increases the complexity to solve the quadratic programming problemfor each step does not change. It is also shown that the original SVM can beseen as a special case. We further derive a simplified algorithm which enablesus to use the existing code for the original SVM.'}
2.35e+03 pattern=  0 task=  0 (u=19946)  {'docs': u'In biological data, it is often the case that observed data are availableonly for a subset of samples. When a kernel matrix is derived from such data,we have to leave the entries for unavailable samples as missing. In this paper,we make use of a parametric model of kernel matrices, and estimate missingentries by fitting the model to existing entries. The parametric model iscreated as a set of spectral variants of a complete kernel matrix derived fromanother information source. For model fitting, we adopt the em algorithm basedon the information geometry of positive definite matrices. We will reportpromising results on bacteria clustering experiments using two markersequences: 16S and gyrB.'}
3.06e+03 pattern=  0 task=  0 (u=20053)  {'docs': u'In this paper, we analyze the performance of space-time block codes whichenable symbolwise maximum likelihood decoding. We derive an upper bound ofmaximum mutual information (MMI) on space-time block codes that enablesymbolwise maximum likelihood decoding for a frequency non-selectivequasi-static fading channel. MMI is an upper bound on how much one can sendinformation with vanishing error probability by using the target code.'}
1.01e+03 pattern=  0 task=  0 (u=20288)  {'docs': u'This paper, following (Dymetman:1998), presents an approach to grammardescription and processing based on the geometry of cancellation diagrams, aconcept which plays a central role in combinatorial group theory(Lyndon-Schuppe:1977). The focus here is on the geometric intuitions and onrelating group-theoretical diagrams to the traditional charts associated withcontext-free grammars and type-0 rewriting systems. The paper is structured asfollows. We begin in Section 1 by analyzing charts in terms of constructscalled cells, which are a geometrical counterpart to rules. Then we move inSection 2 to a presentation of cancellation diagrams and show how they can beused computationally. In Section 3 we give a formal algebraic presentation ofthe concept of group computation structure, which is based on the standardnotions of free group and conjugacy. We then relate in Section 4 the geometricand the algebraic views of computation by using the fundamental theorem ofcombinatorial group theory (Rotman:1994). In Section 5 we study in more detailthe relationship between the two views on the basis of a simple grammar statedas a group computation structure. In section 6 we extend this grammar to handlenon-local constructs such as relative pronouns and quantifiers. We conclude inSection 7 with some brief notes on the differences between normal submonoidsand normal subgroups, group computation versus rewriting systems, and the useof group morphisms to study the computational complexity of parsing andgeneration.'}
1.01e+03 pattern=  0 task=  0 (u=20290)  {'docs': u"Recent technological advances have made it possible to build real-time,interactive spoken dialogue systems for a wide variety of applications.However, when users do not respect the limitations of such systems, performancetypically degrades. Although users differ with respect to their knowledge ofsystem limitations, and although different dialogue strategies make systemlimitations more apparent to users, most current systems do not try to improveperformance by adapting dialogue behavior to individual users. This paperpresents an empirical evaluation of TOOT, an adaptable spoken dialogue systemfor retrieving train schedules on the web. We conduct an experiment in which 20users carry out 4 tasks with both adaptable and non-adaptable versions of TOOT,resulting in a corpus of 80 dialogues. The values for a wide range ofevaluation measures are then extracted from this corpus. Our results show thatadaptable TOOT generally outperforms non-adaptable TOOT, and that the utilityof adaptation depends on TOOT's initial dialogue strategies."}
3.11e+03 pattern=  0 task=  0 (u=20368)  {'docs': u'Multi-dimensional data classification is an important and challenging problemin many astro-particle experiments. Neural networks have proved to be versatileand robust in multi-dimensional data classification. In this article we shallstudy the classification of gamma from the hadrons for the MAGIC Experiment.Two neural networks have been used for the classification task. One isMulti-Layer Perceptron based on supervised learning and other isSelf-Organising Map (SOM), which is based on unsupervised learning technique.The results have been shown and the possible ways of combining these networkshave been proposed to yield better and faster classification results.'}
3.05e+03 pattern=  0 task=  0 (u=20498)  {'docs': u'A great deal of effort has been spent measuring topological features of theInternet. However, it was recently argued that sampling based on taking pathsor traceroutes through the network from a small number of sources introduces afundamental bias in the observed degree distribution. We examine this biasanalytically and experimentally. For Erdos-Renyi random graphs with mean degreec, we show analytically that traceroute sampling gives an observed degreedistribution P(k) ~ 1/k for k < c, even though the underlying degreedistribution is Poisson. For graphs whose degree distributions have power-lawtails P(k) ~ k^-alpha, traceroute sampling from a small number of sources cansignificantly underestimate the value of \\alpha when the graph has a largeexcess (i.e., many more edges than vertices). We find that in order to obtain agood estimate of alpha it is necessary to use a number of sources which growslinearly in the average degree of the underlying graph. Based on theseobservations we comment on the accuracy of the published values of alpha forthe Internet.'}
1.06e+03 pattern=  0 task=  0 (u=20543)  {'docs': u"Consider the finite regular language L_n = {w0 : w \\in {0,1}^*, |w| \\le n}.It was shown by Ambainis, Nayak, Ta-Shma and Vazirani that while this languageis accepted by a deterministic finite automaton of size O(n), any one-wayquantum finite automaton (QFA) for it has size 2^{Omega(n/log n)}. This wasbased on the fact that the evolution of a QFA is required to be reversible.When arbitrary intermediate measurements are allowed, this intuition breaksdown. Nonetheless, we show a 2^{Omega(n)} lower bound for such QFA for L_n,thus also improving the previous bound. The improved bound is obtained bysimple entropy arguments based on Holevo's theorem. This method also allows usto obtain an asymptotically optimal (1-H(p))n bound for the dense quantum codes(random access codes) introduced by Ambainis et al. We then turn to Holevo'stheorem, and show that in typical situations, it may be replaced by a tighterand more transparent in-probability bound."}
1.45e+03 pattern=  0 task=  1 (u=20543)  {'docs': u"One of the most intriguing facts about communication using quantum states isthat these states cannot be used to transmit more classical bits than thenumber of qubits used, yet there are ways of conveying information withexponentially fewer qubits than possible classically. Moreover, these methodshave a very simple structure---they involve little interaction between thecommunicating parties. We look more closely at the ways in which informationencoded in quantum states may be manipulated, and consider the question as towhether every classical protocol may be transformed to a ``simpler'' quantumprotocol of similar efficiency. By a simpler protocol, we mean a protocol thatuses fewer message exchanges. We show that for any constant k, there is aproblem such that its k+1 message classical communication complexity isexponentially smaller than its k message quantum communication complexity, thusanswering the above question in the negative. Our result builds on twoprimitives, local transitions in bi-partite states (based on previous work) andaverage encoding which may be of significance in other applications as well."}
1.63e+03 pattern=  0 task=  0 (u=20547)  {'docs': u'Many logic programming based approaches can be used to describe and solvecombinatorial search problems. On the one hand there is constraint logicprogramming which computes a solution as an answer substitution to a querycontaining the variables of the constraint satisfaction problem. On the otherhand there are systems based on stable model semantics, abductive systems, andfirst order logic model generators which compute solutions as models of sometheory. This paper compares these different approaches from the point of viewof knowledge representation (how declarative are the programs) and from thepoint of view of performance (how good are they at solving typical problems).'}
2.71e+03 pattern=  0 task=  0 (u=20548)  {'docs': u'This paper illustrates how a Prolog program, using chronological backtrackingto find a solution in some search space, can be enhanced to perform intelligentbacktracking. The enhancement crucially relies on the impurity of Prolog thatallows a program to store information when a dead end is reached. To illustratethe technique, a simple search program is enhanced.  To appear in Theory and Practice of Logic Programming.  Keywords: intelligent backtracking, dependency-directed backtracking,backjumping, conflict-directed backjumping, nogood sets, look-back.'}
2.38e+03 pattern=  0 task=  0 (u=20666)  {'docs': u"We prove that the exact versions of the domatic number problem are completefor the levels of the boolean hierarchy over NP. The domatic number problem,which arises in the area of computer networks, is the problem of partitioning agiven graph into a maximum number of disjoint dominating sets. This number iscalled the domatic number of the graph. We prove that the problem ofdetermining whether or not the domatic number of a given graph is {\\em exactly}one of k given values is complete for the 2k-th level of the boolean hierarchyover NP. In particular, for k = 1, it is DP-complete to determine whether ornot the domatic number of a given graph equals exactly a given integer. Notethat DP is the second level of the boolean hierarchy over NP. We obtain similarresults for the exact versions of generalized dominating set problems and ofthe conveyor flow shop problem. Our reductions apply Wagner's conditionssufficient to prove hardness for the levels of the boolean hierarchy over NP."}
1.93e+03 pattern=  9 task=  0 (u=20667)  {'docs': u'Let $M_k \\seq \\nats$ be a given set that consists of $k$ noncontiguousintegers. Define $\\exactcolor{M_k}$ to be the problem of determining whether$\\chi(G)$, the chromatic number of a given graph $G$, equals one of the $k$elements of the set $M_k$ exactly. In 1987, Wagner \\cite{wag:j:min-max} provedthat $\\exactcolor{M_k}$ is $\\bhlevel{2k}$-complete, where $M_k = \\{6k+1, 6k+3,>..., 8k-1 \\}$ and $\\bhlevel{2k}$ is the $2k$th level of the boolean hierarchyover $\\np$. In particular, for $k = 1$, it is DP-complete to determine whether$\\chi(G) = 7$, where $\\DP = \\bhlevel{2}$. Wagner raised the question of howsmall the numbers in a $k$-element set $M_k$ can be chosen such that$\\exactcolor{M_k}$ still is $\\bhlevel{2k}$-complete. In particular, for $k =1$, he asked if it is DP-complete to determine whether $\\chi(G) = 4$. In thisnote, we solve this question of Wagner and determine the precise threshold $t\\in \\{4, 5, 6, 7\\}$ for which the problem $\\exactcolor{\\{t\\}}$ jumps from NP toDP-completeness: It is DP-complete to determine whether $\\chi(G) = 4$, yet$\\exactcolor{\\{3\\}}$ is in $\\np$. More generally, for each $k \\geq 1$, we showthat $\\exactcolor{M_k}$ is $\\bhlevel{2k}$-complete for $M_k = \\{3k+1, 3k+3,...,5k-1\\}$.'}
2.03e+03 pattern=  0 task=  1 (u=20667)  {'docs': u"In 1977, Young proposed a voting scheme that extends the Condorcet Principlebased on the fewest possible number of voters whose removal yields a Condorcetwinner. We prove that both the winner and the ranking problem for Youngelections is complete for the class of problems solvable in polynomial time byparallel access to NP. Analogous results for Lewis Carroll's 1876 voting schemewere recently established by Hemaspaandra et al. In contrast, we prove that thewinner and ranking problems in Fishburn's homogeneous variant of Carroll'svoting scheme can be solved efficiently by linear programming."}
2.38e+03 pattern=  0 task=  0 (u=20672)  {'docs': u'Using a genealogically ordered infinite regular language, we know how torepresent an interval of R. Numbers having an ultimately periodicrepresentation play a special role in classical numeration systems. The aim ofthis paper is to characterize the numbers having an ultimately periodicrepresentation in generalized systems built on a regular language. Thesyntactical properties of these words are also investigated. Finally, we showthe equivalence of the classical "theta"-expansions with our generalizedrepresentations in some special case related to a Pisot number "theta".'}
2.73e+03 pattern=  0 task=  0 (u=20681)  {'docs': u'The extremal characteristics of random structures, including trees, graphs,and networks, are discussed. A statistical physics approach is employed inwhich extremal properties are obtained through suitably defined rate equations.A variety of unusual time dependences and system-size dependences for basicextremal properties are obtained.'}
2.84e+03 pattern=  0 task=  1 (u=20681)  {'docs': u'The distribution of unicyclic components in a random graph is obtainedanalytically. The number of unicyclic components of a given size approaches aself-similar form in the vicinity of the gelation transition. At the gelationpoint, this distribution decays algebraically, U_k ~ 1/(4k) for k>>1. As aresult, the total number of unicyclic components grows logarithmically with thesystem size.'}
2.2e+03 pattern=  0 task=  0 (u=20682)  {'docs': u'Approaches from statistical physics are applied to investigate the structureof network models whose growth rules mimic aspects of the evolution of theworld-wide web. We first determine the degree distribution of a growing networkin which nodes are introduced one at a time and attach to an earlier node ofdegree k with rate A_ksim k^gamma. Very different behaviors arise for gamma<1,gamma=1, and gamma>1. We also analyze the degree distribution of aheterogeneous network, the joint age-degree distribution, the correlationbetween degrees of neighboring nodes, as well as global network properties. Anextension to directed networks is then presented. By tuning model parameters toreasonable values, we obtain distinct power-law forms for the in-degree andout-degree distributions with exponents that are in good agreement with currentdata for the web. Finally, a general growth process with independentintroduction of nodes and links is investigated. This leads to independentlygrowing sub-networks that may coalesce with other sub-networks. General resultsfor both the size distribution of sub-networks and the degree distribution areobtained.'}
2.11e+03 pattern=  0 task=  0 (u=20791)  {'docs': u"We study the problem of compressing massive tables within thepartition-training paradigm introduced by Buchsbaum et al. [SODA'00], in whicha table is partitioned by an off-line training procedure into disjointintervals of columns, each of which is compressed separately by a standard,on-line compressor like gzip. We provide a new theory that unifies previousexperimental observations on partitioning and heuristic observations on columnpermutation, all of which are used to improve compression rates. Based on thetheory, we devise the first on-line training algorithms for table compression,which can be applied to individual files, not just continuously operatingsources; and also a new, off-line training algorithm, based on a link to theasymmetric traveling salesman problem, which improves on prior work byrearranging columns prior to partitioning. We demonstrate these resultsexperimentally. On various test files, the on-line algorithms provide 35-55%improvement over gzip with negligible slowdown; the off-line reorderingprovides up to 20% further improvement over partitioning alone. We also showthat a variation of the table compression problem is MAX-SNP hard."}
2.17e+03 pattern=  0 task=  0 (u=20807)  {'docs': u"Pattern-matching-based document-compression systems (e.g. for faxing) rely onfinding a small set of patterns that can be used to represent all of the ink inthe document. Finding an optimal set of patterns is NP-hard; previouscompression schemes have resorted to heuristics. This paper describes anextension of the cross-entropy approach, used previously for measuring patternsimilarity, to this problem. This approach reduces the problem to a k-mediansproblem, for which the paper gives a new algorithm with a provably goodperformance guarantee. In comparison to previous heuristics (First Fit, withand without generalized Lloyd's/k-means postprocessing steps), the newalgorithm generates a better codebook, resulting in an overall improvement incompression performance of almost 17%."}
2.56e+03 pattern=  0 task=  0 (u=21183)  {'docs': u'Dynamic Bayesian networks (DBNs) offer an elegant way to integrate variousaspects of language in one model. Many existing algorithms developed forlearning and inference in DBNs are applicable to probabilistic languagemodeling. To demonstrate the potential of DBNs for natural language processing,we employ a DBN in an information extraction task. We show how to assemblewealth of emerging linguistic instruments for shallow parsing, syntactic andsemantic tagging, morphological decomposition, named entity recognition etc. inorder to incrementally build a robust information extraction system. Our methodoutperforms previously published results on an established benchmark domain.'}
1.49e+03 pattern=  0 task=  0 (u=21260)  {'docs': u"While Kolmogorov complexity is the accepted absolute measure of informationcontent of an individual finite object, a similarly absolute notion is neededfor the relation between an individual data sample and an individual modelsummarizing the information in the data, for example, a finite set (orprobability distribution) where the data sample typically came from. Thestatistical theory based on such relations between individual objects can becalled algorithmic statistics, in contrast to classical statistical theory thatdeals with relations between probabilistic ensembles. We develop thealgorithmic theory of statistic, sufficient statistic, and minimal sufficientstatistic. This theory is based on two-part codes consisting of the code forthe statistic (the model summarizing the regularity, the meaningfulinformation, in the data) and the model-to-data code. In contrast to thesituation in probabilistic statistical theory, the algorithmic relation of(minimal) sufficiency is an absolute relation between the individual model andthe individual data sample. We distinguish implicit and explicit descriptionsof the models. We give characterizations of algorithmic (Kolmogorov) minimalsufficient statistic for all data samples for both description modes--in theexplicit mode under some constraints. We also strengthen and elaborate earlierresults on the ``Kolmogorov structure function'' and ``absolutelynon-stochastic objects''--those rare objects for which the simplest models thatsummarize their relevant information (minimal sufficient statistics) are atleast as complex as the objects themselves. We demonstrate a close relationbetween the probabilistic notions and the algorithmic ones."}
1.24e+03 pattern=  0 task=  0 (u=21471)  {'docs': u"We propose a hybrid image-space/object-space solution to the classical hiddensurface removal problem: Given n disjoint triangles in Real^3 and p samplepoints (``pixels'') in the xy-plane, determine the first triangle directlybehind each pixel. Our algorithm constructs the sampled visibility map of thetriangles with respect to the pixels, which is the subset of the trapezoids ina trapezoidal decomposition of the analytic visibility map that contain atleast one pixel. The sampled visibility map adapts to local changes in imagecomplexity, and its complexity is bounded both by the number of pixels and bythe complexity of the analytic visibility map. Our algorithm runs in timeO(n^{1+e} + n^{2/3+e}t^{2/3} + p), where t is the output size and e is anypositive constant. This is nearly optimal in the worst case and comparesfavorably with the best output-sensitive algorithms for both ray casting andanalytic hidden surface removal. In the special case where the pixels form aregular grid, a sweepline variant of our algorithm runs in time O(n^{1+e} +n^{2/3+e}t^{2/3} + t log p), which is usually sublinear in the number ofpixels."}
1.96e+03 pattern=  0 task=  1 (u=21471)  {'docs': u'The spread of a finite set of points is the ratio between the longest andshortest pairwise distances. We prove that the Delaunay triangulation of anyset of n points in R^3 with spread D has complexity O(D^3). This bound is tightin the worst case for all D = O(sqrt{n}). In particular, the Delaunaytriangulation of any dense point set has linear complexity. We also generalizethis upper bound to regular triangulations of k-ply systems of balls, unions ofseveral dense point sets, and uniform samples of smooth surfaces. On the otherhand, for any n and D=O(n), we construct a regular triangulation of complexityOmega(nD) whose n vertices have spread D.'}
2.19e+03 pattern=  0 task=  2 (u=21471)  {'docs': u'We present an algorithm to construct meshes suitable for space-timediscontinuous Galerkin finite-element methods. Our method generalizes andimproves the `Tent Pitcher\' algorithm of \\"Ung\\"or and Sheffer. Given anarbitrary simplicially meshed domain M of any dimension and a time interval[0,T], our algorithm builds a simplicial mesh of the space-time domain Mx[0,T],in constant time per element. Our algorithm avoids the limitations of previousmethods by carefully adapting the durations of space-time elements to the localquality and feature size of the underlying space mesh.'}
2.22e+03 pattern=  0 task=  3 (u=21471)  {'docs': u'We consider the problem of cutting a set of edges on a polyhedral manifoldsurface, possibly with boundary, to obtain a single topological disk,minimizing either the total number of cut edges or their total length. We showthat this problem is NP-hard, even for manifolds without boundary and forpunctured spheres. We also describe an algorithm with running time n^{O(g+k)},where n is the combinatorial complexity, g is the genus, and k is the number ofboundary components of the input surface. Finally, we describe a greedyalgorithm that outputs a O(log^2 g)-approximation of the minimum cut graph inO(g^2 n log n) time.'}
2.73e+03 pattern=  0 task=  0 (u=21485)  {'docs': u'Many different rules for decision making have been introduced in theliterature. We show that a notion of generalized expected utility proposed inPart I of this paper is a universal decision rule, in the sense that it canrepresent essentially all other decision rules.'}
2.73e+03 pattern=  0 task=  1 (u=21485)  {'docs': u"We propose a generalization of expected utility that we call generalized EU(GEU), where a decision maker's beliefs are represented by plausibilitymeasures, and the decision maker's tastes are represented by general (i.e.,notnecessarily real-valued) utility functions. We show that every agent,``rational'' or not, can be modeled as a GEU maximizer. We then show that wecan customize GEU by selectively imposing just the constraints we want. Inparticular, we show how each of Savage's postulates corresponds to constraintson GEU."}
2.72e+03 pattern=  0 task=  0 (u=21489)  {'docs': u'Suitable extensions of the monadic second-order theory of k successors havebeen proposed in the literature to capture the notion of time granularity. Inthis paper, we provide the monadic second-order theories of downward unboundedlayered structures, which are infinitely refinable structures consisting of acoarsest domain and an infinite number of finer and finer domains, and ofupward unbounded layered structures, which consist of a finest domain and aninfinite number of coarser and coarser domains, with expressively complete andelementarily decidable temporal logic counterparts.  We obtain such a result in two steps. First, we define a new class ofcombined automata, called temporalized automata, which can be proved to be theautomata-theoretic counterpart of temporalized logics, and show that relevantproperties, such as closure under Boolean operations, decidability, andexpressive equivalence with respect to temporal logics, transfer from componentautomata to temporalized ones. Then, we exploit the correspondence betweentemporalized logics and automata to reduce the task of finding the temporallogic counterparts of the given theories of time granularity to the easier oneof finding temporalized automata counterparts of them.'}
2.72e+03 pattern=  0 task=  0 (u=21491)  {'docs': u'LCG-1 is the second release of the software framework for the LHC ComputingGrid project. In our work we describe the installation process, arisingproblems and their solutions, and configuration tuning details of the completeLCG-1 site, including all LCG elements required for the self-sufficient site.'}
3.28e+03 pattern=  0 task=  0 (u=21536)  {'docs': u'The problem of extracting consistent information from relational databasesviolating integrity constraints on numerical data is addressed. In particular,aggregate constraints defined as linear inequalities on aggregate-sum querieson input data are considered. The notion of repair as consistent set of updatesat attribute-value level is exploited, and the characterization of severalcomplexity issues related to repairing data and computing consistent queryanswers is provided.'}
2.45e+03 pattern=  0 task=  0 (u=21548)  {'docs': u'Use of Peer-to-Peer (P2P) service networks introduces a new communicationparadigm because peers are both clients and servers and so each peer mayprovide/request services to/from other peers. Empirical studies of P2P networkshave been undertaken and reveal useful characteristics. However there is todate little analytical work to describe P2P networks with respect to theircommunication paradigm and their interconnections. This paper provides ananalytical formulation and optimisation of peer connection efficiency, in termsof minimising the fraction of wasted connection time. Peer connectionefficiency is analysed for both a uni- and multi-connected peer. Given thisfundamental optimisation, the paper optimises the number of connections thatpeers should make use of as a function of network load, in terms of minimisingthe total queue size that requests in the P2P network experience. The resultsof this paper provide a basis for engineering high performance P2Pinterconnection networks. The optimisations are useful for reducing bandwidthand power consumption, e.g. in the case of peers being mobile devices with alimited power supply. Also these results could be used to determine when a(virtual) circuit should be switched to support a connection.'}
1.54e+03 pattern=  0 task=  0 (u=21682)  {'docs': u'We present two new algorithms for solving the {\\em All Pairs Shortest Paths}(APSP) problem for weighted directed graphs. Both algorithms use fast matrixmultiplication algorithms.  The first algorithm solves the APSP problem for weighted directed graphs inwhich the edge weights are integers of small absolute value in $\\Ot(n^{2+\\mu})$time, where $\\mu$ satisfies the equation $\\omega(1,\\mu,1)=1+2\\mu$ and$\\omega(1,\\mu,1)$ is the exponent of the multiplication of an $n\\times n^\\mu$matrix by an $n^\\mu \\times n$ matrix. Currently, the best available bounds on$\\omega(1,\\mu,1)$, obtained by Coppersmith, imply that $\\mu<0.575$. The runningtime of our algorithm is therefore $O(n^{2.575})$. Our algorithm improves onthe $\\Ot(n^{(3+\\omega)/2})$ time algorithm, where $\\omega=\\omega(1,1,1)<2.376$is the usual exponent of matrix multiplication, obtained by Alon, Galil andMargalit, whose running time is only known to be $O(n^{2.688})$.  The second algorithm solves the APSP problem {\\em almost} exactly fordirected graphs with {\\em arbitrary} non-negative real weights. The algorithmruns in $\\Ot((n^\\omega/\\eps)\\log (W/\\eps))$ time, where $\\eps>0$ is an errorparameter and W is the largest edge weight in the graph, after the edge weightsare scaled so that the smallest non-zero edge weight in the graph is 1. Itreturns estimates of all the distances in the graph with a stretch of at most$1+\\eps$. Corresponding paths can also be found efficiently.'}
3.21e+03 pattern=  0 task=  0 (u=21730)  {'docs': u'This paper introduces a new mechanism for specifying constraints indistributed workflows. By introducing constraints in a contextual form, it isshown how different people and groups within collaborative communities cancooperatively constrain workflows. A comparison with existing state-of-the-artworkflow systems is made. These ideas are explored in practice with anillustrative example from High Energy Physics.'}
3.17e+03 pattern=  0 task=  0 (u=21849)  {'docs': u'We show how to construct an algorithm to search for binary idempotents whichmay be used to construct binary LDPC codes. The algorithm, which allows controlof the key properties of sparseness, code rate and minimum distance, isconstructed in the Mattson-Solomon domain. Some of the new codes, found byusing this technique, are displayed.'}
3.17e+03 pattern=  0 task=  0 (u=21854)  {'docs': u'Genetic algorithms (GAs) that solve hard problems quickly, reliably andaccurately are called competent GAs. When the fitness landscape of a problemchanges overtime, the problem is called non--stationary, dynamic ortime--variant problem. This paper investigates the use of competent GAs foroptimizing non--stationary optimization problems. More specifically, we use aninformation theoretic approach based on the minimum description lengthprinciple to adaptively identify regularities and substructures that can beexploited to respond quickly to changes in the environment. We also develop aspecial type of problems with bounded difficulties to test non--stationaryoptimization problems. The results provide new insights into non-stationaryoptimization problems and show that a search algorithm which automaticallyidentifies and exploits possible decompositions is more robust and respondsquickly to changes than a simple genetic algorithm.'}
3.17e+03 pattern=  0 task=  0 (u=21855)  {'docs': u'We propose a sub-structural niching method that fully exploits the problemdecomposition capability of linkage-learning methods such as the estimation ofdistribution algorithms and concentrate on maintaining diversity at thesub-structural level. The proposed method consists of three key components: (1)Problem decomposition and sub-structure identification, (2) sub-structurefitness estimation, and (3) sub-structural niche preservation. Thesub-structural niching method is compared to restricted tournament selection(RTS)--a niching method used in hierarchical Bayesian optimizationalgorithm--with special emphasis on sustained preservation of multiple globalsolutions of a class of boundedly-difficult, additively-separable multimodalproblems. The results show that sub-structural niching successfully maintainsmultiple global optima over large number of generations and does so withsignificantly less population than RTS. Additionally, the market share of eachof the niche is much closer to the expected level in sub-structural nichingwhen compared to RTS.'}
3.17e+03 pattern=  0 task=  1 (u=21855)  {'docs': u'This paper derives a population sizing relationship for genetic programming(GP). Following the population-sizing derivation for genetic algorithms inGoldberg, Deb, and Clark (1992), it considers building block decision making asa key facet. The analysis yields a GP-unique relationship because it has toaccount for bloat and for the fact that GP solutions often use subsolutionmultiple times. The population-sizing relationship depends upon tree size,solution complexity, problem difficulty and building block expressionprobability. The relationship is used to analyze and empirically investigatepopulation sizing for three model GP problems named ORDER, ON-OFF and LOUD.These problems exhibit bloat to differing extents and differ in whether theirsolutions require the use of a building block multiple times.'}
3.17e+03 pattern=  0 task=  2 (u=21855)  {'docs': u'Niching enables a genetic algorithm (GA) to maintain diversity in apopulation. It is particularly useful when the problem has multiple optimawhere the aim is to find all or as many as possible of these optima. When thefitness landscape of a problem changes overtime, the problem is callednon--stationary, dynamic or time--variant problem. In these problems, nichingcan maintain useful solutions to respond quickly, reliably and accurately to achange in the environment. In this paper, we present a niching method thatworks on the problem substructures rather than the whole solution, therefore ithas less space complexity than previously known niching mechanisms. We showthat the method is responding accurately when environmental changes occur.'}
2.94e+03 pattern=  0 task=  0 (u=21910)  {'docs': u'Unfair metrical task systems are a generalization of online metrical tasksystems. In this paper we introduce new techniques to combine algorithms forunfair metrical task systems and apply these techniques to obtain improvedrandomized online algorithms for metrical task systems on arbitrary metricspaces.'}
2.84e+03 pattern=  0 task=  0 (u=22057)  {'docs': u"Searle's Chinese Room argument is refuted by showing that he has actuallygiven two different versions of the room, which fail for different reasons.Hence, Searle does not achieve his stated goal of showing ``that a system couldhave input and output capabilities that duplicated those of a native Chinesespeaker and still not understand Chinese''."}
2.83e+03 pattern=  0 task=  0 (u=22058)  {'docs': u'Given a quadratic map Q : K^n -> K^k defined over a computable subring D of areal closed field K, and a polynomial p(Y_1,...,Y_k) of degree d, we considerthe zero set Z=Z(p(Q(X)),K^n) of the polynomial p(Q(X_1,...,X_n)). We present aprocedure that computes, in (dn)^O(k) arithmetic operations in D, a set S of(real univariate representations of) sampling points in K^n that intersectsnontrivially each connected component of Z. As soon as k=o(n), this is fasterthan the standard methods that all have exponential dependence on n in thecomplexity. In particular, our procedure is polynomial-time for constant k. Incontrast, the best previously known procedure (due to A.Barvinok) is onlycapable of deciding in n^O(k^2) operations the nonemptiness (rather thanconstructing sampling points) of the set Z in the case of p(Y)=sum_i Y_i^2 andhomogeneous Q.  A by-product of our procedure is a bound (dn)^O(k) on the number of connectedcomponents of Z.  The procedure consists of exact symbolic computations in D and outputsvectors of algebraic numbers. It involves extending K by infinitesimals andsubsequent limit computation by a novel procedure that utilizes knowledge of anexplicit isomorphism between real algebraic sets.'}
2.87e+03 pattern=  0 task=  0 (u=22205)  {'docs': u"The Eureka-147 Digital Audio Broadcasting (DAB) standard defines the 'dynamiclabels' data field for holding information about the transmission content.However, this information does not follow a well-defined structure since it isdesigned to carry text for direct output to displays, for human interpretation.This poses a problem when machine interpretation of DAB content information isdesired. Extensible Markup Language (XML) was developed to allow for thewell-defined, structured machine-to-machine exchange of data over computernetworks. This article proposes a novel technique of machine-interpretable DABcontent annotation and receiver hardware control, involving the utilisation ofXML as metadata in the transmitted DAB frames."}
2.96e+03 pattern=  0 task=  1 (u=22205)  {'docs': u'Many reconfigurable platforms require that applications be writtenspecifically to take advantage of the reconfigurable hardware. In a PC-basedenvironment, this presents an undesirable constraint in that the many alreadyavailable applications cannot leverage on such hardware. Greatest benefit canonly be derived from reconfigurable devices if even native OS applications cantransparently utilize reconfigurable devices as they would normal full-fledgedhardware devices. This paper presents how Proteus Virtual Devices are used toexpose reconfigurable hardware in a transparent manner for use by typicalnative OS applications.'}
3.09e+03 pattern=  0 task=  2 (u=22205)  {'docs': u'Computational grids typically consist of nodes utilizing ordinary processorssuch as the Intel Pentium. Field Programmable Gate Arrays (FPGAs) are able toperform certain compute-intensive tasks very well due to their inherentparallel architecture, often resulting in orders of magnitude speedups. Thispaper explores how FPGAs can be transparently exposed for remote use via gridservices, by integrating the Proteus Software Platform with the Globus Toolkit3.0.'}
2.8e+03 pattern=  0 task=  0 (u=22403)  {'docs': u'ACL2 was used to prove properties of two simplification procedures. Theprocedures differ in complexity but solve the same programming problem thatarises in the context of a resolution/paramodulation theorem proving system.Term rewriting is at the core of the two procedures, but details of therewriting procedure itself are irrelevant. The ACL2 encapsulate construct wasused to assert the existence of the rewriting function and to state some of itsproperties. Termination, irreducibility, and soundness properties wereestablished for each procedure. The availability of the encapsulation mechanismin ACL2 is considered essential to rapid and efficient verification of thiskind of algorithm.'}
1.84e+03 pattern=  0 task=  0 (u=22404)  {'docs': u'MACE is a program that searches for finite models of first-order statements.The statement to be modeled is first translated to clauses, then to relationalclauses; finally for the given domain size, the ground instances areconstructed. A Davis-Putnam-Loveland-Logeman procedure decides thepropositional problem, and any models found are translated to first-ordermodels. MACE is a useful complement to the theorem prover Otter, with Ottersearching for proofs and MACE looking for countermodels.'}
2.96e+03 pattern=  0 task=  0 (u=22407)  {'docs': u'We compare static arbitrage price bounds on basket calls, i.e. bounds thatonly involve buy-and-hold trading strategies, with the price range obtainedwithin a multi-variate generalization of the Black-Scholes model. While thereis no gap between these two sets of prices in the univariate case, we observehere that contrary to our intuition about model risk for at-the-money calls,there is a somewhat large gap between model prices and static arbitrage prices,hence a similarly large set of prices on which a multivariate Black-Scholesmodel cannot be calibrated but where no conclusion can be drawn on the presenceor not of a static arbitrage opportunity.'}
3.23e+03 pattern=  0 task=  0 (u=22513)  {'docs': u'In the present article, we explore a new approach for the study oforthomodular lattices, where we replace the problematic conjunction by a binaryoperator, called the Sasaki projection. We present a characterization oforthomodular lattices based on the use of an algebraic version of the Sasakiprojection operator (together with orthocomplementation) rather than on theconjunction. We then define of a new logic, which we call Sasaki Orthologic,which is closely related to quantum logic, and provide a rule-based definitionof this logic.'}
1.63e+03 pattern=  0 task=  0 (u=22651)  {'docs': u"In a recent paper Keister proposed two quadrature rules as alternatives toMonte Carlo for certain multidimensional integrals and reported his testresults. In earlier work we had shown that the quasi-Monte Carlo method withgeneralized Faure points is very effective for a variety of high dimensionalintegrals occurng in mathematical finance. In this paper we report test resultsof this method on Keister's examples of dimension 9 and 25, and also forexamples of dimension 60, 80 and 100. For the 25 dimensional integral weachieved accuracy of 0.01 with less than 500 points while the two methodstested by Keister used more than 220,000 points. In all of our tests, for nsample points we obtained an empirical convergence rate proportional to n^{-1}rather than the n^{-1/2} of Monte Carlo."}
3.25e+03 pattern=  0 task=  0 (u=22687)  {'docs': u'Ever since entanglement was identified as a computational and cryptographicresource, effort has been made to find an efficient way to tell whether a givendensity matrix represents an unentangled, or separable, state. Essentially,this is the quantum separability problem.  Chapters 1 to 3 motivate a new interior-point algorithm which, given theexpected values of a subset of an orthogonal basis of observables of anotherwise unknown quantum state, searches for an entanglement witness in thespan of the subset of observables. When all the expected values are known, thealgorithm solves the separability problem. In Chapter 4, I give the motivationfor the algorithm and show how it can be used in a particular physical scenarioto detect entanglement (or decide separability) of an unknown quantum stateusing as few quantum resources as possible. I then explain the intuitive ideabehind the algorithm and relate it to the standard algorithms of its kind. Iend the chapter with a comparison of the complexities of the algorithmssurveyed in Chapter 3. Finally, in Chapter 5, I present the details of thealgorithm and discuss its performance relative to standard methods.'}
2.94e+03 pattern=  0 task=  0 (u=22726)  {'docs': u'An abstract framework of canonical inference is used to explore how differentproof orderings induce different variants of saturation and completeness.Notions like completion, paramodulation, saturation, redundancy elimination,and rewrite-system reduction are connected to proof orderings. Fairness ofdeductive mechanisms is defined in terms of proof orderings, distinguishingbetween (ordinary) "fairness," which yields completeness, and "uniformfairness," which yields saturation.'}
1.55e+03 pattern=  0 task=  0 (u=22966)  {'docs': u'The following critical phenomenon was recently discovered. When a memorylesssource is compressed using a variable-length fixed-distortion code, the fastestconvergence rate of the (pointwise) compression ratio to the optimal $R(D)$bits/symbol is either $O(\\sqrt{n})$ or $O(\\log n)$. We show it is always$O(\\sqrt{n})$, except for discrete, uniformly distributed sources.'}
1.23e+03 pattern=  0 task=  0 (u=22967)  {'docs': u"Suppose A is a finite set equipped with a probability measure P and let M bea ``mass'' function on A. We give a probabilistic characterization of the mostefficient way in which A^n can be almost-covered using spheres of a fixedradius. An almost-covering is a subset C_n of A^n, such that the union of thespheres centered at the points of C_n has probability close to one with respectto the product measure P^n. An efficient covering is one with small massM^n(C_n); n is typically large. With different choices for M and the geometryon A our results give various corollaries as special cases, including Shannon'sdata compression theorem, a version of Stein's lemma (in hypothesis testing),and a new converse to some measure concentration inequalities on discretespaces. Under mild conditions, we generalize our results to abstract spaces andnon-product measures."}
2.84e+03 pattern=  0 task=  0 (u=22992)  {'docs': u'The theory of computational complexity is used to underpin a recent model ofneocortical sensory processing. We argue that encoding into reconstructionnetworks is appealing for communicating agents using Hebbian learning andworking on hard combinatorial problems, which are easy to verify. Computationaldefinition of the concept of intelligence is provided. Simulations illustratethe idea.'}
2.01e+03 pattern=  0 task=  0 (u=23025)  {'docs': u"We study a class of program schemes, NPSB, in which, aside from basicassignments, non-deterministic guessing and while loops, we have access toarrays; but where these arrays are binary write-once in that they areinitialized to `zero' and can only ever be set to `one'. We show, amongst otherresults, that: NPSB can be realized as a vectorized Lindstrom logic; there areproblems accepted by program schemes of NPSB that are not definable in thebounded-variable infinitary logic ${\\cal L}^\\omega_{\\infty\\omega}$; allproblems accepted by the program schemes of NPSB have a zero-one law; and onordered structures, NPSB captures the complexity class $[ L]^[{\\scriptsizeNP\\normalsize}]$. The class of program schemes NPSB is actually the union of aninfinite hierarchy of classes of program schemes. When we amend the semanticsof our program schemes slightly, we find that the classes of the resultinghierarchy capture the complexity classes $\\Sigma^p_i$ (where $i\\geq 1$) of thePolynomial Hierarchy PH. Finally, we give logical equivalences of thecomplexity-theoretic question `Does NP equal PSPACE?' where the logics (andclasses of program schemes) involved define only problems with zero-one laws(and so do not define some computationally trivial problems)."}
2.17e+03 pattern=  0 task=  0 (u=23031)  {'docs': u'Given matrices A and B and vectors a, b, c and d, all with non-negativeentries, we consider the problem of computing min {c.x: x in Z^n_+, Ax > a, Bx< b, x < d}. We give a bicriteria-approximation algorithm that, given epsilonin (0, 1], finds a solution of cost O(ln(m)/epsilon^2) times optimal, meetingthe covering constraints (Ax > a) and multiplicity constraints (x < d), andsatisfying Bx < (1 + epsilon)b + beta, where beta is the vector of row sumsbeta_i = sum_j B_ij. Here m denotes the number of rows of A.  This gives an O(ln m)-approximation algorithm for CIP -- minimum-costcovering integer programs with multiplicity constraints, i.e., the special casewhen there are no packing constraints Bx < b. The previous best approximationratio has been O(ln(max_j sum_i A_ij)) since 1982. CIP contains the set coverproblem as a special case, so O(ln m)-approximation is the best possible unlessP=NP.'}
3.14e+03 pattern=  0 task=  0 (u=23190)  {'docs': u'Histograms are used to summarize the contents of relations into a number ofbuckets for the estimation of query result sizes. Several techniques (e.g.,MaxDiff and V-Optimal) have been proposed in the past for determining bucketboundaries which provide accurate estimations. However, while search strategiesfor optimal bucket boundaries are rather sophisticated, no much attention hasbeen paid for estimating queries inside buckets and all of the above techniquesadopt naive methods for such an estimation. This paper focuses on the problemof improving the estimation inside a bucket once its boundaries have beenfixed. The proposed technique is based on the addition, to each bucket, of32-bit additional information (organized into a 4-level tree index), storingapproximate cumulative frequencies at 7 internal intervals of the bucket. Boththeoretical analysis and experimental results show that, among a number ofalternative ways to organize the additional information, the 4-level tree indexprovides the best frequency estimation inside a bucket. The index is lateradded to two well-known histograms, MaxDiff and V-Optimal, obtaining thenon-obvious result that despite the spatial cost of 4LT which reduces thenumber of allowed buckets once the storage space has been fixed, the originalmethods are strongly improved in terms of accuracy.'}
3.15e+03 pattern=  0 task=  1 (u=23190)  {'docs': u"The problem of recovering (count and sum) range queries over multidimensionaldata only on the basis of aggregate information on such data is addressed. Thisproblem can be formalized as follows. Suppose that a transformation T producinga summary from a multidimensional data set is used. Now, given a data set D, asummary S=T(D) and a range query r on D, the problem consists of studying r bymodelling it as a random variable defined over the sample space of all the datasets D' such that T(D) = S. The study of such a random variable, done by thedefinition of its probability distribution and the computation of its meanvalue and variance, represents a well-founded, theoretical probabilisticapproach for estimating the query only on the basis of the availableinformation (that is the summary S) without assumptions on original data."}
3.14e+03 pattern=  0 task=  0 (u=23199)  {'docs': u'During the last two years the RealityGrid project has allowed us to be one ofthe few scientific groups involved in the development of computational grids.Since smoothly working production grids are not yet available, we have beenable to substantially influence the direction of software development and griddeployment within the project. In this paper we review our results from largescale three-dimensional lattice Boltzmann simulations performed over the lasttwo years. We describe how the proactive use of computational steering andadvanced job migration and visualization techniques enabled us to do ourscientific work more efficiently. The projects reported on in this paper arestudies of complex fluid flows under shear or in porous media, as well aslarge-scale parameter searches, and studies of the self-organisation of liquidcubic mesophases.  Movies are available athttp://www.ica1.uni-stuttgart.de/~jens/pub/05/05-PhilTransReview.html'}
2.12e+03 pattern=  0 task=  0 (u=23203)  {'docs': u'The general stable quantum memory unit is a hybrid consisting of a classicaldigit with a quantum digit (qudit) assigned to each classical state. The shapeof the memory is the vector of sizes of these qudits, which may differ. Wedetermine when N copies of a quantum memory A embed in N(1+o(1)) copies ofanother quantum memory B. This relationship captures the notion that B is as atleast as useful as A for all purposes in the bulk limit. We show that theembeddings exist if and only if for all p >= 1, the p-norm of the shape of Adoes not exceed the p-norm of the shape of B. The log of the p-norm of theshape of A can be interpreted as the maximum of S(\\rho) + H(\\rho)/p (quantumentropy plus discounted classical entropy) taken over all mixed states \\rho onA. We also establish a noiseless coding theorem that justifies these entropies.The noiseless coding theorem and the bulk embedding theorem together say thateither A blindly bulk-encodes into B with perfect fidelity, or A admits a statethat does not visibly bulk-encode into B with high fidelity.  In conclusion, the utility of a hybrid quantum memory is determined by itssimultaneous capacity for classical and quantum entropy, which is not a finitelist of numbers, but rather a convex region in the classical-quantum entropyplane.'}
3.28e+03 pattern=  0 task=  0 (u=23646)  {'docs': u'Discovering patterns from data is an important task in data mining. Thereexist techniques to find large collections of many kinds of patterns from datavery efficiently. A collection of patterns can be regarded as a summary of thedata. A major difficulty with patterns is that pattern collections summarizingthe data well are often very large.  In this dissertation we describe methods for summarizing pattern collectionsin order to make them also more understandable. More specifically, we focus onthe following themes: 1) Quality value simplifications. 2) Pattern orderings.3) Pattern chains and antichains. 4) Change profiles. 5) Inverse patterndiscovery.'}
2.42e+03 pattern=  0 task=  0 (u=23673)  {'docs': u"We prove lower bounds of order $n\\log n$ for both the problem to multiplypolynomials of degree $n$, and to divide polynomials with remainder, in themodel of bounded coefficient arithmetic circuits over the complex numbers.These lower bounds are optimal up to order of magnitude. The proof uses arecent idea of R. Raz [Proc. 34th STOC 2002] proposed for matrixmultiplication. It reduces the linear problem to multiply a random circulantmatrix with a vector to the bilinear problem of cyclic convolution. We treatthe arising linear problem by extending J. Morgenstern's bound [J. ACM 20, pp.305-306, 1973] in a unitarily invariant way. This establishes a new lower boundon the bounded coefficient complexity of linear forms in terms of the singularvalues of the corresponding matrix. In addition, we extend these lower boundsfor linear and bilinear maps to a model of circuits that allows a restrictednumber of unbounded scalar multiplications."}
3.17e+03 pattern=  0 task=  1 (u=23673)  {'docs': u'We continue the study of counting complexity begun in [Buergisser, Cucker 04]and [Buergisser, Cucker, Lotz 05] by proving upper and lower bounds on thecomplexity of computing the Hilbert polynomial of a homogeneous ideal. We showthat the problem of computing the Hilbert polynomial of a smoothequidimensional complex projective variety can be reduced in polynomial time tothe problem of counting the number of complex common zeros of a finite set ofmultivariate polynomials. Moreover, we prove that the more general problem ofcomputing the Hilbert polynomial of a homogeneous ideal is polynomial spacehard. This implies polynomial space lower bounds for both the problems ofcomputing the rank and the Euler characteristic of cohomology groups ofcoherent sheaves on projective space, improving the #P-lower bound of Bach (JSC1999).'}
2.46e+03 pattern=  0 task=  0 (u=23745)  {'docs': u"This note is about the relationship between two theories of negation asfailure -- one based on program completion, the other based on stable models,or answer sets. Francois Fages showed that if a logic program satisfies acertain syntactic condition, which is now called ``tightness,'' then its stablemodels can be characterized as the models of its completion. We extend thedefinition of tightness and Fages' theorem to programs with nested expressionsin the bodies of rules, and study tight logic programs containing thedefinition of the transitive closure of a predicate."}
1.93e+03 pattern=  0 task=  0 (u=23760)  {'docs': u'Allowing users to interact through language borders is an interestingchallenge for information technology. For the purpose of a computer assistedlanguage learning system, we have chosen icons for representing meaning on theinput interface, since icons do not depend on a particular language. However, akey limitation of this type of communication is the expression of articulatedideas instead of isolated concepts. We propose a method to interpret sequencesof icons as complex messages by reconstructing the relations between concepts,so as to build conceptual graphs able to represent meaning and to be used fornatural language sentence generation. This method is based on an electronicdictionary containing semantic information.'}
1.53e+03 pattern=  0 task=  0 (u=24030)  {'docs': u'Network firewalls and routers use a rule database to decide which packetswill be allowed from one network onto another. By filtering packets thefirewalls and routers can improve security and performance. However, as thesize of the rule list increases, it becomes difficult to maintain and validatethe rules, and lookup latency may increase significantly. Ordered binarydecision diagrams (BDDs) - a compact method of representing and manipulatingboolean expressions - are a potential method of representing the rules. Thispaper presents a new algorithm for representing such lists as a BDD and thenshows how the resulting boolean expression can be used to analyse rule sets.'}
1.95e+03 pattern=  0 task=  1 (u=24030)  {'docs': u'The use of IP filtering to improve system security is well established, andalthough limited in what it can achieve has proved to be efficient andeffective.  In the design of a security policy there is always a trade-off betweenusability and security. Restricting access means that legitimate use of thenetwork is prevented; allowing access means illegitimate use may be allowed.Static access list make finding a balance particularly stark -- we pay theprice of decreased security 100% of the time even if the benefit of increasedusability is only gained 1% of the time.  Dynamic access lists would allow the rules to change for short periods oftime, and to allow local changes by non-experts. The network administrator canset basic security guide-lines which allow certain basic services only. Allother services are restricted, but users are able to request temporaryexceptions in order to allow additional access to the network. These exceptionsare granted depending on the privileges of the user.  This paper covers the following topics: (1) basic introduction to TCP/IPfiltering; (2) semantics for dynamic access lists and; (3) a proposed protocolfor allowing dynamic access; and (4) a method for representing access lists sothat dynamic update and look-up can be done efficiently performed.'}
1.95e+03 pattern=  0 task=  0 (u=24043)  {'docs': u'We consider pushdown timed automata (PTAs) that are timed automata (withdense clocks) augmented with a pushdown stack. A configuration of a PTAincludes a control state, dense clock values and a stack word. By using thepattern technique, we give a decidable characterization of the binaryreachability (i.e., the set of all pairs of configurations such that one canreach the other) of a PTA. Since a timed automaton can be treated as a PTAwithout the pushdown stack, we can show that the binary reachability of a timedautomaton is definable in the additive theory of reals and integers. Theresults can be used to verify a class of properties containing linear relationsover both dense variables and unbounded discrete variables. The propertiespreviously could not be verified using the classic region technique norexpressed by timed temporal logics for timed automata and CTL$^*$ for pushdownsystems. The results are also extended to other generalizations of timedautomata.'}
2.68e+03 pattern=  0 task=  0 (u=24076)  {'docs': u'Due to the increased complexity of parallel and distributed programs,debugging of them is considered to be the most difficult and time consumingpart of the software lifecycle. Tool support is hence a crucial necessity tohide complexity from the user. However, most existing tools seem inadequate assoon as the program under consideration exploits more than a few processorsover a long execution time. This problem is addressed by the novel debuggingtool DeWiz (Debugging Wizard), whose focus lies on scalability. DeWiz has amodular, scalable architecture, and uses the event graph model as arepresentation of the investigated program. DeWiz provides a set of modules,which can be combined to generate, analyze, and visualize event graph data.Within this processing pipeline the toolset tries to extract usefulinformation, which is presented to the user at an arbitrary level ofabstraction. Additionally, DeWiz is a framework, which can be used to easilyimplement arbitrary user-defined modules.'}
2.68e+03 pattern=  0 task=  0 (u=24079)  {'docs': u'We apply the optimization algorithm Adaptive Simulated Annealing (ASA) to theproblem of analyzing data on a large population and selecting the best model topredict that an individual with various traits will have a particular disease.We compare ASA with traditional forward and backward regression on computersimulated data. We find that the traditional methods of modeling are better forsmaller data sets whereas a numerically stable ASA seems to perform better onlarger and more complicated data sets.'}
2.74e+03 pattern=  0 task=  0 (u=24114)  {'docs': u'Soft linear logic ([Lafont02]) is a subsystem of linear logic characterizingthe class PTIME. We introduce Soft lambda-calculus as a calculus typable in theintuitionistic and affine variant of this logic. We prove that the (untyped)terms of this calculus are reducible in polynomial time. We then extend thetype system of Soft logic with recursive types. This allows us to considernon-standard types for representing lists. Using these datatypes we examine theconcrete expressivity of Soft lambda-calculus with the example of the insertionsort algorithm.'}
2.83e+03 pattern=  0 task=  0 (u=24129)  {'docs': u'The main problem about replacing LTP as a memory mechanism has been to findother highly abstract, easily understandable principles for induced plasticity.In this paper we attempt to lay out such a basic mechanism, namely intrinsicplasticity. Important empirical observations with theoretical significance aretime-layering of neural plasticity mediated by additional constraints to enterinto later stages, various manifestations of intrinsic neural properties, andconditional gating of synaptic connections. An important consequence of theproposed mechanism is that it can explain the usually latent nature ofmemories.'}
2.16e+03 pattern=  0 task=  0 (u=24253)  {'docs': u'The expected number of pairwise comparisons needed to learn a partial orderon n elements is shown to be at least n*n/4-o(n*n), and an algorithm is giventhat needs only n*n/4+o(n*n) comparisons on average. In addition, the optimalstrategy for learning a poset with four elements is presented.'}
1.58e+03 pattern=  0 task=  0 (u=24393)  {'docs': u"We describe a model that enables us to analyze the running time of analgorithm in a computer with a memory hierarchy with limited associativity, interms of various cache parameters. Our model, an extension of Aggarwal andVitter's I/O model, enables us to establish useful relationships between thecache complexity and the I/O complexity of computations. As a corollary, weobtain cache-optimal algorithms for some fundamental problems like sorting,FFT, and an important subclass of permutations in the single-level cache model.We also show that ignoring associativity concerns could lead to inferiorperformance, by analyzing the average-case cache behavior of mergesort. Wefurther extend our model to multiple levels of cache with limited associativityand present optimal algorithms for matrix transpose and sorting. Our techniquesmay be used for systematic exploitation of the memory hierarchy starting fromthe algorithm design stage, and dealing with the hitherto unresolved problem oflimited associativity."}
2.8e+03 pattern=  0 task=  0 (u=24555)  {'docs': u'Disjunctive Linear Arithmetic (DLA) is a major decidable theory that issupported by almost all existing theorem provers. The theory consists ofBoolean combinations of predicates of the form $\\Sigma_{j=1}^{n}a_j\\cdot x_j\\le b$, where the coefficients $a_j$, the bound $b$ and the variables $x_1 >...x_n$ are of type Real ($\\mathbb{R}$). We show a reduction to propositionallogic from disjunctive linear arithmetic based on Fourier-Motzkin elimination.While the complexity of this procedure is not better than competing techniques,it has practical advantages in solving verification problems. It also promotesthe option of deciding a combination of theories by reducing them to thislogic. Results from experiments show that this method has a strong advantageover existing techniques when there are many disjunctions in the formula.'}
3.07e+03 pattern=  0 task=  0 (u=24579)  {'docs': u'I have plotted an image by using mathematical functions in the Database "4thDimension". I\'m going to show an alternative method to: detect which sector hasbeen clicked; highlight it and combine it with other sectors alreadyhighlighted; store the graph information in an efficient way; load and splatimage layers to reconstruct the stored graph.'}
2.51e+03 pattern=  0 task=  0 (u=24794)  {'docs': u"In this paper we discuss a genetic version (GWA) of the Whitehead'salgorithm, which is one of the basic algorithms in combinatorial group theory.It turns out that GWA is surprisingly fast and outperforms the standardWhitehead's algorithm in free groups of rank >= 5. Experimenting with GWA wecollected an interesting numerical data that clarifies the time-complexity ofthe Whitehead's Problem in general. These experiments led us to severalmathematical conjectures. If confirmed they will shed light on hiddenmechanisms of Whitehead Method and geometry of automorphic orbits in freegroups."}
2.51e+03 pattern=  0 task=  1 (u=24794)  {'docs': u'The Andrews-Curtis conjecture states that every balanced presentation of thetrivial group can be reduced to the standard one by a sequence of theelementary Nielsen transformations and conjugations. In this paper we describeall balanced presentations of the trivial group on two generators and with thetotal length of relators <= 12. We show that all these presentations satisfythe Andrews-Curtis conjecture.'}
3.09e+03 pattern=  0 task=  0 (u=24911)  {'docs': u'Internet evolves and operates largely without a central coordination, thelack of which was and is critically important to the rapid growth and evolutionof Internet. However, the lack of management in turn makes it very difficult toguarantee proper performance and to deal systematically with performanceproblems. Meanwhile, the available network bandwidth and server capacitycontinue to be overwhelmed by the skyrocketing Internet utilization and theaccelerating growth of bandwidth intensive content. As a result, Internetservice quality perceived by customers is largely unpredictable andunsatisfactory. Content Distribution Network (CDN) is an effective approach toimprove Internet service quality. CDN replicates the content from the place oforigin to the replica servers scattered over the Internet and serves a requestfrom a replica server close to where the request originates. In this paper, wefirst give an overview about CDN. We then present the critical issues involvedin designing and implementing an effective CDN and survey the approachesproposed in literature to address these problems. An example of CDN isdescribed to show how a real commercial CDN operates. After this, we present ascheme that provides fast service location for peer-to-peer systems, a specialtype of CDN with no infrastructure support. We conclude with a brief projectionabout CDN.'}
2.33e+03 pattern=  0 task=  0 (u=24914)  {'docs': u'From some observations on economic behaviors, in particular changing economicconditions with time and space, we develop a very simple model for theevolution of economic entities within a geographical type of framework. Weraise a few questions and attempt to investigate whether some of them can betackled by our model. Several cases of interest are reported. It is found thatthe model even in its simple forms can lead to a large variety of situations,including: delocalization and cycles, but also pre-chaotic behavior.'}
2.43e+03 pattern=  0 task=  0 (u=24925)  {'docs': u"We show how to determine the $k$-th bit of Chaitin's algorithmically randomreal number $\\Omega$ by solving $k$ instances of the halting problem. From thiswe then reduce the problem of determining the $k$-th bit of $\\Omega$ todetermining whether a certain Diophantine equation with two parameters, $k$ and$N$, has solutions for an odd or an even number of values of $N$. We alsodemonstrate two further examples of $\\Omega$ in number theory: an exponentialDiophantine equation with a parameter $k$ which has an odd number of solutionsiff the $k$-th bit of $\\Omega$ is 1, and a polynomial of positive integervariables and a parameter $k$ that takes on an odd number of positive valuesiff the $k$-th bit of $\\Omega$ is 1."}
2.79e+03 pattern=  0 task=  1 (u=24925)  {'docs': u'While it is well known that a Turing machine equipped with the ability toflip a fair coin cannot compute more that a standard Turing machine, we showthat this is not true for a biased coin. Indeed, any oracle set $X$ may becoded as a probability $p_{X}$ such that if a Turing machine is given a coinwhich lands heads with probability $p_{X}$ it can compute any functionrecursive in $X$ with arbitrarily high probability. We also show how theassumption of a non-recursive bias can be weakened by using a sequence ofincreasingly accurate recursive biases or by choosing the bias at random from adistribution with a non-recursive mean. We conclude by briefly mentioning someimplications regarding the physical realisability of such methods.'}
1.22e+03 pattern=  0 task=  0 (u=24939)  {'docs': u"In this article, we study parameterized complexity theory from theperspective of logic, or more specifically, descriptive complexity theory.  We propose to consider parameterized model-checking problems for variousfragments of first-order logic as generic parameterized problems and show howthis approach can be useful in studying both fixed-parameter tractability andintractability. For example, we establish the equivalence between themodel-checking for existential first-order logic, the homomorphism problem forrelational structures, and the substructure isomorphism problem. Our maintractability result shows that model-checking for first-order formulas isfixed-parameter tractable when restricted to a class of input structures withan excluded minor. On the intractability side, for every t >= 0 we prove anequivalence between model-checking for first-order formulas with t quantifieralternations and the parameterized halting problem for alternating Turingmachines with t alternations. We discuss the close connection between thisalternation hierarchy and Downey and Fellows' W-hierarchy.  On a more abstract level, we consider two forms of definability, called Fagindefinability and slicewise definability, that are appropriate for describingparameterized problems. We give a characterization of the class FPT of allfixed-parameter tractable problems in terms of slicewise definability in finitevariable least fixed-point logic, which is reminiscent of the Immerman-VardiTheorem characterizing the class PTIME in terms of definability in leastfixed-point logic."}
3.17e+03 pattern=  0 task=  1 (u=24939)  {'docs': u"Most parameterized complexity classes are defined in terms of a parameterizedversion of the Boolean satisfiability problem (the so-called weightedsatisfiability problem). For example, Downey and Fellow's W-hierarchy is ofthis form. But there are also classes, for example, the A-hierarchy, that aremore naturally characterised in terms of model-checking problems for certainfragments of first-order logic.  Downey, Fellows, and Regan were the first to establish a connection betweenthe two formalisms by giving a characterisation of the W-hierarchy in terms offirst-order model-checking problems. We improve their result and then prove asimilar correspondence between weighted satisfiability and model-checkingproblems for the A-hierarchy and the W^*-hierarchy. Thus we obtain very uniformcharacterisations of many of the most important parameterized complexityclasses in both formalisms.  Our results can be used to give new, simple proofs of some of the coreresults of structural parameterized complexity theory."}
1.95e+03 pattern=  0 task=  0 (u=25163)  {'docs': u'Large organizations today are being served by different types of dataprocessing and informations systems, ranging from the operational (OLTP)systems, data warehouse systems, to data mining and business intelligenceapplications. It is important to create an integrated repository of what thesesystems contain and do in order to use them collectively and effectively. Therepository contains metadata of source systems, data warehouse, and also thebusiness metadata. Decision support and business analysis require extensive andin-depth understanding of business entities, tasks, rules and the environment.The purpose of business metadata is to provide this understanding. Realizingthe importance of metadata, many standardization efforts has been initiated todefine metadata models. In trying to define an integrated metadata andinformation systems for a banking application, we discover some importantlimitations or inadequacies of the business metadata proposals. They relate toproviding an integrated and flexible inter-operability and navigation betweenmetadata and data, and to the important issue of systematically handlingtemporal characteristics and evolution of the metadata itself.  In this paper, we study the issue of structuring business metadata so that itcan provide a context for business management and decision support whenintegrated with data warehousing. We define temporal object-oriented businessmetadata model, and relate it both to the technical metadata and the datawarehouse. We also define ways of accessing and navigating metadata inconjunction with data.'}
1.96e+03 pattern=  0 task=  0 (u=25164)  {'docs': u'We propose pretty simple password-authenticated key-exchange protocol whichis based on the difficulty of solving DDH problem. It has the followingadvantages: (1) Both $y_1$ and $y_2$ in our protocol are independent and thusthey can be pre-computed and can be sent independently. This speeds up theprotocol. (2) Clients and servers can use almost the same algorithm. Thisreduces the implementation costs without accepting replay attacks and abuse ofentities as oracles.'}
2.91e+03 pattern=  0 task=  0 (u=25206)  {'docs': u'Higher-order representations of objects such as programs, proofs, formulasand types have become important to many symbolic computation tasks. Systemsthat support such representations usually depend on the implementation of anintensional view of the terms of some variant of the typed lambda-calculus.Various notations have been proposed for lambda-terms to explicitly treatsubstitutions as basis for realizing such implementations. There are, however,several choices in the actual reduction strategies. The most common strategyutilizes such notations only implicitly via an incremental use of environments.This approach does not allow the smaller substitution steps to be intermingledwith other operations of interest on lambda-terms. However, a naive strategyexplicitly using such notations can also be costly: each use of thesubstitution propagation rules causes the creation of a new structure on theheap that is often discarded in the immediately following step. There is thus atradeoff between these two approaches. This thesis describes the actualrealization of the two approaches, discusses their tradeoffs based on this and,finally, offers an amalgamated approach that utilizes recursion in rewrite ruleapplication but also suspends substitution operations where necessary.'}
1.8e+03 pattern=  0 task=  0 (u=25434)  {'docs': u'How did we get from a world where cookies were something you ate and where"non-techies" were unaware of "Netscape cookies" to a world where cookies are ahot-button privacy issue for many computer users? This paper will describe howHTTP "cookies" work, and how Netscape\'s original specification evolved into anIETF Proposed Standard. I will also offer a personal perspective on how whatbegan as a straightforward technical specification turned into a politicalflashpoint when it tried to address non-technical issues such as privacy.'}
1.49e+03 pattern=  0 task=  0 (u=25552)  {'docs': u"With the advent of wide security platforms able to express simultaneously allthe policies comprising an organization's global security policy, the problemof inconsistencies within security policies become harder and more relevant.  We have defined a tool based on the CHR language which is able to detectseveral types of inconsistencies within and between security policies and otherspecifications, namely workflow specifications.  Although the problem of security conflicts has been addressed by severalauthors, to our knowledge none has addressed the general problem of securityinconsistencies, on its several definitions and target specifications."}
1.49e+03 pattern=  0 task=  0 (u=25556)  {'docs': u'We consider worst case time bounds for NP-complete problems including 3-SAT,3-coloring, 3-edge-coloring, and 3-list-coloring. Our algorithms are based on aconstraint satisfaction (CSP) formulation of these problems. 3-SAT isequivalent to (2,3)-CSP while the other problems above are special cases of(3,2)-CSP; there is also a natural duality transformation from (a,b)-CSP to(b,a)-CSP. We give a fast algorithm for (3,2)-CSP and use it to improve thetime bounds for solving the other problems listed above. Our techniques involvea mixture of Davis-Putnam-style backtracking with more sophisticated matchingand network flow based ideas.'}
1.73e+03 pattern=  0 task=  0 (u=25605)  {'docs': u'In this paper we investigate the theoretical foundation of a new bottom-upsemantics for linear logic programs, and more precisely for the fragment ofLinLog that consists of the language LO enriched with the constant 1. We useconstraints to symbolically and finitely represent possibly infinitecollections of provable goals. We define a fixpoint semantics based on a newoperator in the style of Tp working over constraints. An application of thefixpoint operator can be computed algorithmically. As sufficient conditions fortermination, we show that the fixpoint computation is guaranteed to convergefor propositional LO. To our knowledge, this is the first attempt to define aneffective fixpoint semantics for linear logic programs. As an application ofour framework, we also present a formal investigation of the relations betweenLO and Disjunctive Logic Programming. Using an approach based on abstractinterpretation, we show that DLP fixpoint semantics can be viewed as anabstraction of our semantics for LO. We prove that the resulting abstraction iscorrect and complete for an interesting class of LO programs encoding PetriNets.'}
2.74e+03 pattern=  0 task=  0 (u=25888)  {'docs': u"We approach the virtual reality phenomenon by studying its relationship toset theory, and we investigate the case where this is done using thewellfoundedness property of sets. Our hypothesis is that non-wellfounded sets(hypersets) give rise to a different quality of virtual reality than dofamiliar wellfounded sets. We initially provide an alternative approach tovirtual reality based on Sommerhoff's idea of first and second orderself-awareness; both categories of self-awareness are considered as necessaryconditions for consciousness in terms of higher cognitive functions. We thenintroduce a representation of first and second order self-awareness throughsets, and assume that these sets, which we call events, originally form acollection of wellfounded sets. Strong virtual reality characterizes virtualreality environments which have the limited capacity to create only eventsassociated with wellfounded sets. In contrast, the more general concept of weakvirtual reality characterizes collections of virtual reality mediated eventsaltogether forming an entirety larger than any collection of wellfounded sets.By giving reference to Aczel's hyperset theory we indicate that this definitionis not empty, because hypersets encompass wellfounded sets already. Moreover,we argue that weak virtual reality could be realized in human history throughcontinued progress in computer technology. Finally, we reformulate ourcharacterization into a more general framework, and use Baltag's StructuralTheory of Sets (STS) to show that within this general hyperset theorySommerhoff's first and second order self-awareness as well as both concepts ofvirtual reality admit a consistent mathematical representation."}
3.11e+03 pattern=  0 task=  0 (u=25958)  {'docs': u'In this paper, we consider the hidden subgroup problem (HSP) over the classof semi-direct product groups $\\mathbb{Z}_{p^r}\\rtimes\\mathbb{Z}_q$, for p andq prime. We first present a classification of these groups in five classes.Then, we describe a polynomial-time quantum algorithm solving the HSP over allthe groups of one of these classes: the groups of the form$\\mathbb{Z}_{p^r}\\rtimes\\mathbb{Z}_p$, where p is an odd prime. Our algorithmworks even in the most general case where the group is presented as a black-boxgroup with not necessarily unique encoding. Finally, we extend this result andpresent an efficient algorithm solving the HSP over the groups$\\mathbb{Z}^m_{p^r}\\rtimes\\mathbb{Z}_p$.'}
1.92e+03 pattern=  0 task=  0 (u=25975)  {'docs': u'We argue in this paper that many common adverbial phrases generally taken tosignal a discourse relation between syntactically connected units withindiscourse structure, instead work anaphorically to contribute relationalmeaning, with only indirect dependence on discourse structure. This allows asimpler discourse structure to provide scaffolding for compositional semantics,and reveals multiple ways in which the relational meaning conveyed by adverbialconnectives can interact with that associated with discourse structure. Weconclude by sketching out a lexicalised grammar for discourse that facilitatesdiscourse interpretation as a product of compositional rules, anaphorresolution and inference.'}
2.6e+03 pattern=  0 task=  0 (u=26081)  {'docs': u'We analyse the issues involved in the management and mining of astrophysicaldata. The traditional approach to data management in the astrophysical field isnot able to keep up with the increasing size of the data gathered by moderndetectors. An essential role in the astrophysical research will be assumed byautomatic tools for information extraction from large datasets, i.e. datamining techniques, such as clustering and classification algorithms. This asksfor an approach to data management based on data warehousing, emphasizing theefficiency and simplicity of data access; efficiency is obtained usingmultidimensional access methods and simplicity is achieved by properly handlingmetadata. Clustering and classification techniques, on large datasets, poseadditional requirements: computational and memory scalability with respect tothe data size, interpretability and objectivity of clustering or classificationresults. In this study we address some possible solutions.'}
2.31e+03 pattern=  0 task=  0 (u=26083)  {'docs': u'Data-sharing scientific collaborations have particular characteristics,potentially different from the current peer-to-peer environments. In this paperwe advocate the benefits of exploiting emergent patterns in self-configuringnetworks specialized for scientific data-sharing collaborations. We speculatethat a peer-to-peer scientific collaboration network will exhibit small-worldtopology, as do a large number of social networks for which the same patternhas been documented. We propose a solution for locating data in decentralized,scientific, data-sharing environments that exploits the small-worlds topology.The research challenge we raise is: what protocols should be used to allow aself-configuring peer-to-peer network to form small worlds similar to the wayin which the humans that use the network do in their social interactions?'}
2.6e+03 pattern=  0 task=  1 (u=26083)  {'docs': u"Web caches, content distribution networks, peer-to-peer file sharingnetworks, distributed file systems, and data grids all have in common that theyinvolve a community of users who generate requests for shared data. In eachcase, overall system performance can be improved significantly if we can firstidentify and then exploit interesting structure within a community's accesspatterns. To this end, we propose a novel perspective on file sharing based onthe study of the relationships that form among users based on the files inwhich they are interested.  We propose a new structure that captures common user interests in data--thedata-sharing graph-- and justify its utility with studies on threedata-distribution systems: a high-energy physics collaboration, the Web, andthe Kazaa peer-to-peer network. We find small-world patterns in thedata-sharing graphs of all three communities. We analyze these graphs andpropose some probable causes for these emergent small-world patterns. Thesignificance of small-world patterns is twofold: it provides a rigorous supportto intuition and, perhaps most importantly, it suggests ways to designmechanisms that exploit these naturally emerging patterns."}
2.76e+03 pattern=  0 task=  0 (u=26100)  {'docs': u"We compare two recent extensions of the answer set (stable model) semanticsof logic programs. One of them, due to Lifschitz, Tang and Turner, allows thebodies and heads of rules to contain nested expressions. The other, due toNiemela and Simons, uses weight constraints. We show that there is a simple,modular translation from the language of weight constraints into the languageof nested expressions that preserves the program's answer sets. Nestedexpressions can be eliminated from the result of this translation in favor ofadditional atoms. The translation makes it possible to compute answer sets forsome programs with weight constraints using satisfiability solvers, and toprove the strong equivalence of programs with weight constraints using thelogic of here-and there."}
2.4e+03 pattern=  0 task=  0 (u=26351)  {'docs': u'We study theoretical and computational aspects of the least squares fit (LSF)of circles and circular arcs. First we discuss the existence and uniqueness ofLSF and various parametrization schemes. Then we evaluate several popularcircle fitting algorithms and propose a new one that surpasses the existingmethods in reliability. We also discuss and compare direct (algebraic) circlefits.'}
2.48e+03 pattern=  0 task=  1 (u=26351)  {'docs': u"We study the problem of fitting parametrized curves to noisy data. Undercertain assumptions (known as Cartesian and radial functional models), wederive asymptotic expressions for the bias and the covariance matrix of theparameter estimates. We also extend Kanatani's version of the Cramer-Rao lowerbound, which he proved for unbiased estimates only, to more general estimatesthat include many popular algorithms (most notably, the orthogonal leastsquares and algebraic fits). We then show that the gradient-weighted algebraicfit is statistically efficient and describe all other statistically efficientalgebraic fits."}
2.48e+03 pattern=  0 task=  0 (u=26353)  {'docs': u"Today's cluster computers suffer from slow I/O, which slows downI/O-intensive applications. We show that fast disk I/O can be achieved byoperating a parallel file system over fast networks such as Myrinet or GigabitEthernet.  In this paper, we demonstrate how the ParaStation3 communication system helpsspeed-up the performance of parallel I/O on clusters using the open sourceparallel virtual file system (PVFS) as testbed and production system. We willdescribe the set-up of PVFS on the Alpha-Linux-Cluster-Engine (ALiCE) locatedat Wuppertal University, Germany. Benchmarks on ALiCE achievewrite-performances of up to 1 GB/s from a 32-processor compute-partition to a32-processor PVFS I/O-partition, outperforming known benchmark results for PVFSon the same network by more than a factor of 2. Read-performance frombuffer-cache reaches up to 2.2 GB/s. Our benchmarks are giant, I/O-intensiveeigenmode problems from lattice quantum chromodynamics, demonstrating stabilityand performance of PVFS over Parastation in large-scale production runs."}
3.08e+03 pattern=  0 task=  0 (u=26414)  {'docs': u'We prove several results about the relations between injectivity andsurjectivity for sand automata. Moreover, we begin the exploration of thedynamical behavior of sand automata proving that the property of nilpotency isundecidable. We believe that the proof technique used for this last resultmight reveal useful for many other results in this context.'}
2.75e+03 pattern=  0 task=  0 (u=26420)  {'docs': u"In this paper we suggest an architecture for a software agent which operatesa physical device and is capable of making observations and of testing andrepairing the device's components. We present simplified definitions of thenotions of symptom, candidate diagnosis, and diagnosis which are based on thetheory of action language ${\\cal AL}$. The definitions allow one to give asimple account of the agent's behavior in which many of the agent's tasks arereduced to computing stable models of logic programs."}
2.62e+03 pattern=  0 task=  0 (u=26571)  {'docs': u'We consider the implementation of a parallel Monte Carlo code forhigh-performance simulations on PC clusters with MPI. We carry out tests ofspeedup and efficiency. The code is used for numerical simulations of pureSU(2) lattice gauge theory at very large lattice volumes, in order to study theinfrared behavior of gluon and ghost propagators. This problem is directlyrelated to the confinement of quarks and gluons in the physics of stronginteractions.'}
3.13e+03 pattern=  0 task=  0 (u=26582)  {'docs': u'Supercomputing systems today often come in the form of large numbers ofcommodity systems linked together into a computing cluster. These systems, likeany distributed system, can have large numbers of independent hardwarecomponents cooperating or collaborating on a computation. Unfortunately, any ofthis vast number of components can fail at any time, resulting in potentiallyerroneous output. In order to improve the robustness of supercomputingapplications in the presence of failures, many techniques have been developedto provide resilience to these kinds of system faults. This survey provides anoverview of these various fault-tolerance techniques.'}
3.13e+03 pattern=  0 task=  1 (u=26582)  {'docs': u'Existing attempts at utility computing revolve around two approaches. Thefirst consists of proprietary solutions involving renting time on dedicatedutility computing machines. The second requires the use of heavy, monolithicapplications that are difficult to deploy, maintain, and use.  We propose a distributed, community-oriented approach to utility computing.Our approach provides an infrastructure built on Web Services in which modularcomponents are combined to create a seemingly simple, yet powerful system. Thecommunity-oriented nature generates an economic environment which results infair transactions between consumers and providers of computing cycles whilesimultaneously encouraging improvements in the infrastructure of thecomputational grid itself.'}
2.18e+03 pattern=  0 task=  0 (u=26819)  {'docs': u"We give a lower bound on the iteration complexity of a natural class ofLagrangean-relaxation algorithms for approximately solving packing/coveringlinear programs. We show that, given an input with $m$ random 0/1-constraintson $n$ variables, with high probability, any such algorithm requires$\\Omega(\\rho \\log(m)/\\epsilon^2)$ iterations to compute a$(1+\\epsilon)$-approximate solution, where $\\rho$ is the width of the input.The bound is tight for a range of the parameters $(m,n,\\rho,\\epsilon)$.  The algorithms in the class include Dantzig-Wolfe decomposition, Benders'decomposition, Lagrangean relaxation as developed by Held and Karp [1971] forlower-bounding TSP, and many others (e.g. by Plotkin, Shmoys, and Tardos [1988]and Grigoriadis and Khachiyan [1996]). To prove the bound, we use a discrepancyargument to show an analogous lower bound on the support size of$(1+\\epsilon)$-approximate mixed strategies for random two-player zero-sum0/1-matrix games."}
3.28e+03 pattern=  0 task=  0 (u=27032)  {'docs': u'Compatibility of phylogenetic trees is the most important concept underlyingwidely-used methods for assessing the agreement of different phylogenetic treeswith overlapping taxa and combining them into common supertrees to reveal thetree of life. The notion of ancestral compatibility of phylogenetic trees withnested taxa was introduced by Semple et al in 2004. In this paper we analyze indetail the meaning of this compatibility from the points of view of the localstructure of the trees, of the existence of embeddings into a common supertree,and of the joint properties of their cluster representations. Our analysisleads to a very simple polynomial-time algorithm for testing thiscompatibility, which we have implemented and is freely available for downloadfrom the BioPerl collection of Perl modules for computational biology.'}
3.28e+03 pattern=  0 task=  0 (u=27039)  {'docs': u"A cycle double cover (CDC) of an undirected graph is a collection of thegraph's cycles such that every edge of the graph belongs to exactly two cycles.We describe a constructive method for generating all the cubic graphs that havea 6-CDC (a CDC in which every cycle has length 6). As an application of themethod, we prove that all such graphs have a Hamiltonian cycle. A sense ofdirection is an edge labeling on graphs that follows a globally consistentscheme and is known to considerably reduce the complexity of severaldistributed problems. In [9], a particular instance of sense of direction,called a chordal sense of direction (CSD), is studied and the class ofk-regular graphs that admit a CSD with exactly k labels (a minimal CSD) isanalyzed. We now show that nearly all the cubic graphs in this class have a6-CDC, the only exception being K4."}
2.15e+03 pattern=  0 task=  0 (u=27062)  {'docs': u'This paper addresses the problem of finding shortest paths homotopic to agiven disjoint set of paths that wind amongst point obstacles in the plane. Wepresent a faster algorithm than previously known.'}
1.12e+03 pattern=  1 task=  0 (u=27074)  {'docs': u'This paper describes a case study conducted in collaboration with Nortel todemonstrate the feasibility of applying formal modeling techniques totelecommunication systems. A formal description language, SDL, was chosen byour qualitative CASE tool evaluation to model a multimedia-messaging systemdescribed by an 80-page natural language specification. Our model was used toidentify errors in the software requirements document and to derive testsuites, shadowing the existing development process and keeping track of avariety of productivity data.'}
3.23e+03 pattern=  0 task=  0 (u=27117)  {'docs': u'In this paper we systematically investigate the connections between logicswith a finite number of variables, structures of bounded pathwidth, and linearDatalog Programs. We prove that, in the context of Constraint SatisfactionProblems, all these concepts correspond to different mathematical embodimentsof a unique robust notion that we call bounded path duality. We also study thecomputational complexity implications of the notion of bounded path duality. Weshow that every constraint satisfaction problem $\\csp(\\best)$ with bounded pathduality is solvable in NL and that this notion explains in a uniform way allfamilies of CSPs known to be in NL. Finally, we use the results developed inthe paper to identify new problems in NL.'}
3.23e+03 pattern=  0 task=  0 (u=27118)  {'docs': u"In this paper we investigate the problem of searching monotonemulti-dimensional arrays. We generalize Linial and Saks' search algorithm\\cite{LS1} for monotone 3-dimensional arrays to $d$-dimensions with $d\\geq 4$.Our new search algorithm is asymptotically optimal for $d=4$."}
1.87e+03 pattern=  0 task=  0 (u=27375)  {'docs': u'Two parties, Alice and Bob, wish to distill a binary secret key out of a listof correlated variables that they share after running a quantum keydistribution protocol based on continuous-spectrum quantum carriers. We presenta novel construction that allows the legitimate parties to get equal bitstrings out of correlated variables by using a classical channel, with as fewleaked information as possible. This opens the way to securely correctingnon-binary key elements. In particular, the construction is refined to the caseof Gaussian variables as it applies directly to recent continuous-variableprotocols for quantum key distribution.'}
3.25e+03 pattern=  0 task=  0 (u=27453)  {'docs': u'A general condition determining the optimal performance of a complex systemhas not yet been found and the possibility of its existence is unknown. Tocontribute in this direction, an optimization algorithm as a complex system ispresented. The performance of the algorithm for any problem is controlled as aconvex function with a single optimum. To characterize the performanceoptimums, certain quantities of the algorithm and the problem are suggested andinterpreted as their complexities. An optimality condition of the algorithm iscomputationally found: if the algorithm shows its best performance for aproblem, then the complexity of the algorithm is in a linear relationship withthe complexity of the problem. The optimality condition provides a newperspective to the subject by recognizing that the relationship between certainquantities of the complex system and the problem may determine the optimalperformance.'}
3.02e+03 pattern=  0 task=  0 (u=27524)  {'docs': u'This paper presents a generalization of the disjunctive paraconsistentrelational data model in which disjunctive positive and negative informationcan be represented explicitly and manipulated. There are situations where theclosed world assumption to infer negative facts is not valid or undesirable andthere is a need to represent and reason with negation explicitly. We considerexplicit disjunctive negation in the context of disjunctive databases as thereis an interesting interplay between these two types of information. Generalizeddisjunctive paraconsistent relation is introduced as the main structure in thismodel. The relational algebra is appropriately generalized to work ongeneralized disjunctive paraconsistent relations and their correctness isestablished.'}
3.06e+03 pattern=  0 task=  1 (u=27524)  {'docs': u'This paper presents an extension of generalized disjunctive paraconsistentrelational data model in which pure disjunctive positive and negativeinformation as well as mixed disjunctive positive and negative information canbe represented explicitly and manipulated. We consider explicit mixeddisjunctive information in the context of disjunctive databases as there is aninteresting interplay between these two types of information. Extendedgeneralized disjunctive paraconsistent relation is introduced as the mainstructure in this model. The relational algebra is appropriately generalized towork on extended generalized disjunctive paraconsistent relations and theircorrectness is established.'}
3.26e+03 pattern=  0 task=  2 (u=27524)  {'docs': u'This book presents the advancements and applications of neutrosophics.Chapter 1 first introduces the interval neutrosophic sets which is an instanceof neutrosophic sets. In this chapter, the definition of interval neutrosophicsets and set-theoretic operators are given and various properties of intervalneutrosophic set are proved. Chapter 2 defines the interval neutrosophic logicbased on interval neutrosophic sets including the syntax and semantics of firstorder interval neutrosophic propositional logic and first order intervalneutrosophic predicate logic. The interval neutrosophic logic can reason andmodel fuzzy, incomplete and inconsistent information. In this chapter, we alsodesign an interval neutrosophic inference system based on first order intervalneutrosophic predicate logic. The interval neutrosophic inference system can beapplied to decision making. Chapter 3 gives one application of intervalneutrosophic sets and logic in the field of relational databases. Neutrosophicdata model is the generalization of fuzzy data model and paraconsistent datamodel. Here, we generalize various set-theoretic and relation-theoreticoperations of fuzzy data model to neutrosophic data model. Chapter 4 givesanother application of interval neutrosophic logic. A soft semantic WebServices agent framework is proposed to faciliate the registration anddiscovery of high quality semantic Web Services agent. The intelligentinference engine module of soft Semantic Web Services agent is implementedusing interval neutrosophic logic.'}
3.06e+03 pattern=  0 task=  0 (u=27526)  {'docs': u'In this paper, we present a generalization of the relational data model basedon paraconsistent intuitionistic fuzzy sets. Our data model is capable ofmanipulating incomplete as well as inconsistent information. Fuzzy relation orintuitionistic fuzzy relation can only handle incomplete information.Associated with each relation are two membership functions one is calledtruth-membership function $T$ which keeps track of the extent to which webelieve the tuple is in the relation, another is called false-membershipfunction which keeps track of the extent to which we believe that it is not inthe relation. A paraconsistent intuitionistic fuzzy relation is inconsistent ifthere exists one tuple $a$ such that $T(a) + F(a) > 1$. In order to handleinconsistent situation, we propose an operator called split to transforminconsistent paraconsistent intuitionistic fuzzy relations intopseudo-consistent paraconsistent intuitionistic fuzzy relations and do theset-theoretic and relation-theoretic operations on them and finally use anotheroperator called combine to transform the result back to paraconsistentintuitionistic fuzzy relation. For this model, we define algebraic operatorsthat are generalisations of the usual operators such as union, selection, joinon fuzzy relations. Our data model can underlie any database and knowledge-basemanagement system that deals with incomplete and inconsistent information.'}
3.03e+03 pattern=  0 task=  0 (u=27530)  {'docs': u'E. Bach, following an idea of T. Itoh, has shown how to build a small set ofnumbers modulo a prime p such that at least one element of this set is agenerator of $\\pF{p}$\\cite{Bach:1997:sppr,Itoh:2001:PPR}. E. Bach suggests alsothat at least half of his set should be generators. We show here that a slightvariant of this set can indeed be made to contain a ratio of primitive roots asclose to 1 as necessary. We thus derive several algorithms computing primitiveroots correct with very high probability in polynomial time. In particular wepresent an asymptotically $O^{\\sim}(\\sqrt{\\frac{1}{\\epsilon}}log^1.5(p) +\\log^2(p))$ algorithm providing primitive roots of $p$ with probability ofcorrectness greater than $1-\\epsilon$ and several $O(log^\\alpha(p))$, $\\alpha\\leq 5.23$ algorithms computing "Industrial-strength" primitive roots withprobabilities e.g. greater than the probability of "hardware malfunctions".'}
3.16e+03 pattern=  0 task=  0 (u=27642)  {'docs': u'We suggest to employ techniques from Natural Language Processing (NLP) andKnowledge Representation (KR) to transform existing documents into documentsamenable for the Semantic Web. Semantic Web documents have at least part oftheir semantics and pragmatics marked up explicitly in both a machineprocessable as well as human readable manner. XML and its related standards(XSLT, RDF, Topic Maps etc.) are the unifying platform for the tools andmethodologies developed for different application scenarios.'}
3.16e+03 pattern=  0 task=  0 (u=27645)  {'docs': u'We present methods to synthesize cooperative strategies for multi-vehiclecontrol problems using mixed integer linear programming. Complex multi-vehiclecontrol problems are expressed as mixed logical dynamical systems. Optimalstrategies for these systems are then solved for using mixed integer linearprogramming. We motivate the methods on problems derived from an adversarialgame between two teams of robots called RoboFlag. We assume the strategy forone team is fixed and governed by state machines. The strategy for the otherteam is generated using our methods. Finally, we perform an average casecomputational complexity study on our approach.'}
2.41e+03 pattern=  0 task=  0 (u=27728)  {'docs': u'There is a growing interest in using Kalman-filter models in brain modelling.In turn, it is of considerable importance to make Kalman-filters amenable forreinforcement learning. In the usual formulation of optimal control it iscomputed off-line by solving a backward recursion. In this technical note weshow that slight modification of the linear-quadratic-Gaussian Kalman-filtermodel allows the on-line estimation of optimal control and makes the bridge toreinforcement learning. Moreover, the learning rule for value estimationassumes a Hebbian form weighted by the error of the value estimation.'}
2.14e+03 pattern=  0 task=  0 (u=27785)  {'docs': u'We present a new characterization of termination of general logic programs.Most existing termination analysis approaches rely on some static informationabout the structure of the source code of a logic program, such as modes/types,norms/level mappings, models/interargument relations, and the like. We proposea dynamic approach which employs some key dynamic features of an infinite(generalized) SLDNF-derivation, such as repetition of selected subgoals andrecursive increase in term size. We also introduce a new formulation ofSLDNF-trees, called generalized SLDNF-trees. Generalized SLDNF-trees deal withnegative subgoals in the same way as Prolog and exist for any general logicprograms.'}
1.44e+03 pattern=  0 task=  0 (u=27801)  {'docs': u'Existing procedures for model validation have been deemed inadequate for manyengineering systems. The reason of this inadequacy is due to the high degree ofcomplexity of the mechanisms that govern these systems. It is proposed in thispaper to shift the attention from modeling the engineering system itself tomodeling the uncertainty that underlies its behavior. A mathematical frameworkfor modeling the uncertainty in complex engineering systems is developed. Thisframework uses the results of computational learning theory. It is based on thepremise that a system model is a learning machine.'}
2.84e+03 pattern=  0 task=  0 (u=28203)  {'docs': u'We discuss a continuous variables method of quantum key distributionemploying strongly polarized coherent states of light. The key encoding isperformed using the variables known as Stokes parameters, rather than the fieldquadratures. Their quantum counterpart, the Stokes operators $\\hat{S}_i$(i=1,2,3), constitute a set of non-commuting operators, being the precision ofsimultaneous measurements of a pair of them limited by an uncertainty-likerelation. Alice transmits a conveniently modulated two-mode coherent state, andBob randomly measures one of the Stokes parameters of the incoming beam. Afterperforming reconciliation and privacy amplification procedures, it is possibleto distill a secret common key. We also consider a non-ideal situation, inwhich coherent states with thermal noise, instead of pure coherent states, areused for encoding.'}
2.57e+03 pattern=  0 task=  0 (u=28241)  {'docs': u'We present a method for solving service allocation problems in which a set ofservices must be allocated to a set of agents so as to maximize a globalutility. The method is completely distributed so it can scale to any number ofservices without degradation. We first formalize the service allocation problemand then present a simple hill-climbing, a global hill-climbing, and abidding-protocol algorithm for solving it. We analyze the expected performanceof these algorithms as a function of various problem parameters such as thebranching factor and the number of agents. Finally, we use the sensorallocation problem, an instance of a service allocation problem, to show thebidding protocol at work. The simulations also show that phase transition onthe expected quality of the solution exists as the amount of communicationbetween agents increases.'}
3.23e+03 pattern=  0 task=  0 (u=28442)  {'docs': u'Almost all of us have multiple cyberspace identities, and these {\\emcyber}alter egos are networked together to form a vast cyberspace socialnetwork. This network is distinct from the world-wide-web (WWW), which is beingqueried and mined to the tune of billions of dollars everyday, and untilrecently, has gone largely unexplored. Empirically, the cyberspace socialnetworks have been found to possess many of the same complex features thatcharacterize its real counterparts, including scale-free degree distributions,low diameter, and extensive connectivity. We show that these topologicalfeatures make the latent networks particularly suitable for explorations andmanagement via local-only messaging protocols. {\\em Cyber}alter egos cancommunicate via their direct links (i.e., using only their own address books)and set up a highly decentralized and scalable message passing network that canallow large-scale sharing of information and data. As one particular example ofsuch collaborative systems, we provide a design of a spam filtering system, andour large-scale simulations show that the system achieves a spam detection rateclose to 100%, while the false positive rate is kept around zero. This systemhas several advantages over other recent proposals (i) It uses an alreadyexisting network, created by the same social dynamics that govern our dailylives, and no dedicated peer-to-peer (P2P) systems or centralized server-basedsystems need be constructed; (ii) It utilizes a percolation search algorithmthat makes the query-generated traffic scalable; (iii) The network has a builtin trust system (just as in social networks) that can be used to thwartmalicious attacks; iv) It can be implemented right now as a plugin to popularemail programs, such as MS Outlook, Eudora, and Sendmail.'}
1.75e+03 pattern=  0 task=  0 (u=28468)  {'docs': u'Many communication and social networks have power-law link distributions,containing a few nodes which have a very high degree and many with low degree.The high connectivity nodes play the important role of hubs in communicationand networking, a fact which can be exploited when designing efficient searchalgorithms. We introduce a number of local search strategies which utilize highdegree nodes in power-law graphs and which have costs which scale sub-linearlywith the size of the graph. We also demonstrate the utility of these strategieson the Gnutella peer-to-peer network.'}
3.27e+03 pattern=  0 task=  0 (u=28512)  {'docs': u"A constructive proof of the Goedel-Rosser incompleteness theorem has beencompleted using the Coq proof assistant. Some theory of classical first-orderlogic over an arbitrary language is formalized. A development of primitiverecursive functions is given, and all primitive recursive functions are provedto be representable in a weak axiom system. Formulas and proofs are encoded asnatural numbers, and functions operating on these codes are proved to beprimitive recursive. The weak axiom system is proved to be essentiallyincomplete. In particular, Peano arithmetic is proved to be consistent in Coq'stype theory and therefore is incomplete."}
2.44e+03 pattern=  0 task=  0 (u=28749)  {'docs': u'We discuss a problem of handling resource reservations. The resource can bereserved for some time, it can be freed or it can be queried what is thelargest amount of reserved resource during a time interval. We show that theproblem has a lower bound of $\\Omega(\\log n)$ per operation on average and wegive a matching upper bound algorithm. Our solution also solves a dynamicversion of the related problems of a prefix sum and a partial sum.'}
2.64e+03 pattern=  0 task=  1 (u=28749)  {'docs': u'In this paper we present a discrete data structure for reservations oflimited resources. A reservation is defined as a tuple consisting of the timeinterval of when the resource should be reserved, $I_R$, and the amount of theresource that is reserved, $B_R$, formally $R=\\{I_R,B_R\\}$.  The data structure is similar to a segment tree. The maximum spanninginterval of the data structure is fixed and defined in advance. The granularityand thereby the size of the intervals of the leaves is also defined in advance.The data structure is built only once. Neither nodes nor leaves are everinserted, deleted or moved. Hence, the running time of the operations does notdepend on the number of reservations previously made. The running time does notdepend on the size of the interval of the reservation either. Let $n$ be thenumber of leaves in the data structure. In the worst case, the number oftouched (i.e. traversed) nodes is in any operation $O(\\log n)$, hence therunning time of any operation is also $O(\\log n)$.'}
2.64e+03 pattern=  0 task=  0 (u=28751)  {'docs': u'We compare the open source model of software development to peer review inacademia.'}
2.12e+03 pattern=  0 task=  0 (u=28785)  {'docs': u'The Web graph is a giant social network whose properties have been measuredand modeled extensively in recent years. Most such studies concentrate on thegraph structure alone, and do not consider textual properties of the nodes.Consequently, Web communities have been characterized purely in terms of graphstructure and not on page content. We propose that a topic taxonomy such asYahoo! or the Open Directory provides a useful framework for understanding thestructure of content-based clusters and communities. In particular, using atopic taxonomy and an automatic classifier, we can measure the backgrounddistribution of broad topics on the Web, and analyze the capability of recentrandom walk algorithms to draw samples which follow such distributions. Inaddition, we can measure the probability that a page about one broad topic willlink to another broad topic. Extending this experiment, we can measure howquickly topic context is lost while walking randomly on the Web graph.Estimates of this topic mixing distance may explain why a global PageRank isstill meaningful in the context of broad queries. In general, our measurementsmay prove valuable in the design of community-specific crawlers and link-basedranking systems.'}
2.05e+03 pattern=  0 task=  0 (u=28788)  {'docs': u'Boolean functions can be used to express the groundness of, and tracegrounding dependencies between, program variables in (constraint) logicprograms. In this paper, a variety of issues pertaining to the efficient Prologimplementation of groundness analysis are investigated, focusing on the domainof definite Boolean functions, Def. The systematic design of the representationof an abstract domain is discussed in relation to its impact on the algorithmiccomplexity of the domain operations; the most frequently called operationsshould be the most lightweight. This methodology is applied to Def, resultingin a new representation, together with new algorithms for its domain operationsutilising previously unexploited properties of Def -- for instance,quadratic-time entailment checking. The iteration strategy driving the analysisis also discussed and a simple, but very effective, optimisation of inducedmagic is described. The analysis can be implemented straightforwardly in Prologand the use of a non-ground representation results in an efficient, scalabletool which does not require widening to be invoked, even on the largestbenchmarks. An extensive experimental evaluation is given'}
2.11e+03 pattern=  0 task=  1 (u=28788)  {'docs': u'In order to improve precision and efficiency sharing analysis should trackboth freeness and linearity. The abstract unification algorithms for thesecombined domains are suboptimal, hence there is scope for improving precision.This paper proposes three optimisations for tracing sharing in combination withfreeness and linearity. A novel connection between equations and sharingabstractions is used to establish correctness of these optimisations even inthe presence of rational trees. A method for pruning intermediate sharingabstractions to improve efficiency is also proposed. The optimisations arelightweight and therefore some, if not all, of these optimisations will be ofinterest to the implementor.'}
2.05e+03 pattern=  0 task=  0 (u=28789)  {'docs': u'One recurring problem in program development is that of understanding how tore-use code developed by a third party. In the context of (constraint) logicprogramming, part of this problem reduces to figuring out how to query aprogram. If the logic program does not come with any documentation, then theprogrammer is forced to either experiment with queries in an ad hoc fashion ortrace the control-flow of the program (backward) to infer the modes in which apredicate must be called so as to avoid an instantiation error. This paperpresents an abstract interpretation scheme that automates the latter technique.The analysis presented in this paper can infer moding properties which ifsatisfied by the initial query, come with the guarantee that the program andquery can never generate any moding or instantiation errors. Other applicationsof the analysis are discussed. The paper explains how abstract domains withcertain computational properties (they condense) can be used to tracecontrol-flow backward (right-to-left) to infer useful properties of initialqueries. A correctness argument is presented and an implementation is reported.'}
3.1e+03 pattern=  0 task=  0 (u=28798)  {'docs': u'Mapping the Internet generally consists in sampling the network from alimited set of sources by using traceroute-like probes. This methodology, akinto the merging of different spanning trees to a set of destination, has beenargued to introduce uncontrolled sampling biases that might produce statisticalproperties of the sampled graph which sharply differ from the original ones. Inthis paper we explore these biases and provide a statistical analysis of theirorigin. We derive an analytical approximation for the probability of edge andvertex detection that exploits the role of the number of sources and targetsand allows us to relate the global topological properties of the underlyingnetwork with the statistical accuracy of the sampled graph. In particular, wefind that the edge and vertex detection probability depends on the betweennesscentrality of each element. This allows us to show that shortest path routedsampling provides a better characterization of underlying graphs with broaddistributions of connectivity. We complement the analytical discussion with athroughout numerical investigation of simulated mapping strategies in networkmodels with different topologies. We show that sampled graphs provide a fairqualitative characterization of the statistical properties of the originalnetworks in a fair range of different strategies and exploration parameters.Moreover, we characterize the level of redundancy and completeness of theexploration process as a function of the topological properties of the network.Finally, we study numerically how the fraction of vertices and edges discoveredin the sampled graph depends on the particular deployements of probing sources.The results might hint the steps toward more efficient mapping strategies.'}
3.28e+03 pattern=  0 task=  0 (u=28926)  {'docs': u"Recently, several public key exchange protocols based on symbolic computationin non-commutative (semi)groups were proposed as a more efficient alternativeto well established protocols based on numeric computation. Notably, theprotocols due to Anshel-Anshel-Goldfeld and Ko-Lee et al. exploited theconjugacy search problem in groups, which is a ramification of the discretelogarithm problem. However, it is a prevalent opinion now that the conjugacysearch problem alone is unlikely to provide sufficient level of security nomatter what particular group is chosen as a platform.  In this paper we employ another problem (we call it the decompositionproblem), which is more general than the conjugacy search problem, and wesuggest to use R. Thompson's group as a platform. This group is well known inmany areas of mathematics, including algebra, geometry, and analysis. It alsohas several properties that make it fit for cryptographic purposes. Inparticular, we show here that the word problem in Thompson's group is solvablein almost linear time."}
2.53e+03 pattern=  0 task=  0 (u=28946)  {'docs': u"A coloring of a complete bipartite graph is shuffle-preserved if it is thecase that assigning a color $c$ to edges $(u, v)$ and $(u', v')$ enforces thesame color assignment for edges $(u, v')$ and $(u',v)$. (In words, the inducedsubgraph with respect to color $c$ is complete.) In this paper, we investigatea variant of the Ramsey problem for the class of complete bipartitemultigraphs. (By a multigraph we mean a graph in which multiple edges, but noloops, are allowed.) Unlike the conventional m-coloring scheme in Ramsey theorywhich imposes a constraint (i.e., $m$) on the total number of colors allowed ina graph, we introduce a relaxed version called m-local coloring which onlyrequires that, for every vertex $v$, the number of colors associated with $v$'sincident edges is bounded by $m$. Note that the number of colors found in agraph under $m$-local coloring may exceed m. We prove that given any $n \\timesn$ complete bipartite multigraph $G$, every shuffle-preserved $m$-localcoloring displays a monochromatic copy of $K_{p,p}$ provided that $2(p-1)(m-1)< n$. Moreover, the above bound is tight when (i) $m=2$, or (ii) $n=2^k$ and$m=3\\cdot 2^{k-2}$ for every integer $k\\geq 2$. As for the lower bound of $p$,we show that the existence of a monochromatic $K_{p,p}$ is not guaranteed if$p> \\lceil \\frac{n}{m} \\rceil$. Finally, we give a generalization for$k$-partite graphs and a method applicable to general graphs. Many conclusionsfound in $m$-local coloring can be inferred to similar results of $m$-coloring."}
2.67e+03 pattern=  0 task=  0 (u=29013)  {'docs': u"Denial of Service (DoS) attacks are one of the most challenging threats toInternet security. An attacker typically compromises a large number ofvulnerable hosts and uses them to flood the victim's site with malicioustraffic, clogging its tail circuit and interfering with normal traffic. Atpresent, the network operator of a site under attack has no other resolutionbut to respond manually by inserting filters in the appropriate edge routers todrop attack traffic. However, as DoS attacks become increasingly sophisticated,manual filter propagation becomes unacceptably slow or even infeasible.  In this paper, we present Active Internet Traffic Filtering, a new automaticfilter propagation protocol. We argue that this system provides a guaranteed,significant level of protection against DoS attacks in exchange for areasonable, bounded amount of router resources. We also argue that the proposedsystem cannot be abused by a malicious node to interfere with normal Internetoperation. Finally, we argue that it retains its efficiency in the face ofcontinued Internet growth."}
2.86e+03 pattern=  0 task=  1 (u=29013)  {'docs': u"A distributed denial-of-service (DDoS) attack can flood a victim site withmalicious traffic, causing service disruption or even complete failure.Public-access sites like amazon or ebay are particularly vulnerable to suchattacks, because they have no way of a priori blocking unauthorized traffic.  We present Active Internet Traffic Filtering (AITF), a mechanism thatprotects public-access sites from highly distributed attacks by causingundesired traffic to be blocked as close as possible to its sources. Weidentify filters as a scarce resource and show that AITF protects a significantamount of the victim's bandwidth, while requiring from each participatingrouter a number of filters that can be accommodated by today's routers. AITF isincrementally deployable, because it offers a substantial benefit even to thefirst sites that deploy it."}
3.15e+03 pattern= 12 task=  0 (u=29117)  {'docs': u'The effects of spike timing precision and dynamical behavior on errorcorrection in spiking neurons were investigated. Stationary discharges -- phaselocked, quasiperiodic, or chaotic -- were induced in a simulated neuron bypresenting pacemaker presynaptic spike trains across a model of a prototypicalinhibitory synapse. Reduced timing precision was modeled by jitteringpresynaptic spike times. Aftereffects of errors -- in this communication,missed presynaptic spikes -- were determined by comparing postsynaptic spiketimes between simulations identical except for the presence or absence oferrors. Results show that the effects of an error vary greatly depending on theongoing dynamical behavior. In the case of phase lockings, a high degree ofpresynaptic spike timing precision can provide significantly faster errorrecovery. For non-locked behaviors, isolated missed spikes can have little orno discernible aftereffects (or even serve to paradoxically reduce uncertaintyin postsynaptic spike timing), regardless of presynaptic imprecision. Thissuggests two possible categories of error correction: high-precision lockingwith rapid recovery and low-precision non-locked with error immunity.'}
2.59e+03 pattern=  0 task=  0 (u=29165)  {'docs': u'This article introduces the idea that probabilistic reasoning (PR) may beunderstood as "information compression by multiple alignment, unification andsearch" (ICMAUS). In this context, multiple alignment has a meaning which issimilar to but distinct from its meaning in bio-informatics, while unificationmeans a simple merging of matching patterns, a meaning which is related to butsimpler than the meaning of that term in logic.  A software model, SP61, has been developed for the discovery and formation of\'good\' multiple alignments, evaluated in terms of information compression. Themodel is described in outline.  Using examples from the SP61 model, this article describes in outline how theICMAUS framework can model various kinds of PR including: PR in best-matchpattern recognition and information retrieval; one-step \'deductive\' and\'abductive\' PR; inheritance of attributes in a class hierarchy; chains ofreasoning (probabilistic decision networks and decision trees, and PR with\'rules\'); geometric analogy problems; nonmonotonic reasoning and reasoning withdefault values; modelling the function of a Bayesian network.'}
2.59e+03 pattern=  0 task=  1 (u=29165)  {'docs': u"This paper argues that the operations of a 'Universal Turing Machine' (UTM)and equivalent mechanisms such as the 'Post Canonical System' (PCS) - which arewidely accepted as definitions of the concept of `computing' - may beinterpreted as *information compression by multiple alignment, unification andsearch* (ICMAUS).  The motivation for this interpretation is that it suggests ways in which theUTM/PCS model may be augmented in a proposed new computing system designed toexploit the ICMAUS principles as fully as possible. The provision of arelatively sophisticated search mechanism in the proposed 'SP' system appearsto open the door to the integration and simplification of a range of functionsincluding unsupervised inductive learning, best-match pattern recognition andinformation retrieval, probabilistic reasoning, planning and problem solving,and others. Detailed consideration of how the ICMAUS principles may be appliedto these functions is outside the scope of this article but relevant sourcesare cited in this article."}
2.79e+03 pattern=  0 task=  0 (u=29291)  {'docs': u'We embark in a program of studying the problem of better approximatingsurfaces by triangulations(triangular meshes) by considering the approximatingtriangulations as finite metric spaces and the target smooth surface as theirHaussdorff-Gromov limit. This allows us to define in a more natural way therelevant elements, constants and invariants s.a. principal directions andprincipal values, Gaussian and Mean curvature, etc. By a "natural way" we meanan intrinsic, discrete, metric definitions as opposed to approximating orparaphrasing the differentiable notions. In this way we hope to circumventcomputational errors and, indeed, conceptual ones, that are often inherent tothe classical, "numerical" approach. In this first study we consider theproblem of determining the Gaussian curvature of a polyhedral surface, by usingthe {\\em embedding curvature} in the sense of Wald (and Menger). We present twomodalities of employing these definitions for the computation of Gaussiancurvature.'}
2.65e+03 pattern=  0 task=  0 (u=29292)  {'docs': u'We present a novel scheme to the coverage problem, introducing a quantitativeway to estimate the interaction between a block and its enviroment.This isachieved by setting a discrete version of Green`s theorem, specially adaptedfor Model Checking based verification of integrated circuits.This method isbest suited for the coverage problem since it enables one to quantify theincompleteness or, on the other hand, the redundancy of a set of rules,describing the model under verification.Moreover this can be done continuouslythroughout the verification process, thus enabling the user to pinpoint thestages at which incompleteness/redundancy occurs. Although the method ispresented locally on a small hardware example, we additionally show itspossibility to provide precise coverage estimation also for large scalesystems. We compare this method to others by checking it on the sametest-cases.'}
2.23e+03 pattern=  0 task=  0 (u=29356)  {'docs': u'The Gisela framework for declarative programming was developed with thespecific aim of providing a tool that would be useful for knowledgerepresentation and reasoning within real-world applications. To achieve this, acomplete integration into an object-oriented application developmentenvironment was used. The framework and methodology developed provide twoalternative application programming interfaces (APIs): Programming usingobjects or programming using a traditional equational declarative style. Inaddition to providing complete integration, Gisela also allows extensions andmodifications due to the general computation model and well-defined APIs. Wegive a brief overview of the declarative model underlying Gisela and we presentthe methodology proposed for building applications together with some realexamples.'}
3.18e+03 pattern=  0 task=  0 (u=29361)  {'docs': u'In this paper we consider the wavelet synopsis construction problem withoutthe restriction that we only choose a subset of coefficients of the originaldata. We provide the first near optimal algorithm. We arrive at the abovealgorithm by considering space efficient algorithms for the restricted versionof the problem. In this context we improve previous algorithms by almost alinear factor and reduce the required space to almost linear. Our techniquesalso extend to histogram construction, and improve the space-running timetradeoffs for V-Opt and range query histograms. We believe the idea applies toa broad range of dynamic programs and demonstrate it by showing improvements ina knapsack-like setting seen in construction of Extended Wavelets.'}
2.75e+03 pattern=  0 task=  0 (u=29549)  {'docs': u'This paper describes learning in a compiler for algorithms solving classes ofthe logic minimization problem MINSAT, where the underlying propositionalformula is in conjunctive normal form (CNF) and where costs are associated withthe True/False values of the variables. Each class consists of all instancesthat may be derived from a given propositional formula and costs for True/Falsevalues by fixing or deleting variables, and by deleting clauses. The learningstep begins once the compiler has constructed a solution algorithm for a givenclass. The step applies that algorithm to comparatively few instances of theclass, analyses the performance of the algorithm on these instances, andmodifies the underlying propositional formula, with the goal that the algorithmwill perform much better on all instances of the class.'}
1.6e+03 pattern=  0 task=  0 (u=29590)  {'docs': u"We explore a new general-purpose heuristic for finding high-quality solutionsto hard optimization problems. The method, called extremal optimization, isinspired by self-organized criticality, a concept introduced to describeemergent complexity in physical systems. Extremal optimization successivelyreplaces extremely undesirable variables of a single sub-optimal solution withnew, random ones. Large fluctuations ensue, that efficiently explore many localoptima. With only one adjustable parameter, the heuristic's performance hasproven competitive with more elaborate methods, especially near phasetransitions which are believed to coincide with the hardest instances. We useextremal optimization to elucidate the phase transition in the 3-coloringproblem, and we provide independent confirmation of previously reportedextrapolations for the ground-state energy of +-J spin glasses in d=3 and 4."}
1.77e+03 pattern=  0 task=  1 (u=29590)  {'docs': u'Extremal optimization is a new general-purpose method for approximatingsolutions to hard optimization problems. We study the method in detail by wayof the NP-hard graph partitioning problem. We discuss the scaling behavior ofextremal optimization, focusing on the convergence of the average run as afunction of runtime and system size. The method has a single free parameter,which we determine numerically and justify using a simple argument. Ournumerical results demonstrate that on random graphs, extremal optimizationmaintains consistent accuracy for increasing system sizes, with anapproximation error decreasing over runtime roughly as a power law t^(-0.4). Ongeometrically structured graphs, the scaling of results from the average runsuggests that these are far from optimal, with large fluctuations betweenindividual trials. But when only the best runs are considered, resultsconsistent with theoretical arguments are recovered.'}
1.95e+03 pattern=  0 task=  2 (u=29590)  {'docs': u'Extremal Optimization, a recently introduced meta-heuristic for hardoptimization problems, is analyzed on a simple model of jamming. The model ismotivated first by the problem of finding lowest energy configurations for adisordered spin system on a fixed-valence graph. The numerical results for thespin system exhibit the same phenomena found in all earlier studies of extremaloptimization, and our analytical results for the model reproduce many of thesefeatures.'}
2.21e+03 pattern=  0 task=  0 (u=29626)  {'docs': u'Application development for distributed computing "Grids" can benefit fromtools that variously hide or enable application-level management of criticalaspects of the heterogeneous environment. As part of an investigation of theseissues, we have developed MPICH-G2, a Grid-enabled implementation of theMessage Passing Interface (MPI) that allows a user to run MPI programs acrossmultiple computers, at the same or different sites, using the same commandsthat would be used on a parallel computer. This library extends the ArgonneMPICH implementation of MPI to use services provided by the Globus Toolkit forauthentication, authorization, resource allocation, executable staging, andI/O, as well as for process creation, monitoring, and control. Variousperformance-critical operations, including startup and collective operations,are configured to exploit network topology information. The library alsoexploits MPI constructs for performance management; for example, the MPIcommunicator construct is used for application-level discovery of, andadaptation to, both network topology and network quality-of-service mechanisms.We describe the MPICH-G2 design and implementation, present performanceresults, and review application experiences, including record-settingdistributed simulations.'}
2.98e+03 pattern=  0 task=  0 (u=30086)  {'docs': u'After an introduction to the sequential version of FORM and the mechanismsbehind, we report on the status of our project of parallelization. We have nowa parallel version of FORM running on Cluster- and SMP-architectures. Thisversion can be used to run arbitrary FORM programs in parallel.'}
2.88e+03 pattern=  0 task=  0 (u=30199)  {'docs': u'Constraint Logic Programming (CLP) and Hereditary Harrop formulas (HH) aretwo well known ways to enhance the expressivity of Horn clauses. In this paper,we present a novel combination of these two approaches. We show how to enrichthe syntax and proof theory of HH with the help of a given constraint system,in such a way that the key property of HH as a logic programming language(namely, the existence of uniform proofs) is preserved. We also present aprocedure for goal solving, showing its soundness and completeness forcomputing answer constraints. As a consequence of this result, we obtain a newstrong completeness theorem for CLP that avoids the need to build disjunctionsof computed answers, as well as a more abstract formulation of a knowncompleteness theorem for HH.'}
2.88e+03 pattern=  0 task=  0 (u=30202)  {'docs': u'This paper presents the multi-threading and internet message communicationcapabilities of Qu-Prolog. Message addresses are symbolic and thecommunications package provides high-level support that completely hidesdetails of IP addresses and port numbers as well as the underlying TCP/IPtransport layer. The combination of the multi-threads and the high levelinter-thread message communications provide simple, powerful support forimplementing internet distributed intelligent applications.'}
3.06e+03 pattern=  0 task=  0 (u=30333)  {'docs': u'We show that if M is a DFA with n states over an arbitrary alphabet and L =L(M), then the worst-case state complexity of L^2 is n*2^n - 2^{n-1}. If,however, M is a DFA over a unary alphabet, then the worst-case state complexityof L^k is kn-k+1 for all k >= 2.'}
2.07e+03 pattern=  0 task=  0 (u=30372)  {'docs': u'We have constructed a simple semiclassical model of neural network whereneurons have quantum links with one another in a chosen way and affect oneanother in a fashion analogous to action potentials. We have examined the roleof stochasticity introduced by the quantum potential and compare the systemwith the classical system of an integrate-and-fire model by Hopfield. Averageperiodicity and short term retentivity of input memory are noted.'}
2.07e+03 pattern=  0 task=  1 (u=30372)  {'docs': u'We try to design a quantum neural network with qubits instead of classicalneurons with deterministic states, and also with quantum operators replacingteh classical action potentials. With our choice of gates interconnecting tehneural lattice, it appears that the state of the system behaves in waysreflecting both the strengths of coupling between neurons as well as initialconditions. We find that depending whether there is a threshold for emissionfrom excited to ground state, the system shows either aperiodic oscillations orcoherent ones with periodicity depending on the strength of coupling.'}
2.78e+03 pattern=  0 task=  2 (u=30372)  {'docs': u'In this work we compare social clusters with spin clusters and comparedifferent properties. We also try to compare phase changes in market and laborstratification with phase changes of spin clusters. Then we compare therequisites for redrawing the boundaries of social clusters with respect toenergy minimization and efficiency. We finally do a simulation experiment andshow that by choosing suitable link matrices for agents and attributes of thesame and of different agents it is possible to have at the same time behaviorsimilar to chaos or punctuated equilibrium in some attributes or fairly regularoscillations of preferences for other attributes, using greatest utility orefficiency as a criterion for change in conflicting social networks withdifferent agents having different preferences with respect to the attributes inthe agent himself or with similar attributes in other agents.'}
2e+03 pattern=  0 task=  0 (u=30379)  {'docs': u'Let U be a numeration system, a set X of integers is U-star-free if the setmade up of the U-representations of the elements in X is a star-free regularlanguage. Answering a question of A. de Luca and A. Restivo, we obtain acomplete logical characterization of the U-star-free sets of integers forsuitable numeration systems related to a Pisot number and in particular forinteger base systems. For these latter systems, we study as well the problem ofthe base dependence. Finally, the case of k-adic systems is also investigated.'}
1.99e+03 pattern=  0 task=  0 (u=30677)  {'docs': u'A major problem in evaluating stochastic local search algorithms forNP-complete problems is the need for a systematic generation of hard testinstances having previously known properties of the optimal solutions. On thebasis of statistical mechanics results, we propose random generators of hardand satisfiable instances for the 3-satisfiability problem (3SAT). The designof the hardest problem instances is based on the existence of a first orderferromagnetic phase transition and the glassy nature of excited states. Theanalytical predictions are corroborated by numerical results obtained fromcomplete as well as stochastic local algorithms.'}
3.01e+03 pattern=  0 task=  0 (u=30798)  {'docs': u'Logs are one of the most fundamental resources to any security professional.It is widely recognized by the government and industry that it is bothbeneficial and desirable to share logs for the purpose of security research.However, the sharing is not happening or not to the degree or magnitude that isdesired. Organizations are reluctant to share logs because of the risk ofexposing sensitive information to potential attackers. We believe thisreluctance remains high because current anonymization techniques are weak andone-size-fits-all--or better put, one size tries to fit all. We must developstandards and make anonymization available at varying levels, striking abalance between privacy and utility. Organizations have different needs andtrust other organizations to different degrees. They must be able to mapmultiple anonymization levels with defined risks to the trust levels they sharewith (would-be) receivers. It is not until there are industry standards formultiple levels of anonymization that we will be able to move forward andachieve the goal of widespread sharing of logs for security researchers.'}
1.39e+03 pattern=  0 task=  0 (u=30947)  {'docs': u"The possibility of translating logic programs into functional ones has longbeen a subject of investigation. Common to the many approaches is that theoriginal logic program, in order to be translated, needs to be well-moded andthis has led to the common understanding that these programs can be consideredto be the ``functional part'' of logic programs. As a consequence of this ithas become widely accepted that ``complex'' logical variables, the possibilityof a dynamic selection rule, and general properties of non-well-moded programsare exclusive features of logic programs. This is not quite true, as some ofthese features are naturally found in lazy functional languages. We readdressthe old question of what features are exclusive to the logic programmingparadigm by defining a simple translation applicable to a wider range of logicprograms, and demonstrate that the current circumscription is unreasonablyrestrictive."}
3e+03 pattern= 10 task=  0 (u=31611)  {'docs': u'Constructive dimension and constructive strong dimension are effectivizationsof the Hausdorff and packing dimensions, respectively. Each infinite binarysequence A is assigned a dimension dim(A) in [0,1] and a strong dimensionDim(A) in [0,1].  Let DIM^alpha and DIMstr^alpha be the classes of all sequences of dimensionalpha and of strong dimension alpha, respectively. We show that DIM^0 isproperly Pi^0_2, and that for all Delta^0_2-computable alpha in (0,1],DIM^alpha is properly Pi^0_3.  To classify the strong dimension classes, we use a more powerful effectiveBorel hierarchy where a co-enumerable predicate is used rather than aenumerable predicate in the definition of the Sigma^0_1 level. For allDelta^0_2-computable alpha in [0,1), we show that DIMstr^alpha is properly inthe Pi^0_3 level of this hierarchy. We show that DIMstr^1 is properly in thePi^0_2 level of this hierarchy.  We also prove that the class of Schnorr random sequences and the class ofcomputably random sequences are properly Pi^0_3.'}
2.89e+03 pattern=  0 task=  0 (u=31619)  {'docs': u'It is well known that, under certain conditions, it is possible to splitlogic programs under stable model semantics, i.e. to divide such a program intoa number of different "levels", such that the models of the entire program canbe constructed by incrementally constructing models for each level. Similarresults exist for other non-monotonic formalisms, such as auto-epistemic logicand default logic. In this work, we present a general, algebraicsplittingtheory for logics with a fixpoint semantics. Together with the framework ofapproximation theory, a general fixpoint theory for arbitrary operators, thisgives us a uniform and powerful way of deriving splitting results for eachlogic with a fixpoint semantics. We demonstrate the usefulness of theseresults, by generalizing existing results for logic programming, auto-epistemiclogic and default logic.'}
2.69e+03 pattern=  0 task=  0 (u=31820)  {'docs': u"Peer-To-Peer (P2P) networks are self-organizing, distributed systems, with nocentralized authority or infrastructure. Because of the voluntaryparticipation, the availability of resources in a P2P system can be highlyvariable and unpredictable. In this paper, we use ideas from Game Theory tostudy the interaction of strategic and rational peers, and propose adifferential service-based incentive scheme to improve the system'sperformance."}
3.13e+03 pattern=  0 task=  1 (u=31820)  {'docs': u'Wireless sensor networks offer the potential to span and monitor largegeographical areas inexpensively. Sensor network databases like TinyDB are thedominant architectures to extract and manage data in such networks. Sincesensors have significant power constraints (battery life), and highcommunication costs, design of energy efficient communication algorithms is ofgreat importance. The data flow in a sensor database is very different fromdata flow in an ordinary network and poses novel challenges in designingefficient routing algorithms. In this work we explore the problem of energyefficient routing for various different types of database queries and show thatin general, this problem is NP-complete. We give a constant factorapproximation algorithm for one class of query, and for other queries giveheuristic algorithms. We evaluate the efficiency of the proposed algorithms bysimulation and demonstrate their near optimal performance for various networksizes.'}
2.17e+03 pattern=  0 task=  0 (u=32028)  {'docs': u'Congestion control in the current Internet is accomplished mainly by TCP/IP.To understand the macroscopic network behavior that results from TCP/IP andsimilar end-to-end protocols, one main analytic technique is to show that thethe protocol maximizes some global objective function of the network traffic.Here we analyze a particular end-to-end, MIMD (multiplicative-increase,multiplicative-decrease) protocol. We show that if all users of the network usethe protocol, and all connections last for at least logarithmically manyrounds, then the total weighted throughput (value of all packets received) isnear the maximum possible. Our analysis includes round-trip-times, and (incontrast to most previous analyses) gives explicit convergence rates, allowsconnections to start and stop, and allows capacities to change.'}
3.21e+03 pattern=  0 task=  0 (u=32167)  {'docs': u'Consistent query answering is the problem of computing the answers from adatabase that are consistent with respect to certain integrity constraints thatthe database as a whole may fail to satisfy. Those answers are characterized asthose that are invariant under minimal forms of restoring the consistency ofthe database. In this context, we study the problem of repairing databases byfixing integer numerical values at the attribute level with respect to denialand aggregation constraints. We introduce a quantitative definition of databasefix, and investigate the complexity of several decision and optimizationproblems, including DFP, i.e. the existence of fixes within a given distancefrom the original instance, and CQA, i.e. deciding consistency of answers toaggregate conjunctive queries under different semantics. We provide sharpcomplexity bounds, identify relevant tractable cases; and introduceapproximation algorithms for some of those that are intractable. Morespecifically, we obtain results like undecidability of existence of fixes foraggregation constraints; MAXSNP-hardness of DFP, but a good approximationalgorithm for a relevant special case; and intractability but goodapproximation for CQA for aggregate queries for one database atom denials (plusbuilt-ins).'}
2.67e+03 pattern=  0 task=  0 (u=32238)  {'docs': u"Debugging is commonly understood as finding and fixing the cause of aproblem. But what does ``cause'' mean? How can we find causes? How can we provethat a cause is a cause--or even ``the'' cause? This paper defines common termsin debugging, highlights the principal techniques, their capabilities andlimitations."}
3.06e+03 pattern=  0 task=  0 (u=32261)  {'docs': u"We present a new approach to construction of protocols which are proofagainst communication errors. The construction is based on a generalization ofthe well known Ulam's game. We show equivalence between winning strategies inthis game and robust protocols for multi-party computation. We do not give anycomplete theory. We want rather to describe a new fresh idea. We use a treecode defined by Schulman. The tree code is the most important part of theinteractive version of Shannon's Coding Theorem proved by Schulman. He usesprobabilistic argument for the existence of a tree code without giving anyeffective construction. We show another proof yielding a randomizedconstruction which in contrary to his proof almost surely gives a good code.Moreover our construction uses much smaller alphabet."}
2.66e+03 pattern=  0 task=  0 (u=32359)  {'docs': u'The paper proposes a theoretical approach of the debugging of constraintprograms based on a notion of explanation tree. The proposed approach is anattempt to adapt algorithmic debugging to constraint programming. In thistheoretical framework for domain reduction, explanations are proof treesexplaining value removals. These proof trees are defined by inductivedefinitions which express the removals of values as consequences of other valueremovals. Explanations may be considered as the essence of constraintprogramming. They are a declarative view of the computation trace. Thediagnosis consists in locating an error in an explanation rooted by a symptom.'}
2.23e+03 pattern=  0 task=  0 (u=32360)  {'docs': u'Constraint logic programming combines declarativity and efficiency thanks toconstraint solvers implemented for specific domains. Value withdrawalexplanations have been efficiently used in several constraints programmingenvironments but there does not exist any formalization of them. This paper isan attempt to fill this lack. Furthermore, we hope that this theoretical toolcould help to validate some programming environments. A value withdrawalexplanation is a tree describing the withdrawal of a value during a domainreduction by local consistency notions and labeling. Domain reduction isformalized by a search tree using two kinds of operators: operators for localconsistency notions and operators for labeling. These operators are defined bysets of rules. Proof trees are built with respect to these rules. For eachremoved value, there exists such a proof tree which is the withdrawalexplanation of this value.'}
2.67e+03 pattern=  0 task=  0 (u=32368)  {'docs': u'This paper presents a novel technique for the automatic type identificationof arbitrary memory objects from a memory dump. Our motivating application isdebugging memory corruption problems in optimized, production systems -- aproblem domain largely unserved by extant methodologies. We describe ouralgorithm as applicable to any typed language, and we discuss it with respectto the formidable obstacles posed by C. We describe the heuristics that we havedeveloped to overcome these difficulties and achieve effective typeidentification on C-based systems. We further describe the implementation ofour heuristics on one C-based system -- the Solaris operating system kernel --and describe the extensions that we have added to the Solaris postmortemdebugger to allow for postmortem type identification. We show that ourimplementation yields a sufficiently high rate of type identification to beuseful for debugging memory corruption problems. Finally, we discuss some ofthe novel automated debugging mechanisms that can be layered upon postmortemtype identification.'}
2.75e+03 pattern=  0 task=  0 (u=32371)  {'docs': u'In the design of algorithms, the greedy paradigm provides a powerful tool forsolving efficiently classical computational problems, within the framework ofprocedural languages. However, expressing these algorithms within thedeclarative framework of logic-based languages has proven a difficult researchchallenge. In this paper, we extend the framework of Datalog-like languages toobtain simple declarative formulations for such problems, and propose effectiveimplementation techniques to ensure computational complexities comparable tothose of procedural formulations. These advances are achieved through the useof the "choice" construct, extended with preference annotations to effect theselection of alternative stable-models and nondeterministic fixpoints. We showthat, with suitable storage structures, the differential fixpoint computationof our programs matches the complexity of procedural algorithms in classicalsearch and optimization problems.'}
2.55e+03 pattern=  0 task=  0 (u=32410)  {'docs': u'We present some applications of intermediate logics in the field of AnswerSet Programming (ASP). A brief, but comprehensive introduction to the answerset semantics, intuitionistic and other intermediate logics is given. Someequivalence notions and their applications are discussed. Some results onintermediate logics are shown, and applied later to prove properties of answersets. A characterization of answer sets for logic programs with nestedexpressions is provided in terms of intuitionistic provability, generalizing arecent result given by Pearce.  It is known that the answer set semantics for logic programs with nestedexpressions may select non-minimal models. Minimal models can be very importantin some applications, therefore we studied them; in particular we obtain acharacterization, in terms of intuitionistic logic, of answer sets which arealso minimal models. We show that the logic G3 characterizes the notion ofstrong equivalence between programs under the semantic induced by these models.Finally we discuss possible applications and consequences of our results. Theyclearly state interesting links between ASP and intermediate logics, whichmight bring research in these two areas together.'}
2.79e+03 pattern=  0 task=  0 (u=32588)  {'docs': u'It is well-known that freeness and linearity information positively interactwith aliasing information, allowing both the precision and the efficiency ofthe sharing analysis of logic programs to be improved. In this paper we presenta novel combination of set-sharing with freeness and linearity information,which is characterized by an improved abstract unification operator. We providea new abstraction function and prove the correctness of the analysis for boththe finite tree and the rational tree cases. Moreover, we show that the samenotion of redundant information as identified in (Bagnara et al. 2002;Zaffanella et al. 2002) also applies to this abstract domain combination: thisallows for the implementation of an abstract unification operator running inpolynomial time and achieving the same precision on all the consideredobservable properties.'}
1.84e+03 pattern=  0 task=  0 (u=32746)  {'docs': u"We prove that computing a single pair of vertices that are mapped onto eachother by an isomorphism $\\phi$ between two isomorphic graphs is as hard ascomputing $\\phi$ itself. This result optimally improves upon a result ofG\\'{a}l et al. We establish a similar, albeit slightly weaker, result aboutcomputing complete Hamiltonian cycles of a graph from partial Hamiltoniancycles."}
3.27e+03 pattern=  0 task=  0 (u=32779)  {'docs': u'A modified realisability interpretation of infinitary logic is formalised andproved sound in constructive type theory (CTT). The logic considered subsumesfirst order logic. The interpretation makes it possible to extract programswith simplified types and to incorporate and reason about them in CTT.'}
1.95e+03 pattern=  0 task=  0 (u=32796)  {'docs': u'The approach for a network behavior description in terms of numericaltime-dependant functions of the protocol parameters is suggested. This providesa basis for application of methods of mathematical and theoretical physics forinformation flow analysis on network and for extraction of patterns of typicalnetwork behavior. The information traffic can be described as a trajectory inmulti-dimensional parameter-time space with dimension about 10-12. Based onthis study some algorithms for the proposed intrusion detection system arediscussed.'}
2.2e+03 pattern=  0 task=  1 (u=32796)  {'docs': u'An approach for real-time network monitoring in terms of numericaltime-dependant functions of protocol parameters is suggested. Applying complexsystems theory for information f{l}ow analysis of networks, the informationtraffic is described as a trajectory in multi-dimensional parameter-time spacewith about 10-12 dimensions. The network traffic description is synthesized byapplying methods of theoretical physics and complex systems theory, to providea robust approach for network monitoring that detects known intrusions, andsupports developing real systems for detection of unknown intrusions. Themethods of data analysis and pattern recognition presented are the basis of atechnology study for an automatic intrusion detection system that detects theattack in the reconnaissance stage.'}
2.28e+03 pattern=  0 task=  2 (u=32796)  {'docs': u'We introduce a novel approach to description of networks/graphs. It is basedon an analogue physical model which is dynamically evolved. This evolutiondepends on the connectivity matrix and readily brings out many qualitativefeatures of the graph.'}
2.28e+03 pattern=  0 task=  3 (u=32796)  {'docs': u"A general novel approach mapping discrete, combinatorial, graph-theoreticproblems onto ``physical'' models - namely $n$ simplexes in $n-1$ dimensions -is applied to the graph equivalence problem. It is shown to solve this longstanding problem in polynomial, short, time."}
2.3e+03 pattern=  0 task=  4 (u=32796)  {'docs': u'A novel approach to complex problems has been previously applied to graphclassification and the graph equivalence problem. Here we apply it to the NPcomplete problem of finding the largest perfect clique within a graph $G$.'}
1.91e+03 pattern=  0 task=  0 (u=32804)  {'docs': u'The paper presents a study on the portability of statistical syntacticknowledge in the framework of the structured language model (SLM). Weinvestigate the impact of porting SLM statistics from the Wall Street Journal(WSJ) to the Air Travel Information System (ATIS) domain. We compare thisapproach to applying the Microsoft rule-based parser (NLPwin) for the ATIS dataand to using a small amount of data manually parsed at UPenn for gathering theintial SLM statistics. Surprisingly, despite the fact that it performs modestlyin perplexity (PPL), the model initialized on WSJ parses outperforms the otherinitialization methods based on in-domain annotated data, achieving asignificant 0.4% absolute and 7% relative reduction in word error rate (WER)over a baseline system whose word error rate is 5.8%; the improvement measuredrelative to the minimum WER achievable on the N-best lists we worked with is12%.'}
1.95e+03 pattern=  0 task=  1 (u=32804)  {'docs': u'The paper investigates the use of richer syntactic dependencies in thestructured language model (SLM). We present two simple methods of enriching thedependencies in the syntactic parse trees used for intializing the SLM. Weevaluate the impact of both methods on the perplexity (PPL) andword-error-rate(WER, N-best rescoring) performance of the SLM. We show that thenew model achieves an improvement in PPL and WER over the baseline resultsreported using the SLM on the UPenn Treebank and Wall Street Journal (WSJ)corpora, respectively.'}
2.17e+03 pattern=  0 task=  0 (u=33221)  {'docs': u"This thesis introduces a new unsupervised learning framework, calledAlignment-Based Learning, which is based on the alignment of sentences andHarris's (1951) notion of substitutability. Instances of the framework can beapplied to an untagged, unstructured corpus of natural language sentences,resulting in a labelled, bracketed version of that corpus.  Firstly, the framework aligns all sentences in the corpus in pairs, resultingin a partition of the sentences consisting of parts of the sentences that areequal in both sentences and parts that are unequal. Unequal parts of sentencescan be seen as being substitutable for each other, since substituting oneunequal part for the other results in another valid sentence. The unequal partsof the sentences are thus considered to be possible (possibly overlapping)constituents, called hypotheses.  Secondly, the selection learning phase considers all hypotheses found by thealignment learning phase and selects the best of these. The hypotheses areselected based on the order in which they were found, or based on aprobabilistic function.  The framework can be extended with a grammar extraction phase. This extendedframework is called parseABL. Instead of returning a structured version of theunstructured input corpus, like the ABL system, this system also returns astochastic context-free or tree substitution grammar.  Different instances of the framework have been tested on the English ATIScorpus, the Dutch OVIS corpus and the Wall Street Journal corpus. One of theinteresting results, apart from the encouraging numerical results, is that allinstances can (and do) learn recursive structures."}
2.61e+03 pattern=  0 task=  0 (u=33538)  {'docs': u'In this paper, we present XtremWeb, a Global Computing platform used togenerate monte carlos showers in Auger, an HEP experiment to study the highestenergy cosmic rays at Mallargue-Mendoza, Argentina.  XtremWeb main goal, as a Global Computing platform, is to compute distributedapplications using idle time of widely interconnected machines. It isespecially dedicated to -but not limited to- multi-parameters applications suchas monte carlos computations; its security mechanisms ensuring not only hostsintegrity but also results certification and its fault tolerant features,encouraged us to test it and, finally, to deploy it as to support our CPU needsto simulate showers.  We first introduce Auger computing needs and how Global Computing could help.We then detail XtremWeb architecture and goals. The fourth and last partpresents the profits we have gained to choose this platform. We conclude onwhat could be done next.'}
3.22e+03 pattern=  0 task=  0 (u=33655)  {'docs': u'We present a simple point process model of $1/f^{\\beta}$ noise, coveringdifferent values of the exponent $\\beta$. The signal of the model consists ofpulses or events. The interpulse, interevent, interarrival, recurrence orwaiting times of the signal are described by the general Langevin equation withthe multiplicative noise and stochastically diffuse in some interval resultingin the power-law distribution. Our model is free from the requirement of a widedistribution of relaxation times and from the power-law forms of the pulses. Itcontains only one relaxation rate and yields $1/f^ {\\beta}$ spectra in a widerange of frequency. We obtain explicit expressions for the power spectra andpresent numerical illustrations of the model. Further we analyze the relationof the point process model of $1/f$ noise with the Bernamont-Surdin-McWhortermodel, representing the signals as a sum of the uncorrelated components. Weshow that the point process model is complementary to the model based on thesum of signals with a wide-range distribution of the relaxation times. Incontrast to the Gaussian distribution of the signal intensity of the sum of theuncorrelated components, the point process exhibits asymptotically a power-lawdistribution of the signal intensity. The developed multiplicative pointprocess model of $1/f^{\\beta}$ noise may be used for modeling and analysis ofstochastic processes in different systems with the power-law distribution ofthe intensity of pulsing signals.'}
1.11e+03 pattern=  1 task=  0 (u=33899)  {'docs': u'Previous work in the context of natural language querying of temporaldatabases has established a method to map automatically from a large subset ofEnglish time-related questions to suitable expressions of a temporal logic-likelanguage, called TOP. An algorithm to translate from TOP to the TSQL2 temporaldatabase language has also been defined. This paper shows how TOP expressionscould be translated into a simpler logic-like language, called BOT. BOT is veryclose to traditional first-order predicate logic (FOPL), and hence existingmethods to manipulate FOPL expressions can be exploited to interface totime-sensitive applications other than TSQL2 databases, maintaining theexisting English-to-TOP mapping.'}
1.12e+03 pattern=  0 task=  0 (u=33902)  {'docs': u'The article surveys a little of the history of the technology, sets out themain current theoretical approaches in brief, and discusses the on-goingopposition between theoretical and empirical approaches. It illustrates thesituation with some discussion of CONVERSE, a system that won the Loebner prizein 1997 and which displays features of both approaches.'}
3.13e+03 pattern=  0 task=  0 (u=33992)  {'docs': u"Contrarily to standard approaches to topic annotation, the technique used inthis work does not centrally rely on some sort of -- possibly statistical --keyword extraction. In fact, the proposed annotation algorithm uses a largescale semantic database -- the EDR Electronic Dictionary -- that provides aconcept hierarchy based on hyponym and hypernym relations. This concepthierarchy is used to generate a synthetic representation of the document byaggregating the words present in topically homogeneous document segments into aset of concepts best preserving the document's content.  This new extraction technique uses an unexplored approach to topic selection.Instead of using semantic similarity measures based on a semantic resource, thelater is processed to extract the part of the conceptual hierarchy relevant tothe document content. Then this conceptual hierarchy is searched to extract themost relevant set of concepts to represent the topics discussed in thedocument. Notice that this algorithm is able to extract generic concepts thatare not directly present in the document."}
2.95e+03 pattern=  0 task=  0 (u=34123)  {'docs': u'Predicate abstraction provides a powerful tool for verifying properties ofinfinite-state systems using a combination of a decision procedure for a subsetof first-order logic and symbolic methods originally developed for finite-statemodel checking. We consider models containing first-order state variables,where the system state includes mutable functions and predicates. Such a modelcan describe systems containing arbitrarily large memories, buffers, and arraysof identical processes. We describe a form of predicate abstraction thatconstructs a formula over a set of universally quantified variables to describeinvariant properties of the first-order state variables. We provide a formaljustification of the soundness of our approach and describe how it has beenused to verify several hardware and software designs, including adirectory-based cache coherence protocol.'}
2.08e+03 pattern=  0 task=  0 (u=34258)  {'docs': u"Previous works suggested the use of Branch and Bound techniques for findingthe optimal allocation in (multi-unit) combinatorial auctions. They remarkedthat Linear Programming could provide a good upper-bound to the optimalallocation, but they went on using lighter and less tight upper-boundheuristics, on the ground that LP was too time-consuming to be usedrepetitively to solve large combinatorial auctions. We present the results ofextensive experiments solving large (multi-unit) combinatorial auctionsgenerated according to distributions proposed by different researchers. Oursurprising conclusion is that Linear Programming is worth using. Investingalmost all of one's computing time in using LP to bound from above the value ofthe optimal solution in order to prune aggressively pays off. We present a wayto save on the number of calls to the LP routine and experimental resultscomparing different heuristics for choosing the bid to be considered next.Those results show that the ordering based on the square root of the size ofthe bids that was shown to be theoretically optimal in a previous paper by theauthors performs surprisingly better than others in practice. Choosing to dealfirst with the bid with largest coefficient (typically 1) in the optimalsolution of the relaxed LP problem, is also a good choice. The gap between thelower bound provided by greedy heuristics and the upper bound provided by LP istypically small and pruning is therefore extensive. For most distributions,auctions of a few hundred goods among a few thousand bids can be solved inpractice. All experiments were run on a PC under Matlab."}
2.14e+03 pattern=  0 task=  0 (u=34787)  {'docs': u"In 1974 Kolmogorov proposed a non-probabilistic approach to statistics andmodel selection. Let data be finite binary strings and models be finite sets ofbinary strings. Consider model classes consisting of models of given maximal(Kolmogorov) complexity. The ``structure function'' of the given data expressesthe relation between the complexity level constraint on a model class and theleast log-cardinality of a model in the class containing the data. We show thatthe structure function determines all stochastic properties of the data: forevery constrained model class it determines the individual best-fitting modelin the class irrespective of whether the ``true'' model is in the model classconsidered or not. In this setting, this happens {\\em with certainty}, ratherthan with high probability as is in the classical case. We precisely quantifythe goodness-of-fit of an individual model with respect to individual data. Weshow that--within the obvious constraints--every graph is realized by thestructure function of some data. We determine the (un)computability propertiesof the various functions contemplated and of the ``algorithmic minimalsufficient statistic.''"}
3.24e+03 pattern=  0 task=  0 (u=34852)  {'docs': u'Bounds on the entropy of patterns of sequences generated by independentlyidentically distributed (i.i.d.) sources are derived. A pattern is a sequenceof indices that contains all consecutive integer indices in increasing order offirst occurrence. If the alphabet of a source that generated a sequence isunknown, the inevitable cost of coding the unknown alphabet symbols can beexploited to create the pattern of the sequence. This pattern can in turn becompressed by itself. The bounds derived here are functions of the i.i.d.source entropy, alphabet size, and letter probabilities. It is shown that forlarge alphabets, the pattern entropy must decrease from the i.i.d. one. Thedecrease is in many cases more significant than the universal coding redundancybounds derived in prior works. The pattern entropy is confined between twobounds that depend on the arrangement of the letter probabilities in theprobability space. For very large alphabets whose size may be greater than thecoded pattern length, all low probability letters are packed into one symbol.The pattern entropy is upper and lower bounded in terms of the i.i.d. entropyof the new packed alphabet. Correction terms, which are usually negligible, areprovided for both upper and lower bounds.'}
3.15e+03 pattern=  0 task=  0 (u=35019)  {'docs': u"This note shows that split-2 bisimulation equivalence (also known as timedequivalence) affords a finite equational axiomatization over the processalgebra obtained by adding an auxiliary operation proposed by Hennessy in 1981to the recursion, relabelling and restriction free fragment of Milner'sCalculus of Communicating Systems. Thus the addition of a single binaryoperation, viz. Hennessy's merge, is sufficient for the finite equationalaxiomatization of parallel composition modulo this non-interleavingequivalence. This result is in sharp contrast to a theorem previously obtainedby the same authors to the effect that the same language is not finitely basedmodulo bisimulation equivalence."}
1.49e+03 pattern=  0 task=  0 (u=35098)  {'docs': u'The allocation of scarce spectral resources to support as many userapplications as possible while maintaining reasonable quality of service is afundamental problem in wireless communication. We argue that the problem isbest formulated in terms of decision theory. We propose a scheme that takesdecision-theoretic concerns (like preferences) into account and discuss thedifficulties and subtleties involved in applying standard techniques from thetheory of Markov Decision Processes (MDPs) in constructing an algorithm that isdecision-theoretically optimal. As an example of the proposed framework, weconstruct such an algorithm under some simplifying assumptions. Additionally,we present analysis and simulation results that show that our algorithm meetsits design goals. Finally, we investigate how far from optimal one well-knownheuristic is. The main contribution of our results is in providing insight andguidance for the design of near-optimal admission-control policies.'}
3.11e+03 pattern=  0 task=  0 (u=35312)  {'docs': u'We investigate how increasing the dimension of the array can help to drawsignals on cellular automata.We show the existence of a gap of constructiblesignals in any dimension. We exhibit two cellular automata in dimension 2 toshow that increasing the dimension allows to reduce the number of statesrequired for some constructions.'}
2.57e+03 pattern=  0 task=  0 (u=35639)  {'docs': u"In the context of the ATLAS experiment there is growing evidence of theimportance of different kinds of Meta-data including all the important detailsof the detector and data acquisition that are vital for the analysis of theacquired data. The Online BookKeeper (OBK) is a component of ATLAS onlinesoftware that stores all information collected while running the experiment,including the Meta-data associated with the event acquisition, triggering andstorage. The facilities for acquisition of control data within the on-linesoftware framework, together with a full functional Web interface, make the OBKa powerful tool containing all information needed for event analysis, includingan electronic log book.  In this paper we explain how OBK plays a role as one of the main collectorsand managers of Meta-data produced on-line, and we'll also focus on the Webfacilities already available. The usage of the web interface as an electronicrun logbook is also explained, together with the future extensions.  We describe the technology used in OBK development and how we arrived at thepresent level explaining the previous experience with various DBMStechnologies. The extensive performance evaluations that have been performedand the usage in the production environment of the ATLAS test beams are alsoanalysed."}
2.06e+03 pattern=  0 task=  0 (u=35697)  {'docs': u"This paper is aimed at providing a uniform framework for reasoning aboutbeliefs of multiple agents and their fusion. In the first part of the paper, wedevelop logics for reasoning about cautiously merged beliefs of agents withdifferent degrees of reliability. The logics are obtained by combining themulti-agent epistemic logic and multi-sources reasoning systems. Every orderingfor the reliability of the agents is represented by a modal operator, so we canreason with the merged results under different situations. The fusion iscautious in the sense that if an agent's belief is in conflict with those ofhigher priorities, then his belief is completely discarded from the mergedresult. We consider two strategies for the cautious merging of beliefs. In thefirst one, if inconsistency occurs at some level, then all beliefs at the lowerlevels are discarded simultaneously, so it is called level cutting strategy.For the second one, only the level at which the inconsistency occurs isskipped, so it is called level skipping strategy. The formal semantics andaxiomatic systems for these two strategies are presented. In the second part,we extend the logics both syntactically and semantically to cover some moresophisticated belief fusion and revision operators. While most existingapproaches treat belief fusion operators as meta-level constructs, theseoperators are directly incorporated into our object logic language. Thus it ispossible to reason not only with the merged results but also about the fusionprocess in our logics. The relationship of our extended logics with theconditional logics of belief revision is also discussed."}
2.06e+03 pattern=  0 task=  0 (u=35698)  {'docs': u'This article aims at clarifying the language and practice of scientificexperiment, mainly by hooking observability on calculability.'}
2.62e+03 pattern=  0 task=  0 (u=35879)  {'docs': u'OpenPGP, an IETF Proposed Standard based on PGP application, has its ownPublic Key Infrastructure (PKI) architecture which is different from the onebased on X.509, another standard from ITU. This paper describes the OpenPGPPKI; the historical perspective as well as its current use. The current OpenPGPPKI issues include the capability of a PGP keyserver and its performance. PGPkeyservers have been developed and operated by volunteers since the 1990s. Thekeyservers distribute, merge, and expire the OpenPGP public keys. Majorkeyserver managers from several countries have built the globally distributednetwork of PGP keyservers. However, the current PGP Public Keyserver (pksd) hassome limitations. It does not support fully the OpenPGP format so that it isneither expandable nor flexible, without any cluster technology. Finally weintroduce the project on the next generation OpenPGP public keyserver calledthe OpenPKSD, lead by Hironobu Suzuki, one of the authors, and funded byJapanese Information-technology Promotion Agency(IPA).'}
2.65e+03 pattern=  0 task=  0 (u=36126)  {'docs': u'The overall goal of this paper is to investigate the theoretical foundationsof algorithmic verification techniques for first order linear logicspecifications. The fragment of linear logic we consider in this paper is basedon the linear logic programming language called LO enriched with universallyquantified goal formulas. Although LO was originally introduced as atheoretical foundation for extensions of logic programming languages, it canalso be viewed as a very general language to specify a wide range ofinfinite-state concurrent systems.  Our approach is based on the relation between backward reachability andprovability highlighted in our previous work on propositional LO programs.Following this line of research, we define here a general framework for thebottom-up evaluation of first order linear logic specifications. The evaluationprocedure is based on an effective fixpoint operator working on a symbolicrepresentation of infinite collections of first order linear logic formulas.The theory of well quasi-orderings can be used to provide sufficient conditionsfor the termination of the evaluation of non trivial fragments of first orderlinear logic.'}
3.29e+03 pattern=  0 task=  0 (u=36170)  {'docs': u'One of the main obstacles to the wider use of the modern error-correctioncodes is that, due to the complex behavior of their decoding algorithms, nosystematic method which would allow characterization of the Bit-Error-Rate(BER) is known. This is especially true at the weak noise where many systemsoperate and where coding performance is difficult to estimate because of thediminishingly small number of errors. We show how the instanton method ofphysics allows one to solve the problem of BER analysis in the weak noise rangeby recasting it as a computationally tractable minimization problem.'}
2.23e+03 pattern=  0 task=  0 (u=36349)  {'docs': u"We present GUPU, a side-effect free environment specialized for programmingcourses. It seamlessly guides and supports students during all phases ofprogram development, covering specification, implementation, and programdebugging. GUPU features several innovations in this area. The specificationphase is supported by reference implementations augmented with diagnosticfacilities. During implementation, immediate feedback from test cases and fromvisualization tools helps the programmer's program understanding. A set ofslicing techniques narrows down programming errors. The whole process is guidedby a marking system."}
3.27e+03 pattern=  0 task=  0 (u=36457)  {'docs': u'A fertile field of research in theoretical computer science investigates therepresentation of general recursive functions in intensional type theories.Among the most successful approaches are: the use of wellfounded relations,implementation of operational semantics, formalization of domain theory, andinductive definition of domain predicates. Here, a different solution isproposed: exploiting coinductive types to model infinite computations. To everytype A we associate a type of partial elements Partial(A), coinductivelygenerated by two constructors: the first, return(a) just returns an elementa:A; the second, step(x), adds a computation step to a recursive elementx:Partial(A). We show how this simple device is sufficient to formalize allrecursive functions between two given types. It allows the definition of fixedpoints of finitary, that is, continuous, operators. We will compare thisapproach to different ones from the literature. Finally, we mention that theformalization, with appropriate structural maps, defines a strong monad.'}
3.27e+03 pattern=  0 task=  0 (u=36460)  {'docs': u'Motivated by the increasing prominence of loosely-coupled systems, such asmobile and sensor networks, which are characterised by intermittentconnectivity and volatile data, we study the tagging of data with so-calledexpiration times. More specifically, when data are inserted into a database,they may be tagged with time values indicating when they expire, i.e., whenthey are regarded as stale or invalid and thus are no longer considered part ofthe database. In a number of applications, expiration times are known and canbe assigned at insertion time. We present data structures and algorithms foronline management of data tagged with expiration times. The algorithms arebased on fully functional, persistent treaps, which are a combination of binarysearch trees with respect to a primary attribute and heaps with respect to asecondary attribute. The primary attribute implements primary keys, and thesecondary attribute stores expiration times in a minimum heap, thus keeping apriority queue of tuples to expire. A detailed and comprehensive experimentalstudy demonstrates the well-behavedness and scalability of the approach as wellas its efficiency with respect to a number of competitors.'}
3.24e+03 pattern=  0 task=  0 (u=36535)  {'docs': u'The problem of predicting a sequence $x_1,x_2,...$ generated by a discretesource with unknown statistics is considered. Each letter $x_{t+1}$ ispredicted using information on the word $x_1x_2... x_t$ only. In fact, thisproblem is a classical problem which has received much attention. Its historycan be traced back to Laplace. We address the problem where each $x_i$ belongsto some large (or even infinite) alphabet. A method is presented for which theprecision is greater than for known algorithms, where precision is estimated bythe Kullback-Leibler divergence. The results can readily be translated toresults about adaptive coding.'}
2.57e+03 pattern=  0 task=  0 (u=36556)  {'docs': u'Software dependence networks are shown to be scale-free and asymmetric. Wethen study how software components are affected by the failure of one of them,and the inverse problem of locating the faulty component. Software at alllevels is fragile with respect to the failure of a random single component.Locating a faulty component is easy if the failures only affect their nearestneighbors, while it is hard if the failures propagate further.'}
2.42e+03 pattern=  0 task=  0 (u=36730)  {'docs': u'Stochastic local search algorithms are frequently used to numerically solvehard combinatorial optimization or decision problems. We give numerical andapproximate analytical descriptions of the dynamics of such algorithms appliedto random satisfiability problems. We find two different dynamical regimes,depending on the number of constraints per variable: For low constraintness,the problems are solved efficiently, i.e. in linear time. For higherconstraintness, the solution times become exponential. We observe that thedynamical behavior is characterized by a fast equilibration and fluctuationsaround this equilibrium. If the algorithm runs long enough, an exponentiallyrare fluctuation towards a solution appears.'}
3.17e+03 pattern=  0 task=  0 (u=36731)  {'docs': u'In this report, we present a formal model of fair iteration of events for Bevent systems. The model is used to justify proof obligations for basicliveness properties and preservation under refinement of general livenessproperties. The model of fair iteration of events uses the dovetail operator,an operator proposed by Broy and Nelson to model fair choice. The proofs aremainly founded in fixpoint calculations of fair iteration of events and weakestprecondition calculus.'}
3.13e+03 pattern=  0 task=  0 (u=36733)  {'docs': u'In this paper we present an estimate of accuracy for a piecewise polynomialapproximation of a classical numerical solution to a non linear differentialproblem. We suppose the numerical solution U is computed using a grid with asmall linear step and interval time Tu, while the polynomial approximation V isan interpolation of the values of a numerical solution on a less fine grid andinterval time Tv << Tu. The estimate shows that the interpolant solution V canbe, under suitable hypotheses, a good approximation and in general itscomputational cost is much lower of the cost of the fine numerical solution. Wepresent two possible applications to linear case and periodic case.'}
3.17e+03 pattern=  0 task=  1 (u=36733)  {'docs': u'In this paper we present a possible model of adaptive grids for numericalresolution of differential problems, using physical or geometrical properties,as viscosity or velocity gradient of a moving fluid. The relation between thevalues of grid step and these entities is based on the mathematical schemeoffered by the model of scale-free networks, due to Barabasi, so that the stepcan be connected to the other variables by a constitutive relation. Someexamples and an application are discussed, showing that this approach can befurther developed for treatment of more complex situations.'}
3.17e+03 pattern=  0 task=  0 (u=36734)  {'docs': u'We introduce series-triangular graph embeddings and show how to partitionpoint sets with them. This result is then used to improve the upper bound onthe number of Steiner points needed to obtain compatible triangulations ofpoint sets. The problem is generalized to finding compatible triangulations formore than two point sets and we show that such triangulations can beconstructed with only a linear number of Steiner points added to each pointset.'}
3e+03 pattern=  0 task=  0 (u=36898)  {'docs': u'Wireless sensor networks offer the potential to span and monitor largegeographical areas inexpensively. Sensors, however, have significant powerconstraint (battery life), making communication very expensive. Anotherimportant issue in the context of sensor-based information systems is thatindividual sensor readings are inherently unreliable. In order to address thesetwo aspects, sensor database systems like TinyDB and Cougar enable in-networkdata aggregation to reduce the communication cost and improve reliability. Theexisting data aggregation techniques, however, are limited to relatively simpletypes of queries such as SUM, COUNT, AVG, and MIN/MAX. In this paper we proposea data aggregation scheme that significantly extends the class of queries thatcan be answered using sensor networks. These queries include (approximate)quantiles, such as the median, the most frequent data values, such as theconsensus value, a histogram of the data distribution, as well as rangequeries. In our scheme, each sensor aggregates the data it has received fromother sensors into a fixed (user specified) size message. We provide stricttheoretical guarantees on the approximation quality of the queries in terms ofthe message size. We evaluate the performance of our aggregation scheme bysimulation and demonstrate its accuracy, scalability and low resourceutilization for highly variable input data sets.'}
2.99e+03 pattern=  0 task=  0 (u=36900)  {'docs': u'Recently, many works focus on the implementation of collective communicationoperations adapted to wide area computational systems, like computational Gridsor global-computing. Due to the inherently heterogeneity of such environments,most works separate "clusters" in different hierarchy levels. to better modelthe communication. However, in our opinion, such works do not give enoughattention to the delimitation of such clusters, as they normally use thelocality or the IP subnet from the machines to delimit a cluster withoutverifying the "homogeneity" of such clusters. In this paper, we describe astrategy to gather network information from different local-area networks andto construct "logical homogeneous clusters", better suited to the performancemodelling.'}
2.99e+03 pattern=  0 task=  1 (u=36900)  {'docs': u'Recent works try to optimise collective communication in grid systemsfocusing mostly on the optimisation of communications among different clusters.We believe that intra-cluster collective communications should also beoptimised, as a way to improve the overall efficiency and to allow theconstruction of multi-level collective operations. Indeed, inside homogeneousclusters, a simple optimisation approach rely on the comparison from differentimplementation strategies, through their communication models. In this paper weevaluate this approach, comparing different implementation strategies withtheir predicted performances. As a result, we are able to choose thecommunication strategy that better adapts to each network environment.'}
2.86e+03 pattern=  0 task=  0 (u=36964)  {'docs': u'U-Datalog has been developed with the aim of providing a set-oriented logicalupdate language, guaranteeing update parallelism in the context of aDatalog-like language. In U-Datalog, updates are expressed by introducingconstraints (+p(X), to denote insertion, and [minus sign]p(X), to denotedeletion) inside Datalog rules. A U-Datalog program can be interpreted as a CLPprogram. In this framework, a set of updates (constraints) is satisfiable if itdoes not represent an inconsistent theory, that is, it does not require theinsertion and the deletion of the same fact. This approach resembles a verysimple form of negation. However, on the other hand, U-Datalog does not provideany mechanism to explicitly deal with negative information, resulting in alanguage with limited expressive power. In this paper, we provide a semantics,based on stratification, handling the use of negated atoms in U-Datalogprograms, and we show which problems arise in defining a compositionalsemantics.'}
2.86e+03 pattern=  0 task=  0 (u=36966)  {'docs': u'Evolvable hardware combines the powerful search capability of evolutionaryalgorithms with the flexibility of reprogrammable devices, thereby providing anatural framework for reconfiguration. This framework has generated an interestin using evolvable hardware for fault-tolerant systems because reconfigurationcan effectively deal with hardware faults whenever it is impossible to providespares. But systems cannot tolerate faults indefinitely, which meansreconfiguration does have a deadline. The focus of previous evolvable hardwareresearch relating to fault-tolerance has been primarily restricted to restoringfunctionality, with no real consideration of time constraints. In this paper weare concerned with evolvable hardware performing reconfiguration under deadlineconstraints. In particular, we investigate reconfigurable hardware thatundergoes intrinsic evolution. We show that fault recovery done by intrinsicreconfiguration has some restrictions, which designers cannot ignore.'}
2.36e+03 pattern=  0 task=  0 (u=36973)  {'docs': u'We present new results on the relation between purely symbolic context-freeparsing strategies and their probabilistic counter-parts. Such parsingstrategies are seen as constructions of push-down devices from grammars. Weshow that preservation of probability distribution is possible under twoconditions, viz. the correct-prefix property and the property of strongpredictiveness. These results generalize existing results in the literaturethat were obtained by considering parsing strategies in isolation. From ourgeneral results we also derive negative results on so-called generalized LRparsing.'}
2.86e+03 pattern=  0 task=  1 (u=36973)  {'docs': u"This is a tutorial on tabular parsing, on the basis of tabulation ofnondeterministic push-down automata. Discussed are Earley's algorithm, theCocke-Kasami-Younger algorithm, tabular LR parsing, the construction of parsetrees, and further issues."}
3.15e+03 pattern=  0 task=  0 (u=37054)  {'docs': u'Given a collection S of subsets of some set U, and M a subset of U, the setcover problem is to find the smallest subcollection C of S such that M is asubset of the union of the sets in C. While the general problem is NP-hard tosolve, even approximately, here we consider some geometric special cases, whereusually U = R^d. Extending prior results, we show that approximation algorithmswith provable performance exist, under a certain general condition: that for arandom subset R of S and function f(), there is a decomposition of the portionof U not covered by R into an expected f(|R|) regions, each region of aparticular simple form. We show that under this condition, a cover of sizeO(f(|C|)) can be found. Our proof involves the generalization of shallowcuttings to more general geometric situations. We obtain constant-factorapproximation algorithms for covering by unit cubes in R^3, for guarding aone-dimensional terrain, and for covering by similar-sized fat triangles inR^2. We also obtain improved approximation guarantees for fat triangles, ofarbitrary size, and for a class of fat objects.'}
2.69e+03 pattern=  0 task=  0 (u=37331)  {'docs': u'Relational representation of knowledge makes it possible to perform all thecomputations and decision making in a uniform relational way by means ofspecial relational compositions called triangle and square products. In thispaper some applications in manufacturing related to cost analysis aredescribed. Testing fuzzy relational structures for various relationalproperties allows us to discover dependencies, hierarchies, similarities, andequivalences of the attributes characterizing technological processes andmanufactured artifacts in their relationship to costs and performance.  A brief overview of mathematical aspects of BK-relational products is givenin Appendix 1 together with further references in the literature.'}
2.69e+03 pattern=  0 task=  1 (u=37331)  {'docs': u"The present paper extends generalized morphisms of relations into the realmof Monoidal Fuzzy Logics by first proving and then using relationalinequalities over pseudo-associative BK-products (compositions) of relations inthese logics.  In 1977 Bandler and Kohout introduced generalized homomorphism,proteromorphism, amphimorphism, forward and backward compatibility ofrelations, and non-associative and pseudo-associative products (compositions)of relations into crisp (non-fuzzy Boolean) theory of relations. This wasgeneralized later by Kohout to relations based on fuzzy Basic Logic systems(BL) of H\\'ajek and also for relational systems based on left-continuoust-norms.  The present paper is based on monoidal logics, hence it subsumes as specialcases the theories of generalized morphisms (etc.) based on the followingsystems of logics: BL systems (which include the well known Goedel, productlogic systems; Lukasiewicz logic and its extension to MV-algebras related toquantum logics), intuitionistic logics and linear logics."}
1.63e+03 pattern=  0 task=  0 (u=37403)  {'docs': u'Model-based reasoning is a central concept in current research intointelligent diagnostic systems. It is based on the assumption that sources ofincorrect behavior in technical devices can be located and identified via theexistence of a model describing the basic properties of components of a certainapplication domain. When actual data concerning the misbehavior of a systemcomposed from such components is available, a domain-independent diagnosisengine can be used to infer which parts of the system contribute to theobserved behavior. This paper describes the application of the model-basedapproach to the debugging of Java programs written in a subset of Java. We showhow a simple dependency model can be derived from a program, demonstrate theuse of the model for debugging and reducing the required user interactions,give a comparison of the functional dependency model with program slicing, andfinally discuss some current research issues.'}
3.22e+03 pattern=  0 task=  0 (u=38036)  {'docs': u'The problem of distributed or decentralized detection and estimation inapplications such as wireless sensor networks has often been considered in theframework of parametric models, in which strong assumptions are made about astatistical description of nature. In certain applications, such assumptionsare warranted and systems designed from these models show promise. However, inother scenarios, prior knowledge is at best vague and translating suchknowledge into a statistical model is undesirable. Applications such as thesepave the way for a nonparametric study of distributed detection and estimation.In this paper, we review recent work of the authors in which some elementarymodels for distributed learning are considered. These models are in the spiritof classical work in nonparametric statistics and are applicable to wirelesssensor networks.'}
1.23e+03 pattern=  0 task=  0 (u=38105)  {'docs': u"The logic of equality with uninterpreted functions (EUF) provides a means ofabstracting the manipulation of data by a processor when verifying thecorrectness of its control logic. By reducing formulas in this logic topropositional formulas, we can apply Boolean methods such as Ordered BinaryDecision Diagrams (BDDs) and Boolean satisfiability checkers to perform theverification.  We can exploit characteristics of the formulas describing the verificationconditions to greatly simplify the propositional formulas generated. Inparticular, we exploit the property that many equations appear only in positiveform. We can therefore reduce the set of interpretations of the functionsymbols that must be considered to prove that a formula is universally valid tothose that are ``maximally diverse.''  We present experimental results demonstrating the efficiency of this approachwhen verifying pipelined processors using the method proposed by Burch andDill."}
1.52e+03 pattern=  0 task=  1 (u=38105)  {'docs': u'We consider a variant of the Boolean satisfiability problem where a subset Eof the propositional variables appearing in formula Fsat encode a symmetric,transitive, binary relation over N elements. Each of these relationalvariables, e[i,j], for 1 <= i < j <= N, expresses whether or not the relationholds between elements i and j. The task is to either find a satisfyingassignment to Fsat that also satisfies all transitivity constraints over therelational variables (e.g., e[1,2] & e[2,3] ==> e[1,3]), or to prove that nosuch assignment exists. Solving this satisfiability problem is the final andmost difficult step in our decision procedure for a logic of equality withuninterpreted functions. This procedure forms the core of our tool forverifying pipelined microprocessors.  To use a conventional Boolean satisfiability checker, we augment the set ofclauses expressing Fsat with clauses expressing the transitivity constraints.We consider methods to reduce the number of such clauses based on the sparsestructure of the relational variables.  To use Ordered Binary Decision Diagrams (OBDDs), we show that for some setsE, the OBDD representation of the transitivity constraints has exponential sizefor all possible variable orderings. By considering only those relationalvariables that occur in the OBDD representation of Fsat, our experiments showthat we can readily construct an OBDD representation of the relevanttransitivity constraints and thus solve the constrained satisfiability problem.'}
1.5e+03 pattern=  0 task=  0 (u=38114)  {'docs': u'We derive tight bounds on cache misses for evaluation of explicit stenciloperators on structured grids. Our lower bound is based on the isoperimetricalproperty of the discrete octahedron. Our upper bound is based on good surfaceto volume ratio of a parallelepiped spanned by a reduced basis of the inter-ference lattice of a grid. Measurements show that our algorithm typicallyreduces the number of cache misses by factor of three relative to a compileroptimized code. We show that stencil calculations on grids whose interferencelattice have a short vector feature abnormally high numbers of cache misses. Wecall such grids unfavorable and suggest to avoid these in computations byappropriate padding. By direct measurements on MIPS R10000 we show a goodcorrelation of abnormally high cache misses and unfavorable three-dimensionalgrids.'}
2.71e+03 pattern=  0 task=  0 (u=38498)  {'docs': u'A programming tactic involving polyhedra is reported that has been widelyapplied in the polyhedral analysis of (constraint) logic programs. The methodenables the computations of convex hulls that are required for polyhedralanalysis to be coded with linear constraint solving machinery that is availablein many Prolog systems.  To appear in Theory and Practice of Logic Programming (TPLP)'}
1.73e+03 pattern=  0 task=  0 (u=38607)  {'docs': u"We present a development of parts of rate-distortion theory and pattern-matching algorithms for lossy data compression, centered around a lossy versionof the Asymptotic Equipartition Property (AEP). This treatment closelyparallels the corresponding development in lossless compression, a point ofview that was advanced in an important paper of Wyner and Ziv in 1989. In thelossless case we review how the AEP underlies the analysis of the Lempel-Zivalgorithm by viewing it as a random code and reducing it to the idealizedShannon code. This also provides information about the redundancy of theLempel-Ziv algorithm and about the asymptotic behavior of several relevantquantities. In the lossy case we give various versions of the statement of thegeneralized AEP and we outline the general methodology of its proof via largedeviations. Its relationship with Barron's generalized AEP is also discussed.The lossy AEP is applied to: (i) prove strengthened versions of Shannon'ssource coding theorem and universal coding theorems; (ii) characterize theperformance of mismatched codebooks; (iii) analyze the performance of pattern-matching algorithms for lossy compression; (iv) determine the first orderasymptotics of waiting times (with distortion) between stationary processes;(v) characterize the best achievable rate of weighted codebooks as an optimalsphere-covering exponent. We then present a refinement to the lossy AEP and useit to: (i) prove second order coding theorems; (ii) characterize which sourcesare easier to compress; (iii) determine the second order asymptotics of waitingtimes; (iv) determine the precise asymptotic behavior of longest match-lengths.Extensions to random fields are also given."}
3.22e+03 pattern=  0 task=  0 (u=38753)  {'docs': u'One-way hash chains have been used in many micropayment schemes due to theirsimplicity and efficiency. In this paper we introduce the notion ofmulti-dimensional hash chains, which is a new generalization of traditionalone-way hash chains. We show that this construction has storage-computationalcomplexity of O(logN) per chain element, which is comparable with the bestresult reported in recent literature. Based on multi-dimensional hash chains,we then propose two cash-like micropayment schemes, which have a number ofadvantages in terms of efficiency and security. We also point out some possibleimprovements to PayWord and similar schemes by using multi-dimensional hashchains'}
2.52e+03 pattern=  0 task=  0 (u=38951)  {'docs': u'It is impossible to obtain accurate frequencies from time signals of a veryshort duration. This is a common believe among contemporary physicists. Here Ipresent a practical way of extracting energies to a high precision from veryshort time signals produced by a quantum system. The product of time span ofthe signal and the precision of found energies is well bellow the limit imposedby the time-energy uncertainty relation.'}
1.21e+03 pattern=  0 task=  0 (u=39103)  {'docs': u"We show that, contrary to common belief, Dijkstra's self-stabilizing mutualexclusion algorithm on a ring [Dij74,Dij82] also stabilizes when the number ofstates per node is one less than the number of nodes on the ring."}
2.97e+03 pattern=  0 task=  0 (u=39232)  {'docs': u"We extend answer set semantics to deal with inconsistent programs (containingclassical negation), by finding a ``best'' answer set. Within the context ofinconsistent programs, it is natural to have a partial order on rules,representing a preference for satisfying certain rules, possibly at the cost ofviolating less important ones. We show that such a rule order induces a naturalorder on extended answer sets, the minimal elements of which we call preferredanswer sets. We characterize the expressiveness of the resulting semantics andshow that it can simulate negation as failure, disjunction and some otherformalisms such as logic programs with ordered disjunction. The approach isshown to be useful in several application areas, e.g. repairing database, whereminimal repairs correspond to preferred answer sets.  To appear in Theory and Practice of Logic Programming (TPLP)."}
2.92e+03 pattern=  0 task=  0 (u=39240)  {'docs': u'Estimation of Distribution Algorithms have been proposed as a new paradigmfor evolutionary optimization. This paper focuses on the parallelization ofEstimation of Distribution Algorithms. More specifically, the paper discusseshow to predict performance of parallel Mixed Bayesian Optimization Algorithm(MBOA) that is based on parallel construction of Bayesian networks withdecision trees. We determine the time complexity of parallel Mixed BayesianOptimization Algorithm and compare this complexity with experimental resultsobtained by solving the spin glass optimization problem. The empirical resultsfit well the theoretical time complexity, so the scalability and efficiency ofparallel Mixed Bayesian Optimization Algorithm for unknown instances of spinglass benchmarks can be predicted. Furthermore, we derive the guidelines thatcan be used to design effective parallel Estimation of Distribution Algorithmswith the speedup proportional to the number of variables in the problem.'}
1.96e+03 pattern=  0 task=  0 (u=39323)  {'docs': u"There is much discussion and debate about how to improve the security andprivacy of mobile communication systems, both voice and data. Most proposalsattempt to provide incremental improvements to systems that are deployed today.Indeed, only incremental improvements are possible, given the regulatory,technological, economic, and historical structure of the telecommunicationssystem. In this paper, we conduct a ``thought experiment'' to redesign themobile communications system to provide a high level of security and privacyfor the users of the system. We discuss the important requirements and how adifferent architecture might successfully satisfy them. In doing so, we hope toilluminate the possibilities for secure and private systems, as well as exploretheir real limits."}
1.98e+03 pattern=  0 task=  0 (u=39412)  {'docs': u'The control system (TICS) for the test interferometer being built to supportthe development of the Atacama Large Millimeter Array (ALMA)will itself be aprototype for the final ALMA array, providing a test for the distributedcontrol system under development. TICS will be based on the ALMA CommonSoftware (ACS) (developed at the European Southern Observatory), which providesCORBA-based services and a device management framework for the controlsoftware.  Simple device controllers will run on single board computers, one of which(known as an LCU) is located at each antenna; whereas complex, compound devicecontrollers may run on centrally located computers. In either circumstance,client programs may obtain direct CORBA references to the devices and theirproperties. Monitor and control requests are sent to devices or properties,which then process and forward the commands to the appropriate hardware devicesas required. Timing requirements are met by tagging commands with (future)timestamps synchronized to a timing pulse, which is regulated by a centralreference generator, and is distributed to all hardware devices in the array.Monitoring is provided through a publish/subscribe CORBA-based service.'}
2.95e+03 pattern=  0 task=  0 (u=39482)  {'docs': u"This paper discusses the problems and possibility of collecting bee dancedata in a linguistic \\textit{corpus} and use linguistic instruments such asZipf's law and entropy statistics to decide on the question whether the dancecarries information of any kind. We describe this against the historicalbackground of attempts to analyse non-human communication systems."}
2.57e+03 pattern=  0 task=  0 (u=40216)  {'docs': u'The simulation of CMS raw data requires the random selection of one hundredand fifty pileup events from a very large set of files, to be superimposed inmemory to the signal event. The use of ROOT I/O for that purpose is quiteunusual: the events are not read sequentially but pseudo-randomly, they are notprocessed one by one in memory but by bunches, and they do not contain orthodoxROOT objects but many foreign objects and templates. In this context, we havecompared the performance of ROOT containers versus the STL vectors, and the useof trees versus a direct storage of containers. The strategy with bestperformances is by far the one using clones within trees, but it stays hard totune and very dependant on the exact use-case. The use of STL vectors couldbring more easily similar performances in a future ROOT release.'}
2.93e+03 pattern=  0 task=  0 (u=40237)  {'docs': u'Imperative programmers often use cyclically linked trees in order to achieveO(1) navigation time to neighbours. Some logic programmers believe that cyclicterms are necessary to achieve the same in logic-based languages. An old butlittle-known technique provides O(1) time and space navigation without cycliclinks, in the form of reversible predicates. A small modification provides O(1)amortised time and space editing.'}
2.71e+03 pattern=  0 task=  0 (u=40459)  {'docs': u'We describe WSAT(cc), a local-search solver for computing models of theoriesin the language of propositional logic extended by cardinality atoms. WSAT(cc)is a processing back-end for the logic PS+, a recently proposed formalism foranswer-set programming.'}
2.35e+03 pattern=  0 task=  0 (u=40460)  {'docs': u'We consider the problem of laying out a tree with fixed parent/childstructure in hierarchical memory. The goal is to minimize the expected numberof block transfers performed during a search along a root-to-leaf path, subjectto a given probability distribution on the leaves. This problem was previouslyconsidered by Gil and Itai, who developed optimal but slow algorithms when theblock-transfer size B is known. We present faster but approximate algorithmsfor the same problem; the fastest such algorithm runs in linear time andproduces a solution that is within an additive constant of optimal.  In addition, we show how to extend any approximately optimal algorithm to thecache-oblivious setting in which the block-transfer size is unknown to thealgorithm. The query performance of the cache-oblivious layout is within aconstant factor of the query performance of the optimal known-block-sizelayout. Computing the cache-oblivious layout requires only logarithmically manycalls to the layout algorithm for known block size; in particular, thecache-oblivious layout can be computed in O(N lg N) time, where N is the numberof nodes.  Finally, we analyze two greedy strategies, and show that they have aperformance ratio between Omega(lg B / lg lg B) and O(lg B) when compared tothe optimal layout.'}
2.71e+03 pattern=  0 task=  1 (u=40460)  {'docs': u"We introduce top trees as a design of a new simpler interface for datastructures maintaining information in a fully-dynamic forest. We demonstratehow easy and versatile they are to use on a host of different applications. Forexample, we show how to maintain the diameter, center, and median of each treein the forest. The forest can be updated by insertion and deletion of edges andby changes to vertex and edge weights. Each update is supported in O(log n)time, where n is the size of the tree(s) involved in the update. Also, we showhow to support nearest common ancestor queries and level ancestor queries withrespect to arbitrary roots in O(log n) time. Finally, with marked and unmarkedvertices, we show how to compute distances to a nearest marked vertex. Thelater has applications to approximate nearest marked vertex in general graphs,and thereby to static optimization problems over shortest path metrics.  Technically speaking, top trees are easily implemented either withFrederickson's topology trees [Ambivalent Data Structures for Dynamic2-Edge-Connectivity and k Smallest Spanning Trees, SIAM J. Comput. 26 (2) pp.484-538, 1997] or with Sleator and Tarjan's dynamic trees [A Data Structure forDynamic Trees. J. Comput. Syst. Sc. 26 (3) pp. 362-391, 1983]. However, weclaim that the interface is simpler for many applications, and indeed our newbounds are quadratic improvements over previous bounds where they exist."}
2.21e+03 pattern=  0 task=  0 (u=40680)  {'docs': u"Motivated by Hubert's segmentation procedure we discuss the application ofhidden Markov models (HMM) to the segmentation of hydrological and enviromentaltime series. We use a HMM algorithm which segments time series of severalhundred terms in a few seconds and is computationally feasible for even longertime series. The segmentation algorithm computes the Maximum Likelihoodsegmentation by use of an expectation / maximization iteration. We rigorouslyprove algorithm convergence and use numerical experiments, involvingtemperature and river discharge time series, to show that the algorithm usuallyconverges to the globally optimal segmentation. The relation of the proposedalgorithm to Hubert's segmentation procedure is also discussed."}
3.05e+03 pattern=  0 task=  0 (u=40839)  {'docs': u'Fibonacci connection between non-decreasing sequences of positive integersproducing maximum height Huffman trees and the Wythoff array has been proved.'}
3.05e+03 pattern=  0 task=  0 (u=40840)  {'docs': u'Emergent behaviors are in the focus of recent research interest. It is thenof considerable importance to investigate what optimizations suit the learningand prediction of chaotic systems, the putative candidates for emergence. Wehave compared L1 and L2 regularizations on predicting chaotic time series usinglinear recurrent neural networks. The internal representation and the weightsof the networks were optimized in a unifying framework. Computational tests ondifferent problems indicate considerable advantages for the L1 regularization:It had considerably better learning time and better interpolating capabilities.We shall argue that optimization viewed as a maximum likelihood estimationjustifies our results, because L1 regularization fits heavy-taileddistributions -- an apparently general feature of emergent systems -- better.'}
2.75e+03 pattern=  0 task=  0 (u=40955)  {'docs': u'In this paper we consider the problem of proving properties of infinitebehaviour of formalisms suitable to describe (infinite state) systems withrecursion and parallelism. As a formal setting, we consider the framework ofProcess Rewriting Systems (PRSs). For a meaningfull fragment of PRSs, allowingto accommodate both Pushdown Automata and Petri Nets, we state decidabilityresults for a class of properties about infinite derivations (infinite termrewritings). The given results can be exploited for the automatic verificationof some classes of linear time properties of infinite state systems describedby PRSs. In order to exemplify the assessed results, we introduce a meaningfulautomaton based formalism which allows to express both recursion andmulti--treading.'}
3.12e+03 pattern=  0 task=  0 (u=40995)  {'docs': u'Recently, a quasi-orthogonal space-time block code (QSTBC) capable ofachieving a significant fraction of the outage mutual information of amultiple-input-multiple output (MIMO) wireless communication system for thecase of four transmit and one receive antennas was proposed. We generalizethese results to $n_T=2^n$ transmit and an arbitrary number of receive antennas$n_R$. Furthermore, we completely characterize the structure of the equivalentchannel for the general case and show that for all $n_T=2^n$ and $n_R$ theeigenvectors of the equivalent channel are fixed and independent from thechannel realization. Furthermore, the eigenvalues of the equivalent channel areindependent identically distributed random variables each following anoncentral chi-square distribution with $4n_R$ degrees of freedom.  Based on these important insights into the structure of the QSTBC, we derivean analytical lower bound for the fraction of outage probability achieved withQSTBC and show that this bound is tight for low signal-to-noise-ratios (SNR)values and also for increasing number of receive antennas. We also present anupper bound, which is tight for high SNR values and derive analyticalexpressions for the case of four transmit antennas. Finally, by utilizing thespecial structure of the QSTBC we propose a new transmit strategy, whichdecouples the signals transmitted from different antennas in order to detectthe symbols separately with a linear ML-detector rather than joint detection,an up to now only known advantage of orthogonal space-time block codes (OSTBC).'}
3.12e+03 pattern=  0 task=  0 (u=41002)  {'docs': u'We study the isomorphic implication problem for Boolean constraints. We showthat this is a natural analog of the subgraph isomorphism problem. We provethat, depending on the set of constraints, this problem is in P, NP-complete,or NP-hard, coNP-hard, and in parallel access to NP. We show how to extend theNP-hardness and coNP-hardness to hardness for parallel access to NP for somecases, and conjecture that this can be done in all cases.'}
3.23e+03 pattern=  0 task=  0 (u=41688)  {'docs': u"Attempts to understand the consequence of any individual scientist's activitywithin the long-term trajectory of science is one of the most difficultquestions within the philosophy of science. Because scientific publicationsplay such as central role in the modern enterprise of science, bibliometrictechniques which measure the ``impact'' of an individual publication as afunction of the number of citations it receives from subsequent authors haveprovided some of the most useful empirical data on this question. Untilrecently, Thompson/ISI has provided the only source of large-scale ``inverted''bibliographic data of the sort required for impact analysis. In the end of2004, Google introduced a new service, GoogleScholar, making much of this samedata available. Here we analyze 203 publications, collectively cited by morethan 4000 other publications. We show surprisingly good agreement between datacitation counts provided by the two services. Data quality across the systemsis analyzed, and potentially useful complementarities between are considered.The additional robustness offered by multiple sources of such data promises toincrease the utility of these measurements as open citation protocols and openaccess increase their impact on electronic scientific publication practices."}
2.22e+03 pattern=  0 task=  0 (u=41695)  {'docs': u'We derive analytical solutions for p-spin models with finite connectivity atzero temperature. These models are the statistical mechanics equivalent ofp-XORSAT problems in theoretical computer science. We give a fullcharacterization of the phase diagram: location of the phase transitions(static and dynamic), together with a description of the clustering phenomenontaking place in configurational space. We use two alternative methods: thecavity approach and a rigorous derivation.'}
2.75e+03 pattern=  0 task=  0 (u=41862)  {'docs': u"Logic programs P and Q are strongly equivalent if, given any program R,programs P union R and Q union R are equivalent (that is, have the same answersets). Strong equivalence is convenient for the study of equivalenttransformations of logic programs: one can prove that a local change is correctwithout considering the whole program. Lifschitz, Pearce and Valverde showedthat Heyting's logic of here-and-there can be used to characterize strongequivalence for logic programs with nested expressions (which subsume thebetter-known extended disjunctive programs). This note considers a simpler,more direct characterization of strong equivalence for such programs, and showsthat it can also be applied without modification to the weight constraintprograms of Niemela and Simons. Thus, this characterization of strongequivalence is convenient for the study of equivalent transformations of logicprograms written in the input languages of answer set programming systems dlvand smodels. The note concludes with a brief discussion of results that can beused to automate reasoning about strong equivalence, including a novel encodingthat reduces the problem of deciding the strong equivalence of a pair of weightconstraint programs to that of deciding the inconsistency of a weightconstraint program."}
1.84e+03 pattern=  0 task=  0 (u=41865)  {'docs': u'A formal consideration in this paper is given for the essential notations tocharacterize the object that is distinguished in a problem domain. The distinctobject is represented by another idealized object, which is a schematicelement. When the existence of an element is significant, then a class of thesepartial elements is dropped down into actual, potential and virtual objects.The potential objects are gathered into the variable domains which are theextended ranges for unbound variables. The families of actual objects are shownto be parameterized with the types and events. The transitions between eventsare shown to be driven by the scripts. A computational framework arises whichis described by the commutative diagrams.'}
1.91e+03 pattern=  0 task=  0 (u=42361)  {'docs': u'We describe a family of MPI applications we call the Parallel Unix Commands.These commands are natural parallel versions of common Unix user commands suchas ls, ps, and find, together with a few similar commands particular to theparallel environment. We describe the design and implementation of theseprograms and present some performance results on a 256-node Linux cluster. TheParallel Unix Commands are open source and freely available.'}
2.9e+03 pattern=  0 task=  0 (u=42491)  {'docs': u'Triangle strips have been widely used for efficient rendering. It isNP-complete to test whether a given triangulated model can be represented as asingle triangle strip, so many heuristics have been proposed to partitionmodels into few long strips. In this paper, we present a new algorithm forcreating a single triangle loop or strip from a triangulated model. Our methodapplies a dual graph matching algorithm to partition the mesh into cycles, andthen merges pairs of cycles by splitting adjacent triangles when necessary. Newvertices are introduced at midpoints of edges and the new triangles thus formedare coplanar with their parent triangles, hence the visual fidelity of thegeometry is not changed. We prove that the increase in the number of trianglesdue to this splitting is 50% in the worst case, however for all models wetested the increase was less than 2%. We also prove tight bounds on the numberof triangles needed for a single-strip representation of a model with holes onits boundary. Our strips can be used not only for efficient rendering, but alsofor other applications including the generation of space filling curves on amanifold of any arbitrary topology.'}
2.96e+03 pattern=  0 task=  0 (u=42573)  {'docs': u'Real-time heuristic search is a popular model of acting and learning inintelligent autonomous agents. Learning real-time search agents improve theirperformance over time by acquiring and refining a value function guiding theapplication of their actions. As computing the perfect value function istypically intractable, a heuristic approximation is acquired instead. Moststudies of learning in real-time search (and reinforcement learning) assumethat a simple value-function-greedy policy is used to select actions. This isin contrast to practice, where high-performance is usually attained byinterleaving planning and acting via a lookahead search of a non-trivial depth.In this paper, we take a step toward bridging this gap and propose a novelalgorithm that (i) learns a heuristic function to be used specifically with alookahead-based policy, (ii) selects the lookahead depth adaptively in eachstate, (iii) gives the user control over the trade-off between exploration andexploitation. We extensively evaluate the algorithm in the sliding tile puzzletestbed comparing it to the classical LRTA* and the more recent weighted LRTA*,bounded LRTA*, and FALCONS. Improvements of 5 to 30 folds in convergence speedare observed.'}
2.58e+03 pattern=  0 task=  0 (u=42777)  {'docs': u"A large computer program is typically divided into many hundreds or eventhousands of smaller units, whose logical connections define a network in anatural way. This network reflects the internal structure of the program, anddefines the ``information flow'' within the program. We show that, (1) due toits growth in time this network displays a scale-free feature in that theprobability of the number of links at a node obeys a power-law distribution,and (2) as a result of performance optimization of the program the network hasa small-world structure. We believe that these features are generic for largecomputer programs. Our work extends the previous studies on growing networks,which have mostly been for physical networks, to the domain of computersoftware."}
3.22e+03 pattern=  0 task=  0 (u=42877)  {'docs': u'Most MANET (Mobile Ad hoc NETwork) research assumes idealized propagationmodels. Experimental results have shown significant divergence from simulationresults due to the effect of signal fading in realistic wireless communicationchannels. In this paper, we characterize the impact of fading on protocolperformance. We first study the effect of fading on MAC performance and showthat its effect can be dominating. One of our important conclusions is thateliminating RTS/CTS packets results in more effective operation under fading.We also identify an unfairness problem that arises due to backoffs in thepresence of fading. Moreover, fading results in several subtle interactionsbetween the MAC and routing layers. We identify several of these problems andmake observations about effective approaches for addressing them. For example,the criteria for determining the best path should not only consider the linkstatus but also the link order. In addition, because routing protocols rely onMAC level transmission failure (when the retry limit is exceeded), routefailure errors are often generated unnecessarily. Finally, because MAC levelbroadcasts are unreliable, they are especially vulnerable to fading. We analyzethese effects and outline preliminary solutions to them.'}
3.09e+03 pattern=  0 task=  0 (u=42897)  {'docs': u'We present a framework for extending the functionality of LDAP servers fromtheir typical use as a public directory in public key infrastructures. In thisframework the LDAP servers are used for administrating infrastructureprocesses. One application of this framework is a method for providingproof-of-possession, especially in the case of encryption keys. Another one isthe secure delivery of software personal security environments.'}
3.01e+03 pattern=  0 task=  0 (u=43161)  {'docs': u'Li and Wu proposed Rule 2, a localized approximation algorithm that attemptsto find a small connected dominating set in a graph. Here we study theasymptotic performance of Rule 2 on random unit disk graphs formed from nrandom points in an s_n by s_n square region of the plane. If s_n is below thethreshold for connectivity, then Rule 2 produces a dominating set whoseexpected size is O(n/(loglog n)^{3/2}). We conjecture that this bound is notoptimal.'}
3.01e+03 pattern=  0 task=  0 (u=43230)  {'docs': u"We study the problem of disseminating a piece of information through all thenodes of a network, given that it is known originally only to a single node. Inthe absence of any structural knowledge on the network other than the nodes'neighborhoods, this problem is traditionally solved by flooding all thenetwork's edges. We analyze a recently introduced probabilistic algorithm forflooding and give an alternative probabilistic heuristic that can lead to somecost-effective improvements, like better trade-offs between the message andtime complexities involved. We analyze the two algorithms both mathematicallyand by means of simulations, always within a random-graph framework andconsidering relevant node-degree distributions."}
3.1e+03 pattern=  0 task=  1 (u=43230)  {'docs': u"We study the use of local heuristics to determine spanning subgraphs for usein the dissemination of information in complex networks. We introduce twodifferent heuristics and analyze their behavior in giving rise to spanningsubgraphs that perform well in terms of allowing every node of the network tobe reached, of requiring relatively few messages and small node bandwidth forinformation dissemination, and also of stretching paths with respect to theunderlying network only modestly. We contribute a detailed mathematicalanalysis of one of the heuristics and provide extensive simulation results onrandom graphs for both of them. These results indicate that, within certainlimits, spanning subgraphs are indeed expected to emerge that perform well inrespect to all requirements. We also discuss the spanning subgraphs' inherentresilience to failures and adaptability to topological changes."}
2.87e+03 pattern=  0 task=  0 (u=43232)  {'docs': u"For an intelligent agent to be truly autonomous, it must be able to adapt itsrepresentation to the requirements of its task as it interacts with the world.Most current approaches to on-line feature extraction are ad hoc; in contrast,this paper presents an algorithm that bases judgments of state compatibilityand state-space abstraction on principled criteria derived from thepsychological principle of cognitive economy. The algorithm incorporates anactive form of Q-learning, and partitions continuous state-spaces by mergingand splitting Voronoi regions. The experiments illustrate a new methodology fortesting and comparing representations by means of learning curves. Results fromthe puck-on-a-hill task demonstrate the algorithm's ability to learn effectiverepresentations, superior to those produced by some other, well-known, methods."}
1.58e+03 pattern=  0 task=  0 (u=43377)  {'docs': u'Rabi and Sherman present a cryptographic paradigm based on associative,one-way functions that are strong (i.e., hard to invert even if one of theirarguments is given) and total. Hemaspaandra and Rothe proved that such powerfulone-way functions exist exactly if (standard) one-way functions exist, thusshowing that the associative one-way function approach is as plausible asprevious approaches. In the present paper, we study the degree of ambiguity ofone-way functions. Rabiand Sherman showed that no associative one-way function(over a universe having at least two elements) can be unambiguous (i.e.,one-to-one). Nonetheless, we prove that if standard, unambiguous, one-wayfunctions exist, then there exist strong, total, associative, one-way functionsthat are $\\mathcal{O}(n)$-to-one. This puts a reasonable upper bound on theambiguity.'}
1.43e+03 pattern=  0 task=  0 (u=43396)  {'docs': u'The paper has established and verified the theory prevailing widely amongimage and pattern recognition specialists that the bottom-up indirect regionalmatching process is the more stable and the more robust than the globalmatching process against concentrated types of noise represented by clutter,outlier or occlusion in the imagery. We have demonstrated this by analyzing theeffect of concentrated noise on a typical decision making process of asimplified two candidate voting model where our theorem establishes the lowerbounds to a critical breakdown point of election (or decision) result by thebottom-up matching process are greater than the exact bound of the globalmatching process implying that the former regional process is capable ofaccommodating a higher level of noise than the latter global process before theresult of decision overturns. We present a convincing experimental verificationsupporting not only the theory by a white-black flag recognition problem in thepresence of localized noise but also the validity of the conjecture by a facialrecognition problem that the theorem remains valid for other decision makingprocesses involving an important dimension-reducing transform such as principalcomponent analysis or a Gabor transform.'}
1.72e+03 pattern=  0 task=  0 (u=44083)  {'docs': u"Rainfall in Kerala State, the southern part of Indian Peninsula in particularis caused by the two monsoons and the two cyclones every year. In general,climate and rainfall are highly nonlinear phenomena in nature giving rise towhat is known as the `butterfly effect'. We however attempt to train an ABFneural network on the time series rainfall data and show for the first timethat in spite of the fluctuations resulting from the nonlinearity in thesystem, the trends in the rainfall pattern in this corner of the globe haveremained unaffected over the past 87 years from 1893 to 1980. We alsosuccessfully filter out the chaotic part of the system and illustrate that itseffects are marginal over long term predictions."}
2.1e+03 pattern=  0 task=  0 (u=44211)  {'docs': u'We introduce a methodology and framework for expressing general preferenceinformation in logic programming under the answer set semantics. An orderedlogic program is an extended logic program in which rules are named by uniqueterms, and in which preferences among rules are given by a set of atoms of forms < t where s and t are names. An ordered logic program is transformed into asecond, regular, extended logic program wherein the preferences are respected,in that the answer sets obtained in the transformed program correspond with thepreferred answer sets of the original program. Our approach allows thespecification of dynamic orderings, in which preferences can appear arbitrarilywithin a program. Static orderings (in which preferences are external to alogic program) are a trivial restriction of the general dynamic case. First, wedevelop a specific approach to reasoning with preferences, wherein thepreference ordering specifies the order in which rules are to be applied. Wethen demonstrate the wide range of applicability of our framework by showinghow other approaches, among them that of Brewka and Eiter, can be capturedwithin our framework. Since the result of each of these transformations is anextended logic program, we can make use of existing implementations, such asdlv and smodels. To this end, we have developed a publicly available compileras a front-end for these programming systems.'}
2.99e+03 pattern=  0 task=  0 (u=44534)  {'docs': u'We give the logical description of a new kind of quantum measurement that isa reversible operation performed by an hypothetical insider observer, or, whichis the same, a quantum measurement made in a quantum space background, like thefuzzy sphere. The result is that the non-contradiction and the excluded middleprinciples are both invalidated, leading to a paraconsistent, symmetric logic.Our conjecture is that, in this setting, one can develop the adequate logic ofquantum computing. The role of standard quantum logic is then confined todescribe the projective measurement scheme.'}
3.13e+03 pattern=  0 task=  1 (u=44534)  {'docs': u"We investigate the internal logic of a quantum computer with two qubits, inthe two particular cases of non-entanglement (separable states) and maximalentanglement (Bell's states). To this aim, we consider an internal (reversible)measurement which preserves the probabilities by mirroring the states. We thenobtain logical judgements for both cases of separable and Bell's states."}
  247 pattern=  1 task=  0 (u=44641)  {'docs': u'This thesis addresses automatic lexical error recovery and tokenization ofcorrupt text input. We propose a technique that can automatically correctmisspellings, segmentation errors and real-word errors in a unified frameworkthat uses both a model of language production and a model of the typingbehavior, and which makes tokenization part of the recovery process.  The typing process is modeled as a noisy channel where Hidden Markov Modelsare used to model the channel characteristics. Weak statistical language modelsare used to predict what sentences are likely to be transmitted through thechannel. These components are held together in the Token Passing frameworkwhich provides the desired tight coupling between orthographic pattern matchingand linguistic expectation.  The system, CTR (Connected Text Recognition), has been tested on two corporaderived from two different applications, a natural language dialogue system anda transcription typing scenario. Experiments show that CTR can automaticallycorrect a considerable portion of the errors in the test sets withoutintroducing too much noise. The segmentation error correction rate is virtuallyfaultless.'}
2.84e+03 pattern=  0 task=  0 (u=44676)  {'docs': u'Lambda Prolog is known to be well-suited for expressing and implementinglogics and inference systems. We show that lemmas and definitions in suchlogics can be implemented with a great economy of expression. We encode ahigher-order logic using an encoding that maps both terms and types of theobject logic (higher-order logic) to terms of the metalanguage (Lambda Prolog).We discuss both the Terzo and Teyjus implementations of Lambda Prolog. We alsoencode the same logic in Twelf and compare the features of these twometalanguages for our purposes.'}
1.12e+03 pattern=  0 task=  0 (u=44706)  {'docs': u"For over a decade, researchers in formal methods tried to create formalismsthat permit natural specification of systems and allow mathematical reasoningabout their correctness. The availability of fully-automated reasoning toolsenables more non-specialists to use formal methods effectively --- theirresponsibility reduces to just specifying the model and expressing the desiredproperties. Thus, it is essential that these properties be represented in alanguage that is easy to use and sufficiently expressive.  Linear-time temporal logic is a formalism that has been extensively used byresearchers for specifying properties of systems. When such properties areclosed under stuttering, i.e. their interpretation is not modified bytransitions that leave the system in the same state, verification tools canutilize a partial-order reduction technique to reduce the size of the model andthus analyze larger systems. If LTL formulas do not contain the ``next''operator, the formulas are closed under stuttering, but the resulting languageis not expressive enough to capture many important properties, e.g., propertiesinvolving events. Determining if an arbitrary LTL formula is closed understuttering is hard --- it has been proven to be PSPACE-complete.  In this paper we relax the restriction on LTL that guarantees closure understuttering, introduce the notion of edges in the context of LTL, and providetheorems that enable syntactic reasoning about closure under stuttering of LTLformulas."}
3.28e+03 pattern=  0 task=  0 (u=44750)  {'docs': u'We develop a pseudo-metric analogue of bisimulation for generalizedsemi-Markov processes. The kernel of this pseudo-metric corresponds tobisimulation; thus we have extended bisimulation for continuous-timeprobabilistic processes to a much broader class of distributions thanexponential distributions. This pseudo-metric gives a useful handle onapproximate reasoning in the presence of numerical information -- such asprobabilities and time -- in the model. We give a fixed point characterizationof the pseudo-metric. This makes available coinductive reasoning principles forreasoning about distances. We demonstrate that our approach is insensitive topotentially ad hoc articulations of distance by showing that it is intrinsic toan underlying uniformity. We provide a logical characterization of thisuniformity using a real-valued modal logic. We show that several quantitativeproperties of interest are continuous with respect to the pseudo-metric. Thus,if two processes are metrically close, then observable quantitative propertiesof interest are indeed close.'}
3.28e+03 pattern=  0 task=  0 (u=44752)  {'docs': u'A major challenge for the realization of intelligent robots is to supply themwith cognitive abilities in order to allow ordinary users to program themeasily and intuitively. One way of such programming is teaching work tasks byinteractive demonstration. To make this effective and convenient for the user,the machine must be capable to establish a common focus of attention and beable to use and integrate spoken instructions, visual perceptions, andnon-verbal clues like gestural commands. We report progress in building ahybrid architecture that combines statistical methods, neural networks, andfinite state machines into an integrated system for instructing grasping tasksby man-machine interaction. The system combines the GRAVIS-robot for visualattention and gestural instruction with an intelligent interface for speechrecognition and linguistic interpretation, and an modality fusion module toallow multi-modal task-oriented man-machine communication with respect todextrous robot manipulation of objects.'}
3.24e+03 pattern=  0 task=  0 (u=44827)  {'docs': u'We present methods that generate cooperative strategies for multi-vehiclecontrol problems using a decomposition approach. By introducing a set of tasksto be completed by the team of vehicles and a task execution method for eachvehicle, we decomposed the problem into a combinatorial component and acontinuous component. The continuous component of the problem is captured bytask execution, and the combinatorial component is captured by task assignment.In this paper, we present a solver for task assignment that generatesnear-optimal assignments quickly and can be used in real-time applications. Tomotivate our methods, we apply them to an adversarial game between two teams ofvehicles. One team is governed by simple rules and the other by our algorithms.In our study of this game we found phase transitions, showing that the taskassignment problem is most difficult to solve when the capabilities of theadversaries are comparable. Finally, we implement our algorithms in amulti-level architecture with a variable replanning rate at each level toprovide feedback on a dynamically changing and uncertain environment.'}
3.27e+03 pattern=  0 task=  1 (u=44827)  {'docs': u'Mixed integer linear programming (MILP) is a powerful tool for planning andcontrol problems because of its modeling capability and the availability ofgood solvers. However, for large models, MILP methods suffer computationally.In this paper, we present iterative MILP algorithms that address this issue. Weconsider trajectory generation problems with obstacle avoidance requirementsand minimum time trajectory generation problems. The algorithms use fewerbinary variables than standard MILP methods and require less computationaleffort.'}
3.24e+03 pattern=  0 task=  0 (u=44828)  {'docs': u'Steganography is about how to send secret message covertly. And the purposeof steganalysis is to not only detect the existence of the hidden message butalso extract it. So far there have been many reliable detecting methods onvarious steganographic algorithms, while there are few approaches that canextract the hidden information. In this paper, the difficulty of extractinghidden information, which is essentially a kind of privacy, is analyzed withinformation-theoretic method in the terms of unicity distance of steganographickey (abbreviated stego key). A lower bound for the unicity distance isobtained, which shows the relations between key rate, message rate, hidingcapacity and difficulty of extraction. Furthermore the extracting attack tosteganography is viewed as a special kind of cryptanalysis, and an effectivemethod on recovering the stego key of popular LSB replacing steganography inspatial images is presented by combining the detecting technique ofsteganalysis and correlation attack of cryptanalysis together. The analysis forthis method and experimental results on steganographic software ``Hide and Seek4.1" are both accordant with the information-theoretic conclusion.'}
2.21e+03 pattern=  0 task=  0 (u=44886)  {'docs': u'In this note, we fill a gap in the proof of the heuristic GCD in themultivariate case made by Char, Geddes and Gonnet (JSC 1989) and give someadditionnal information on this method.'}
3.21e+03 pattern=  0 task=  0 (u=44926)  {'docs': u"A systematic, language-independent method of finding a minimal set of pathscovering the code of a sequential program is proposed for application in WhiteBox testing. Execution of all paths from the set ensures also statementcoverage. Execution fault marks problematic areas of the code. The methodstarts from a UML activity diagram of a program. The diagram is transformedinto a directed graph: graph's nodes substitute decision and action points;graph's directed edges substitute action arrows.  The number of independent paths equals easy-to-compute cyclomatic complexityof the graph. Association of a vector to each path creates a path vector space.Independence of the paths is equivalent to linear independence of the vectors.It is sufficient to test any base of the path space to complete the procedure.An effective algorithm for choosing the base paths is presented."}
2.69e+03 pattern=  0 task=  0 (u=44954)  {'docs': u"We present a general method for fault localization based on abstracting overprogram traces, and a tool that implements the method using Ernst's notion ofpotential invariants. Our experiments so far have been unsatisfactory,suggesting that further research is needed before invariants can be used tolocate faults."}
1.66e+03 pattern=  0 task=  0 (u=44999)  {'docs': u'Slicing is a program analysis technique originally developed for imperativelanguages. It facilitates understanding of data flow and debugging.  This paper discusses slicing of Constraint Logic Programs. Constraint LogicProgramming (CLP) is an emerging software technology with a growing number ofapplications. Data flow in constraint programs is not explicit, and for thisreason the concepts of slice and the slicing techniques of imperative languagesare not directly applicable.  This paper formulates declarative notions of slice suitable for CLP. Theyprovide a basis for defining slicing techniques (both dynamic and static) basedon variable sharing. The techniques are further extended by using groundnessinformation.  A prototype dynamic slicer of CLP programs implementing the presented ideasis briefly described together with the results of some slicing experiments.'}
2.19e+03 pattern=  0 task=  0 (u=45344)  {'docs': u'Recent studies on frequent itemset mining algorithms resulted in significantperformance improvements. However, if the minimal support threshold is set toolow, or the data is highly correlated, the number of frequent itemsets itselfcan be prohibitively large. To overcome this problem, recently severalproposals have been made to construct a concise representation of the frequentitemsets, instead of mining all frequent itemsets. The main goal of this paperis to identify redundancies in the set of all frequent itemsets and to exploitthese redundancies in order to reduce the result of a mining operation. Wepresent deduction rules to derive tight bounds on the support of candidateitemsets. We show how the deduction rules allow for constructing a minimalrepresentation for all frequent itemsets. We also present connections betweenour proposal and recent proposals for concise representations and we give theresults of experiments on real-life datasets that show the effectiveness of thededuction rules. In fact, the experiments even show that in many cases, firstmining the concise representation, and then creating the frequent itemsets fromthis representation outperforms existing frequent set mining algorithms.'}
1.94e+03 pattern=  0 task=  0 (u=45725)  {'docs': u"This paper focuses on the branching process for solving any constraintsatisfaction problem (CSP). A parametrised schema is proposed that (withsuitable instantiations of the parameters) can solve CSP's on both finite andinfinite domains. The paper presents a formal specification of the schema and astatement of a number of interesting properties that, subject to certainconditions, are satisfied by any instances of the schema.  It is also shown that the operational procedures of many constraint systemsincluding cooperative systems) satisfy these conditions.  Moreover, the schema is also used to solve the same CSP in different ways bymeans of different instantiations of its parameters."}
2.02e+03 pattern=  0 task=  0 (u=45747)  {'docs': u'A major problem in using iterative number generators of the formx_i=f(x_{i-1}) is that they can enter unexpectedly short cycles. This is hardto analyze when the generator is designed, hard to detect in real time when thegenerator is used, and can have devastating cryptanalytic implications. In thispaper we define a measure of security, called_sequence_diversity_, whichgeneralizes the notion of cycle-length for non-iterative generators. We thenintroduce the class of counter assisted generators, and show how to turn anyiterative generator (even a bad one designed or seeded by an adversary) into acounter assisted generator with a provably high diversity, without reducing thequality of generators which are already cryptographically strong.'}
2.02e+03 pattern=  0 task=  0 (u=45748)  {'docs': u"The {\\em height} of a trace is the height of the corresponding heap of piecesin Viennot's representation, or equivalently the number of factors in itsCartier-Foata decomposition. Let $h(t)$ and $|t|$ stand respectively for theheight and the length of a trace $t$. Roughly speaking, $|t|$ is the`sequential' execution time and $h(t)$ is the `parallel' execution time. Weprove that the bivariate commutative series $\\sum_t x^{h(t)}y^{|t|}$ isrational, and we give a finite representation of it. We use the rationality toobtain precise information on the asymptotics of the number of traces of agiven height or length. Then, we study the average height of a trace forvarious probability distributions on traces. For the uniform probabilitydistribution on traces of the same length (resp. of the same height), theasymptotic average height (resp. length) exists and is an algebraic number. Toillustrate our results and methods, we consider a couple of examples: the freecommutative monoid and the trace monoid whose independence graph is the laddergraph."}
2.73e+03 pattern=  0 task=  0 (u=45865)  {'docs': u'We make progress on two important problems regarding attribute efficientlearnability.  First, we give an algorithm for learning decision lists of length $k$ over$n$ variables using $2^{\\tilde{O}(k^{1/3})} \\log n$ examples and time$n^{\\tilde{O}(k^{1/3})}$. This is the first algorithm for learning decisionlists that has both subexponential sample complexity and subexponential runningtime in the relevant parameters. Our approach establishes a relationshipbetween attribute efficient learning and polynomial threshold functions and isbased on a new construction of low degree, low weight polynomial thresholdfunctions for decision lists. For a wide range of parameters our constructionmatches a 1994 lower bound due to Beigel for the ODDMAXBIT predicate and givesan essentially optimal tradeoff between polynomial threshold function degreeand weight.  Second, we give an algorithm for learning an unknown parity function on $k$out of $n$ variables using $O(n^{1-1/k})$ examples in time polynomial in $n$.For $k=o(\\log n)$ this yields a polynomial time algorithm with samplecomplexity $o(n)$. This is the first polynomial time algorithm for learningparity on a superconstant number of variables with sublinear sample complexity.'}
2.55e+03 pattern=  0 task=  0 (u=45954)  {'docs': u'Software systems emerge from mere keystrokes to form intricate functionalnetworks connecting many collaborating modules, objects, classes, methods, andsubroutines. Building on recent advances in the study of complex networks, Ihave examined software collaboration graphs contained within severalopen-source software systems, and have found them to reveal scale-free,small-world networks similar to those identified in other technological,sociological, and biological systems. I present several measures of thesenetwork topologies, and discuss their relationship to software engineeringpractices. I also present a simple model of software system evolution based onrefactoring processes which captures some of the salient features of theobserved systems. Some implications of object-oriented design for questionsabout network robustness, evolvability, degeneracy, and organization arediscussed in the wake of these findings.'}
1.61e+03 pattern=  0 task=  0 (u=46102)  {'docs': u"The Autoepistemic Logic of Knowledge and Belief (AELB) is a powerfulnonmonotic formalism introduced by Teodor Przymusinski in 1994. In this paper,we specialize it to a class of theories called `super logic programs'. We arguethat these programs form a natural generalization of standard logic programs.In particular, they allow disjunctions and default negation of arbibrarypositive objective formulas.  Our main results are two new and powerful characterizations of the staticsemant ics of these programs, one syntactic, and one model-theoretic. Thesyntactic fixed point characterization is much simpler than the fixed pointconstruction of the static semantics for arbitrary AELB theories. Themodel-theoretic characterization via Kripke models allows one to constructfinite representations of the inherently infinite static expansions.  Both characterizations can be used as the basis of algorithms for queryanswering under the static semantics. We describe a query-answering interpreterfor super programs which we developed based on the model-theoreticcharacterization and which is available on the web."}
2.72e+03 pattern=  0 task=  0 (u=46138)  {'docs': u'Deterministic replay is a method for allowing complex multitasking real-timesystems to be debugged using standard interactive debuggers. Even thoughseveral replay techniques have been proposed for parallel, multi-tasking andreal-time systems, the solutions have so far lingered on a prototype academiclevel, with very little results to show from actual state-of-the-practicecommercial applications. This paper describes a major deterministic replaydebugging case study performed on a full-scale industrial robot control system,as well as a minor replay instrumentation case study performed on a militaryaircraft radar system. In this article, we will show that replay debugging isfeasible in complex multi-million lines of code software projects running ontop of off-the-shelf real-time operating systems. Furthermore, we will discusshow replay debugging can be introduced in existing systems withoutimpracticable analysis efforts. In addition, we will present benchmarkingresults from both studies, indicating that the instrumentation overhead isacceptable and affordable.'}
2.69e+03 pattern=  0 task=  0 (u=46268)  {'docs': u'Independent component analysis (ICA) has proven useful for modeling brain andelectroencephalographic (EEG) data. Here, we present a new, generalized methodto better capture the dynamics of brain signals than previous ICA algorithms.We regard EEG sources as eliciting spatio-temporal activity patterns,corresponding to, e.g., trajectories of activation propagating across cortex.This leads to a model of convolutive signal superposition, in contrast with thecommonly used instantaneous mixing model. In the frequency-domain, convolutivemixing is equivalent to multiplicative mixing of complex signal sources withindistinct spectral bands. We decompose the recorded spectral-domain signals intoindependent components by a complex infomax ICA algorithm. First results from avisual attention EEG experiment exhibit (1) sources of spatio-temporal dynamicsin the data, (2) links to subject behavior, (3) sources with a limited spectralextent, and (4) a higher degree of independence compared to sources derived bystandard ICA.'}
1.24e+03 pattern=  0 task=  0 (u=46352)  {'docs': u"Agents are small programs that autonomously take actions based on changes intheir environment or ``state.'' Over the last few years, there have been anincreasing number of efforts to build agents that can interact and/orcollaborate with other agents. In one of these efforts, Eiter, Subrahmanian amdPick (AIJ, 108(1-2), pages 179-255) have shown how agents may be built on topof legacy code. However, their framework assumes that agent states arecompletely determined, and there is no uncertainty in an agent's state. Thus,their framework allows an agent developer to specify how his agents will reactwhen the agent is 100% sure about what is true/false in the world state. Inthis paper, we propose the concept of a \\emph{probabilistic agent program} andshow how, given an arbitrary program written in any imperative language, we maybuild a declarative ``probabilistic'' agent program on top of it which supportsdecision making in the presence of uncertainty. We provide two alternativesemantics for probabilistic agent programs. We show that the second semantics,though more epistemically appealing, is more complex to compute. We providesound and complete algorithms to compute the semantics of \\emph{positive} agentprograms."}
2.68e+03 pattern=  0 task=  0 (u=46463)  {'docs': u'In message passing programs, once a process terminates with an unexpectederror, the terminated process can propagate the error to the rest of processesthrough communication dependencies, resulting in a program failure. Therefore,to locate faults, developers must identify the group of processes involved inthe original error and faulty processes that activate faults. This paperpresents a novel debugging tool, named MPI-PreDebugger (MPI-PD), for localizingfaulty processes in message passing programs. MPI-PD automaticallydistinguishes the original and the propagated errors by checking communicationerrors during program execution. If MPI-PD observes any communication errors,it backtraces communication dependencies and points out potential faultyprocesses in a timeline view. We also introduce three case studies, in whichMPI-PD has been shown to play the key role in their debugging. From thesestudies, we believe that MPI-PD helps developers to locate faults and allowsthem to concentrate in correcting their programs.'}
2.12e+03 pattern=  0 task=  0 (u=46466)  {'docs': u'Projective geometry provides the preferred framework for most implementationsof Euclidean space in graphics applications. Translations and rotations areboth linear transformations in projective geometry, which helps when it comesto programming complicated geometrical operations. But there is a fundamentalweakness in this approach - the Euclidean distance between points is nothandled in a straightforward manner. Here we discuss a solution to thisproblem, based on conformal geometry. The language of geometric algebra is bestsuited to exploiting this geometry, as it handles the interior and exteriorproducts in a single, unified framework. A number of applications arediscussed, including a compact formula for reflecting a line off a generalspherical surface.'}
2.68e+03 pattern=  0 task=  1 (u=46466)  {'docs': u"Blending schemes based on circles provide smooth `fair' interpolationsbetween series of points. Here we demonstrate a simple, robust set ofalgorithms for performing circle blends for a range of cases. An arbitrarylevel of G-continuity can be achieved by simple alterations to the underlyingparameterisation. Our method exploits the computational framework provided byconformal geometric algebra. This employs a five-dimensional representation ofpoints in space, in contrast to the four-dimensional representation typicallyused in projective geometry. The advantage of the conformal scheme is thatstraight lines and circles are treated in a single, unified framework. As afurther illustration of the power of the conformal framework, the basic idea isextended to the case of sphere blending to interpolate over a surface."}
2.68e+03 pattern=  0 task=  0 (u=46467)  {'docs': u"By recording every state change in the run of a program, it is possible topresent the programmer every bit of information that might be desired.Essentially, it becomes possible to debug the program by going ``backwards intime,'' vastly simplifying the process of debugging. An implementation of thisidea, the ``Omniscient Debugger,'' is used to demonstrate its viability and hasbeen used successfully on a number of large programs. Integration with an eventanalysis engine for searching and control is presented. Several small-scaleuser studies provide encouraging results. Finally performance issues andimplementation are discussed along with possible optimizations.  This paper makes three contributions of interest: the concept and techniqueof ``going backwards in time,'' the GUI which presents a global view of theprogram state and has a formal notion of ``navigation through time,'' and theintegration with an event analyzer."}
1.25e+03 pattern=  0 task=  0 (u=46575)  {'docs': u'The influence of time-dependent fitnesses on the infinite population dynamicsof simple genetic algorithms (without crossover) is analyzed. Based on generalarguments, a schematic phase diagram is constructed that allows one tocharacterize the asymptotic states in dependence on the mutation rate and thetime scale of changes. Furthermore, the notion of regular changes is raised forwhich the population can be shown to converge towards a generalizedquasispecies. Based on this, error thresholds and an optimal mutation rate areapproximately calculated for a generational genetic algorithm with a movingneedle-in-the-haystack landscape. The so found phase diagram is fullyconsistent with our general considerations.'}
3.07e+03 pattern=  0 task=  0 (u=46826)  {'docs': u"Data intensive applications on clusters often require requests quickly besent to the node managing the desired data. In many applications, one must lookthrough a sorted tree structure to determine the responsible node for accessingor storing the data.  Examples include object tracking in sensor networks, packet routing over theinternet, request processing in publish-subscribe middleware, and queryprocessing in database systems. When the tree structure is larger than the CPUcache, the standard implementation potentially incurs many cache misses foreach lookup; one cache miss at each successive level of the tree. As theCPU-RAM gap grows, this performance degradation will only become worse in thefuture.  We propose a solution that takes advantage of the growing speed of local areanetworks for clusters. We split the sorted tree structure among the nodes ofthe cluster. We assume that the structure will fit inside the aggregation ofthe CPU caches of the entire cluster. We then send a word over the network (aspart of a larger packet containing other words) in order to examine the treestructure in another node's CPU cache. We show that this is often faster thanthe standard solution, which locally incurs multiple cache misses whileaccessing each successive level of the tree."}
3.07e+03 pattern=  0 task=  0 (u=46827)  {'docs': u'The Internet is a loose amalgamation of independent service providers actingin their own self-interest. We examine the implications of this economicreality on peering relationships. Specifically, we consider how the incentivesof the providers might determine where they choose to interconnect with eachother. We consider a game where two selfish network providers must establishpeering points between their respective network graphs, given knowledge oftraffic conditions and a nearest-exit routing policy for out-going traffic, aswell as costs based on congestion and peering connectivity. We focus on thepairwise stability equilibrium concept and use a stochastic procedure to solvefor the stochastically pairwise stable configurations. Stochastically stablenetworks are selected for their robustness to deviations in strategy and aretherefore posited as the more likely networks to emerge in a dynamic setting.We note a paucity of stochastically stable peering configurations underasymmetric conditions, particularly to unequal interdomain traffic flow, withadverse effects on system-wide efficiency. Under bilateral flow conditions, wefind that as the cost associated with the establishment of peering linksapproaches zero, the variance in the number of peering links of stochasticallypairwise stable equilibria increases dramatically.'}
3.07e+03 pattern=  0 task=  0 (u=46829)  {'docs': u'Anomaly-based intrusion detection (AID) techniques are useful for detectingnovel intrusions into computing resources. One of the most successful AIDdetectors proposed to date is stide, which is based on analysis of system callsequences. In this paper, we present a detailed formal framework to analyze,understand and improve the performance of stide and similar AID techniques.Several important properties of stide-like detectors are established throughformal proofs, and validated by carefully conducted experiments using testdatasets. Finally, the framework is utilized to design two applications toimprove the cost and performance of stide-like detectors which are based onsequence analysis. The first application reduces the cost of developing AIDdetectors by identifying the critical sections in the training dataset, and thesecond application identifies the intrusion context in the intrusive dataset,that helps to fine-tune the detectors. Such fine-tuning in turn helps toimprove detection rate and reduce false alarm rate, thereby increasing theeffectiveness and efficiency of the intrusion detectors.'}
3.19e+03 pattern=  0 task=  0 (u=46833)  {'docs': u'The UNICORE Grid-technology provides a seamless, secure and intuitive accessto distributed Grid resources. In this paper we present the recent evolutionfrom project results to production Grids. At the beginning UNICORE wasdeveloped as a prototype software in two projects funded by the German researchministry (BMBF). Over the following years, in various European-funded projects,UNICORE evolved to a full-grown and well-tested Grid middleware system, whichtoday is used in daily production at many supercomputing centers worldwide.Beyond this production usage, the UNICORE technology serves as a solid basis inmany European and International research projects, which use existing UNICOREcomponents to implement advanced features, high level services, and support forapplications from a growing range of domains. In order to foster these ongoingdevelopments, UNICORE is available as open source under BSD licence atSourceForge, where new releases are published on a regular basis. This paper isa review of the UNICORE achievements so far and gives a glimpse on the UNICOREroadmap.'}
3.19e+03 pattern=  0 task=  0 (u=46843)  {'docs': u'We describe and implement a policy language. In our system, agents candistribute data along with usage policies in a decentralized architecture. Ourlanguage supports the specification of conditions and obligations, and also thepossibility to refine policies. In our framework, the compliance with usagepolicies is not actively enforced. However, agents are accountable for theiractions, and may be audited by an authority requiring justifications.'}
3.19e+03 pattern=  0 task=  0 (u=46847)  {'docs': u'In this paper, we investigate the use of compactly supported divergence-freewavelets for the representation of the Navier-Stokes solution. After remindingthe theoretical construction of divergence-free wavelet vectors, we present indetail the bases and corresponding fast algorithms for 2D and 3D incompressibleflows. In order to compute the nonlinear term, we propose a new method whichprovides in practice with the Hodge decomposition of any flow: thisdecomposition enables us to separate the incompressible part of the flow fromits orthogonal complement, which corresponds to the gradient component of theflow. Finally we show numerical tests to validate our approach.'}
1.69e+03 pattern=  0 task=  0 (u=46985)  {'docs': u'This paper is motivated by the fact that verifying liveness properties undera fairness condition is often problematic, especially when abstraction is used.It shows that using a more abstract notion than truth under fairness,specifically the concept of a property being satisfied within fairness can leadto interesting possibilities. Technically, it is first established thatdeciding satisfaction within fairness is a PSPACE-complete problem and it isshown that properties satisfied within fairness can always be satisfied by somefair implementation. Thereafter, the interaction between behavior abstractionand satisfaction within fairness is studied and it is proved that satisfactionof properties within fairness can be verified on behavior abstractions, if theabstraction homomorphism is weakly continuation-closed.'}
1.69e+03 pattern=  0 task=  0 (u=46990)  {'docs': u"GPCG is an algorithm within the Toolkit for Advanced Optimization (TAO) forsolving bound constrained, convex quadratic problems. Originally developed byMore' and Toraldo, this algorithm was designed for large-scale problems but hadbeen implemented only for a single processor. The TAO implementation isavailable for a wide range of high-performance architecture, and has beentested on up to 64 processors to solve problems with over 2.5 millionvariables."}
3.09e+03 pattern=  0 task=  0 (u=47074)  {'docs': u'Field Programmable Gate Arrays (FPGAs) have recently been increasingly usedfor highly-parallel processing of compute intensive tasks. This paperintroduces an FPGA hardware platform architecture that is PC-based, allows forfast reconfiguration over the PCI bus, and retains a simple physical hardwaredesign. The design considerations are first discussed, then the resultingsystem architecture designed is illustrated. Finally, experimental results onthe FPGA resources utilized for this design are presented.'}
2.77e+03 pattern=  0 task=  0 (u=47083)  {'docs': u"The proliferation of the Open Archive Initiative Protocol for MetadataHarvesting (OAI-PMH) has resulted in the creation of a large number of serviceproviders, all harvesting from either data providers or aggregators. If datawere available regarding the similarity of metadata records, service providerscould track redundant records across harvests from multiple sources as well asprovide additional end-user services. Due to the large number of metadataformats and the diverse mapping strategies employed by data providers,similarity calculation requirements necessitate the use of informationretrieval strategies. We describe an OAI-PMH aggregator implementation thatuses the optional ``<about>'' container to re-export the results of similaritycalculations. Metadata records (3751) were harvested from a NASA data providerand similarities for the records were computed. The results were useful fordetecting duplicates, similarities and metadata errors."}
2.78e+03 pattern=  0 task=  0 (u=47087)  {'docs': u"Fortran 77 programs for the computation of modified Bessel functions ofpurely imaginary order are presented. The codes compute the functions$K_{ia}(x)$, $L_{ia}(x)$ and their derivatives for real $a$ and positive $x$;these functions are independent solutions of the differential equation $x^2 w''+x w' +(a^2 -x^2)w=0$. The code also computes exponentially scaled functions.The range of computation is $(x,a)\\in (0,1500]\\times [-1500,1500]$ when scaledfunctions are considered and it is larger than $(0,500]\\times [-400,400]$ forstandard IEEE double precision arithmetic. The relative accuracy is better than$10^{-13}$ in the range $(0,200]\\times [-200,200]$ and close to $10^{-12}$ in$(0,1500]\\times [-1500,1500]$."}
2.86e+03 pattern=  0 task=  0 (u=47120)  {'docs': u'Given a system of equations in a "random" finitely generated subgroup of thebraid group, we show how to find a small ordered list of elements in thesubgroup, which contains a solution to the equations with a significantprobability. Moreover, with a significant probability, the solution will be thefirst in the list. This gives a probabilistic solution to: The conjugacyproblem, the group membership problem, the shortest representation of anelement, and other combinatorial group-theoretic problems in random subgroupsof the braid group.  We use a memory-based extension of the standard length-based approach, whichin principle can be applied to any group admitting an efficient, reasonablybehaving length function.'}
1.9e+03 pattern=  0 task=  0 (u=47169)  {'docs': u'Artificial neurons with arbitrarily complex internal structure areintroduced. The neurons can be described in terms of a set of internalvariables, a set activation functions which describe the time evolution ofthese variables and a set of characteristic functions which control how theneurons interact with one another. The information capacity of attractornetworks composed of these generalized neurons is shown to reach the maximumallowed bound. A simple example taken from the domain of pattern recognitiondemonstrates the increased computational power of these neurons. Furthermore, aspecific class of generalized neurons gives rise to a simple transformationrelating attractor networks of generalized neurons to standard three layerfeed-forward networks. Given this correspondence, we conjecture that themaximum information capacity of a three layer feed-forward network is 2 bitsper weight.'}
2.23e+03 pattern=  0 task=  0 (u=47243)  {'docs': u'Type analyses of logic programs which aim at inferring the types of theprogram being analyzed are presented in a unified abstract interpretation-basedframework. This covers most classical abstract interpretation-based typeanalyzers for logic programs, built on either top-down or bottom-upinterpretation of the program. In this setting, we discuss the wideningoperator, arguably a crucial one. We present a new widening which is moreprecise than those previously proposed. Practical results with our analysisdomain are also presented, showing that it also allows for efficient analysis.'}
1.96e+03 pattern=  0 task=  0 (u=47305)  {'docs': u'An important aspect of data integration involves answering queries usingvarious resources rather than by accessing database relations. The process oftransforming a query from the database relations to the resources is oftenreferred to as query folding or answering queries using views, where the viewsare the resources. We present a uniform approach that includes as special casesmuch of the previous work on this subject. Our approach is logic-based usingresolution. We deal with integrity constraints, negation, and recursion alsowithin this framework.'}
2.91e+03 pattern=  0 task=  0 (u=47322)  {'docs': u'Mining frequent itemsets is at the core of mining association rules, and isby now quite well understood algorithmically. However, most algorithms formining frequent itemsets assume that the main memory is large enough for thedata structures used in the mining, and very few efficient algorithms deal withthe case when the database is very large or the minimum support is very low.Mining frequent itemsets from a very large database poses new challenges, asastronomical amounts of raw data is ubiquitously being recorded in commerce,science and government. In this paper, we discuss approaches to mining frequentitemsets when data structures are too large to fit in main memory. Severaldivide-and-conquer algorithms are given for mining from disks. Many noveltechniques are introduced. Experimental results show that the techniques reducethe required disk accesses by orders of magnitude, and enable truly scalabledata mining.'}
1.52e+03 pattern=  0 task=  0 (u=47362)  {'docs': u'Recent trends in information management involve the periodic transcription ofdata onto secondary devices in a networked environment, and the properscheduling of these transcriptions is critical for efficient data management.To assist in the scheduling process, we are interested in modeling thereduction of consistency over time between a relation and its replica, termedobsolescence of data. The modeling is based on techniques from the field ofstochastic processes, and provides several stochastic models for contentevolution in the base relations of a database, taking referential integrityconstraints into account. These models are general enough to accommodate mostof the common scenarios in databases, including batch insertions and life spansboth with and without memory. As an initial "proof of concept" of theapplicability of our approach, we validate the insertion portion of our modelframework via experiments with real data feeds. We also discuss a set oftranscription protocols which make use of the proposed stochastic model.'}
3.2e+03 pattern=  0 task=  0 (u=47408)  {'docs': u"In-degree, PageRank, number of visits and other measures of Web pagepopularity significantly influence the ranking of search results by modernsearch engines. The assumption is that popularity is closely correlated withquality, a more elusive concept that is difficult to measure directly.Unfortunately, the correlation between popularity and quality is very weak fornewly-created pages that have yet to receive many visits and/or in-links.Worse, since discovery of new content is largely done by querying searchengines, and because users usually focus their attention on the top fewresults, newly-created but high-quality pages are effectively ``shut out,'' andit can take a very long time before they become popular.  We propose a simple and elegant solution to this problem: the introduction ofa controlled amount of randomness into search result ranking methods. Doing sooffers new pages a chance to prove their worth, although clearly using too muchrandomness will degrade result quality and annul any benefits achieved. Hencethere is a tradeoff between exploration to estimate the quality of new pagesand exploitation of pages already known to be of high quality. We study thistradeoff both analytically and via simulation, in the context of an economicobjective function based on aggregate result quality amortized over time. Weshow that a modest amount of randomness leads to improved search results."}
1.81e+03 pattern=  0 task=  0 (u=47477)  {'docs': u'We describe the use of array expressions as constraints, which represents aconsequent generalisation of the "element" constraint. Constraint propagationfor array constraints is studied theoretically, and for a set of domainreduction rules the local consistency they enforce, arc-consistency, is proved.An efficient algorithm is described that encapsulates the rule set and soinherits the capability to enforce arc-consistency from the rules.'}
1.08e+03 pattern=  0 task=  0 (u=47497)  {'docs': u'We recently presented a methodology for quantitatively reducing the risk andcost of executing electronic transactions in a bursty network environment suchas the Internet. In the language of portfolio theory, time to complete atransaction and its variance replace the expected return and risk associatedwith a security, whereas restart times replace combinations of securities.While such a strategy works well with single users, the question remains as toits usefulness when used by many. By using mean field arguments and agent-basedsimulations, we determine that a restart strategy remains advantageous even ifeverybody uses it.'}
2.74e+03 pattern=  0 task=  0 (u=47782)  {'docs': u'Shape analysis concerns the problem of determining "shape invariants" forprograms that perform destructive updating on dynamically allocated storage. Inrecent work, we have shown how shape analysis can be performed, using anabstract interpretation based on 3-valued first-order logic. In that work,concrete stores are finite 2-valued logical structures, and the sets of storesthat can possibly arise during execution are represented (conservatively) usinga certain family of finite 3-valued logical structures. In this paper, we showhow 3-valued structures that arise in shape analysis can be characterized usingformulas in first-order logic with transitive closure.  We also define a non-standard ("supervaluational") semantics for 3-valuedfirst-order logic that is more precise than a conventional 3-valued semantics,and demonstrate that the supervaluational semantics can be effectivelyimplemented using existing theorem provers.'}
2.49e+03 pattern=  0 task=  0 (u=48127)  {'docs': u'We present a fully automatic method for music classification, based only oncompression of strings that represent the music pieces. The method uses nobackground knowledge about music whatsoever: it is completely general and can,without change, be used in different areas like linguistic classification andgenomics. It is based on an ideal theory of the information content inindividual objects (Kolmogorov complexity), information distance, and auniversal similarity metric. Experiments show that the method distinguishesreasonably well between various musical genres and can even cluster pieces bycomposer.'}
2.76e+03 pattern=  0 task=  1 (u=48127)  {'docs': u"We present a new method for clustering based on compression. The methoddoesn't use subject-specific features or background knowledge, and works asfollows: First, we determine a universal similarity distance, the normalizedcompression distance or NCD, computed from the lengths of compressed data files(singly and in pairwise concatenation). Second, we apply a hierarchicalclustering method. The NCD is universal in that it is not restricted to aspecific application area, and works across application area boundaries. Atheoretical precursor, the normalized information distance, co-developed by oneof the authors, is provably optimal but uses the non-computable notion ofKolmogorov complexity. We propose precise notions of similarity metric, normalcompressor, and show that the NCD based on a normal compressor is a similaritymetric that approximates universality. To extract a hierarchy of clusters fromthe distance matrix, we determine a dendrogram (binary tree) by a new quartetmethod and a fast heuristic to implement it. The method is implemented andavailable as public software, and is robust under choice of differentcompressors. To substantiate our claims of universality and robustness, wereport evidence of successful application in areas as diverse as genomics,virology, languages, literature, music, handwritten digits, astronomy, andcombinations of objects from completely different domains, using statistical,dictionary, and block sorting compressors. In genomics we presented newevidence for major questions in Mammalian evolution, based onwhole-mitochondrial genomic analysis: the Eutherian orders and the Marsupiontahypothesis against the Theria hypothesis."}
3.12e+03 pattern=  0 task=  2 (u=48127)  {'docs': u"Words and phrases acquire meaning from the way they are used in society, fromtheir relative semantics to other words and phrases. For computers theequivalent of `society' is `database,' and the equivalent of `use' is `way tosearch the database.' We present a new theory of similarity between words andphrases based on information distance and Kolmogorov complexity. To fixthoughts we use the world-wide-web as database, and Google as search engine.The method is also applicable to other search engines and databases. Thistheory is then applied to construct a method to automatically extractsimilarity, the Google similarity distance, of words and phrases from theworld-wide-web using Google page counts. The world-wide-web is the largestdatabase on earth, and the context information entered by millions ofindependent users averages out to provide automatic semantics of usefulquality. We give applications in hierarchical clustering, classification, andlanguage translation. We give examples to distinguish between colors andnumbers, cluster names of paintings by 17th century Dutch masters and names ofbooks by English novelists, the ability to understand emergencies, and primes,and we demonstrate the ability to do a simple automatic English-Spanishtranslation. Finally, we use the WordNet database as an objective baselineagainst which to judge the performance of our method. We conduct a massiverandomized trial in binary classification using support vector machines tolearn categories based on our Google distance, resulting in an a mean agreementof 87% with the expert crafted WordNet categories."}
1.1e+03 pattern=  4 task=  0 (u=48185)  {'docs': u'String transductions that are definable in monadic second-order (mso) logic(without the use of parameters) are exactly those realized by deterministictwo-way finite state transducers. Nondeterministic mso definable stringtransductions (i.e., those definable with the use of parameters) correspond tocompositions of two nondeterministic two-way finite state transducers that havethe finite visit property. Both families of mso definable string transductionsare characterized in terms of Hennie machines, i.e., two-way finite statetransducers with the finite visit property that are allowed to rewrite theirinput tape.'}
2.79e+03 pattern=  0 task=  0 (u=48314)  {'docs': u'A recursive approach for shrinking coefficients of an atomic decomposition isproposed. The corresponding algorithm evolves so as to provide at eachiteration a) the orthogonal projection of a signal onto a reduced subspace andb) the index of the coefficient to be disregarded in order to construct acoarser approximation minimizing the norm of the residual error.'}
3.08e+03 pattern=  0 task=  0 (u=48508)  {'docs': u"The article presents an online relevancy tuning method using explicit userfeedback. The author developed and tested a method of words' weightsmodification based on search result evaluation by user. User decides whetherthe result is useful or not after inspecting the full result content. Theexperiment proved that the constantly accumulated words weights base leads tobetter search quality in a specified data domain. The author also suggestedfuture improvements of the method."}
3.04e+03 pattern=  0 task=  0 (u=49047)  {'docs': u'We consider the problem of maximizing the revenue raised from tolls set onthe arcs of a transportation network, under the constraint that users areassigned to toll-compatible shortest paths. We first prove that this problem isstrongly NP-hard. We then provide a polynomial time algorithm with a worst-caseprecision guarantee of ${1/2}\\log_2 m_T+1$, where $m_T$ denotes the number oftoll arcs. Finally we show that the approximation is tight with respect to anatural relaxation by constructing a family of instances for which therelaxation gap is reached.'}
1.38e+03 pattern=  0 task=  0 (u=49281)  {'docs': u'Logical frameworks based on intuitionistic or linear logics with higher-typequantification have been successfully used to give high-level, modular, andformal specifications of many important judgments in the area of programminglanguages and inference systems. Given such specifications, it is natural toconsider proving properties about the specified systems in the framework: forexample, given the specification of evaluation for a functional programminglanguage, prove that the language is deterministic or that evaluation preservestypes. One challenge in developing a framework for such reasoning is thathigher-order abstract syntax (HOAS), an elegant and declarative treatment ofobject-level abstraction and substitution, is difficult to treat in proofsinvolving induction. In this paper, we present a meta-logic that can be used toreason about judgments coded using HOAS; this meta-logic is an extension of asimple intuitionistic logic that admits higher-order quantification over simplytyped lambda-terms (key ingredients for HOAS) as well as induction and a notionof definition. We explore the difficulties of formal meta-theoretic analysis ofHOAS encodings by considering encodings of intuitionistic and linear logics,and formally derive the admissibility of cut for important subsets of theselogics. We then propose an approach to avoid the apparent tradeoff between thebenefits of higher-order abstract syntax and the ability to analyze theresulting encodings. We illustrate this approach through examples involving thesimple functional and imperative programming languages PCF and PCF:=. Weformally derive such properties as unicity of typing, subject reduction,determinacy of evaluation, and the equivalence of transition semantics andnatural semantics presentations of evaluation.'}
2.91e+03 pattern=  0 task=  0 (u=49649)  {'docs': u'We present a prescriptive type system with parametric polymorphism andsubtyping for constraint logic programs. The aim of this type system is todetect programming errors statically. It introduces a type discipline forconstraint logic programs and modules, while maintaining the capabilities ofperforming the usual coercions between constraint domains, and of typingmeta-programming predicates, thanks to the flexibility of subtyping. Theproperty of subject reduction expresses the consistency of a prescriptive typesystem w.r.t. the execution model: if a program is "well-typed", then allderivations starting from a "well-typed" goal are again "well-typed". Thatproperty is proved w.r.t. the abstract execution model of constraintprogramming which proceeds by accumulation of constraints only, and w.r.t. anenriched execution model with type constraints for substitutions. We describeour implementation of the system for type checking and type inference. Wereport our experimental results on type checking ISO-Prolog, the (constraint)libraries of Sicstus Prolog and other Prolog programs.'}
2.91e+03 pattern=  0 task=  0 (u=49651)  {'docs': u'This note illustrates theoretical worst-case scenarios for groundnessanalyses obtained through abstract interpretation over the abstract domains ofdefinite (Def) and positive (Pos) Boolean functions. For Def, an example isgiven for which any Def-based abstract interpretation for groundness analysisfollows a chain which is exponential in the number of argument positions aswell as in the number of clauses but sub-exponential in the size of theprogram. For Pos, we strengthen a previous result by illustrating an examplefor which any Pos-based abstract interpretation for groundness analysis followsa chain which is exponential in the size of the program. It remains an openproblem to determine if the worst case for Def is really as bad as that forPos.'}
2.91e+03 pattern=  0 task=  0 (u=49652)  {'docs': u'How to extract negative information from programs is an important issue inlogic programming. Here we address the problem for functional logic programs,from a proof-theoretic perspective. The starting point of our work is CRWL(Constructor based ReWriting Logic), a well established theoretical frameworkfor functional logic programming, whose fundamental notion is that ofnon-strict non-deterministic function. We present a proof calculus, CRWLF,which is able to deduce negative information from CRWL-programs. In particular,CRWLF is able to prove finite failure of reduction within CRWL.'}
2.91e+03 pattern=  0 task=  0 (u=49654)  {'docs': u'A system is data-independent with respect to a data type X iff the operationsit can perform on values of type X are restricted to just equality testing. Thesystem may also store, input and output values of type X. We study modelchecking of systems which are data-independent with respect to two distincttype variables X and Y, and may in addition use arrays with indices from X andvalues from Y . Our main interest is the following parameterised model-checkingproblem: whether a given program satisfies a given temporal-logic formula forall non-empty nite instances of X and Y . Initially, we consider instead theabstraction where X and Y are infinite and where partial functions with finitedomains are used to model arrays. Using a translation to data-independentsystems without arrays, we show that the u-calculus model-checking problem isdecidable for these systems. From this result, we can deduce properties of allsystems with finite instances of X and Y . We show that there is a procedurefor the above parameterised model-checking problem of the universal fragment ofthe u-calculus, such that it always terminates but may give false negatives. Wealso deduce that the parameterised model-checking problem of the universaldisjunction-free fragment of the u-calculus is decidable. Practical motivationsfor model checking data-independent systems with arrays include verification ofmemory and cache systems, where X is the type of memory addresses, and Y thetype of storable values. As an example we verify a fault-tolerant memoryinterface over a set of unreliable memories.'}
2.37e+03 pattern=  0 task=  0 (u=49837)  {'docs': u'Theoretical computer science has found fertile ground in many areas ofmathematics. The approach has been to consider classical problems through theprism of computational complexity, where the number of basic computationalsteps taken to solve a problem is the crucial qualitative parameter. This newapproach has led to a sequence of advances, in setting and solving newmathematical challenges as well as in harnessing discrete mathematics to thetask of solving real-world problems.  In this talk, I will survey the development of modern cryptography -- themathematics behind secret communications and protocols -- in this light. I willdescribe the complexity theoretic foundations underlying the cryptographictasks of encryption, pseudo-randomness number generators and functions, zeroknowledge interactive proofs, and multi-party secure protocols. I will attemptto highlight the paradigms and proof techniques which unify these foundations,and which have made their way into the mainstream of complexity theory.'}
2.37e+03 pattern=  0 task=  1 (u=49837)  {'docs': u"Madhu Sudan's work spans many areas of computer science theory includingcomputational complexity theory, the design of efficient algorithms,algorithmic coding theory, and the theory of program checking and correcting.  Two results of Sudan stand out in the impact they have had on the mathematicsof computation. The first work shows a probabilistic characterization of theclass NP -- those sets for which short and easily checkable proofs ofmembership exist, and demonstrates consequences of this characterization toclassifying the complexity of approximation problems. The second work shows apolynomial time algorithm for list decoding the Reed Solomon error correctingcodes.  This short note will be devoted to describing Sudan's work onprobabilistically checkable proofs -- the so called {\\it PCP theorem} and itsimplications."}
2.65e+03 pattern=  0 task=  0 (u=49986)  {'docs': u'We study numerically how a sound signal stored in a quantum computer can berecognized and restored with a minimal number of measurements in presence ofrandom quantum gate errors. A method developed uses elements of MP3 soundcompression and allows to recover human speech and sound of complex quantumwavefunctions.'}
 1.34 pattern=  0 task=  0 (u=50015)  {'docs': u'In this paper, a hierarchical context definition is added to an existingclustering algorithm in order to increase its robustness. The resultingalgorithm, which clusters contexts and events separately, is used to experimentwith different ways of defining the context a language model takes intoaccount. The contexts range from standard bigram and trigram contexts to partof speech five-grams. Although none of the models can compete directly with abackoff trigram, they give up to 9\\% improvement in perplexity wheninterpolated with a trigram. Moreover, the modified version of the algorithmleads to a performance increase over the original version of up to 12\\%.'}
2.54e+03 pattern=  0 task=  0 (u=50037)  {'docs': u'Numerous systems for dissemination, retrieval, and archiving of documentshave been developed in the past. Those systems often focus on one of theseaspects and are hard to extend and combine. Typically, the transmissionprotocols, query and filtering languages are fixed as well as the interfaces toother systems. We rather envisage the seamless establishment of networks amongthe providers, repositories and consumers of information, supportinginformation retrieval and dissemination while being highly interoperable andextensible.  We propose a framework with a single event-based mechanism that unifiesdocument storage, retrieval, and dissemination. This framework offers completeopenness with respect to document and metadata formats, transmission protocols,and filtering mechanisms. It specifies a high-level building kit, by whicharbitrary processors for document streams can be incorporated to support theretrieval, transformation, aggregation and disaggregation of documents. Usingthe same kit, interfaces for different transmission protocols can be addedeasily to enable the communication with various information sources andinformation consumers.'}
1.5e+03 pattern=  0 task=  0 (u=50054)  {'docs': u'This paper describes an experimental comparison between two standardsupervised learning methods, namely Naive Bayes and Exemplar-basedclassification, on the Word Sense Disambiguation (WSD) problem. The aim of thework is twofold. Firstly, it attempts to contribute to clarify some confusinginformation about the comparison between both methods appearing in the relatedliterature. In doing so, several directions have been explored, including:testing several modifications of the basic learning algorithms and varying thefeature space. Secondly, an improvement of both algorithms is proposed, inorder to deal with large attribute sets. This modification, which basicallyconsists in using only the positive information appearing in the examples,allows to improve greatly the efficiency of the methods, with no loss inaccuracy. The experiments have been performed on the largest sense-taggedcorpus available containing the most frequent and ambiguous English words.Results show that the Exemplar-based approach to WSD is generally superior tothe Bayesian approach, especially when a specific metric for dealing withsymbolic attributes is used.'}
1.5e+03 pattern=  0 task=  1 (u=50054)  {'docs': u"In this paper Schapire and Singer's AdaBoost.MH boosting algorithm is appliedto the Word Sense Disambiguation (WSD) problem. Initial experiments on a set of15 selected polysemous words show that the boosting approach surpasses NaiveBayes and Exemplar-based approaches, which represent state-of-the-art accuracyon supervised WSD. In order to make boosting practical for a real learningdomain of thousands of words, several ways of accelerating the algorithm byreducing the feature space are studied. The best variant, which we callLazyBoosting, is tested on the largest sense-tagged corpus available containing192,800 examples of the 191 most frequent and ambiguous English words. Again,boosting compares favourably to the other benchmark algorithms."}
1.6e+03 pattern=  0 task=  0 (u=50173)  {'docs': u'This article describes an algorithm for reducing the intermediate alphabetsin cascades of finite-state transducers (FSTs). Although the method modifiesthe component FSTs, there is no change in the overall relation described by thewhole cascade. No additional information or special algorithm, that coulddecelerate the processing of input, is required at runtime. Two examples fromNatural Language Processing are used to illustrate the effect of the algorithmon the sizes of the FSTs and their alphabets. With some FSTs the number of arcsand symbols shrank considerably.'}
2.98e+03 pattern=  0 task=  0 (u=50189)  {'docs': u'In this paper we present a cut-free sequent calculus, called SeqS, for somestandard conditional logics, namely CK, CK+ID, CK+MP and CK+MP+ID. The calculususes labels and transition formulas and can be used to prove decidability andspace complexity bounds for the respective logics. We also present CondLean, atheorem prover for these logics implementing SeqS calculi written in SICStusProlog.'}
2.23e+03 pattern=  0 task=  0 (u=50319)  {'docs': u'This paper describes the COINS (COnstraint-based INteractive Solving) system:a conflict-based constraint solver. It helps understanding inconsistencies,simulates constraint additions and/or retractions (without any propagation),determines if a given constraint belongs to a conflict and provides diagnosistools (e.g. why variable v cannot take value val). COINS also usesuser-friendly representation of conflicts and explanations.'}
3.26e+03 pattern=  0 task=  0 (u=50341)  {'docs': u'In the study of depth functions it is important to decide whether we wantsuch a function to be sensitive to multimodality or not. In this paper weanalyze the Delaunay depth function, which is sensitive to multimodality andcompare this depth with others, as convex depth and location depth. We studythe stratification that Delaunay depth induces in the point set (layers) and inthe whole plane (levels), and we develop an algorithm for computing theDelaunay depth contours, associated to a point set in the plane, with runningtime O(n log^2 n). The depth of a query point p with respect to a data set S inthe plane is the depth of p in the union of S and p. When S and p are given inthe input the Delaunay depth can be computed in O(n log n), and we prove thatthis value is optimal.'}
3.18e+03 pattern=  0 task=  0 (u=50568)  {'docs': u'The goal of this work is to model the peering arrangements between AutonomousSystems (ASes). Most existing models of the AS-graph assume an undirectedgraph. However, peering arrangements are mostly asymmetric Customer-Providerarrangements, which are better modeled as directed edges. Furthermore, it iswell known that the AS-graph, and in particular its clustering structure, isinfluenced by geography.  We introduce a new model that describes the AS-graph as a directed graph,with an edge going from the customer to the provider, but also models symmetricpeer-to-peer arrangements, and takes geography into account. We are able tomathematically analyze its power-law exponent and number of leaves. Beyond theanalysis we have implemented our model as a synthetic network generator we callGdTang. Experimentation with GdTang shows that the networks it produces aremore realistic than those generated by other network generators, in terms ofits power-law exponent, fractions of customer-provider and symmetric peeringarrangements, and the size of its dense core. We believe that our model is thefirst to manifest realistic regional dense cores that have a clear geographicflavor. Our synthetic networks also exhibit path inflation effects that aresimilar to those observed in the real AS graph.'}
1.72e+03 pattern=  0 task=  0 (u=50679)  {'docs': u'Parallel jobs are different from sequential jobs and require a different typeof process management. We present here a process management system for parallelprograms such as those written using MPI. A primary goal of the system, whichwe call MPD (for multipurpose daemon), is to be scalable. By this we mean thatstartup of interactive parallel jobs comprising thousands of processes isquick, that signals can be quickly delivered to processes, and that stdin,stdout, and stderr are managed intuitively. Our primary target is parallelmachines made up of clusters of SMPs, but the system is also useful in moretightly integrated environments. We describe how MPD enables much fasterstartup and better runtime management of parallel jobs. We show how closecontrol of stdio can support the easy implementation of a number of convenientsystem utilities, even a parallel debugger. We describe a simple but generalinterface that can be used to separate any process manager from a parallellibrary, which we use to keep MPD separate from MPICH.'}
2.86e+03 pattern=  0 task=  0 (u=50707)  {'docs': u'The IEEE 802.11 protocol is a popular standard for wireless local areanetworks. Its medium access control layer (MAC) is a carrier sense multipleaccess with collision avoidance (CSMA/CA) design and includes an exponentialbackoff mechanism that makes it a possible target for probabilistic modelchecking. In this work, we identify ways to increase the scope of applicationof probabilistic model checking to the 802.11 MAC. Current techniques do notscale to networks of even moderate size. To work around this problem, weidentify properties of the protocol that can be used to simplify the models andmake verification feasible. Using these observations, we directly optimize theprobabilistic timed automata models while preserving probabilistic reachabilitymeasures. We substantiate our claims of significant reduction by our resultsfrom using the probabilistic model checker PRISM.'}
2.56e+03 pattern=  0 task=  0 (u=51003)  {'docs': u'We model the performance of an ideal closed chain of L processing elementsthat work in parallel in an asynchronous manner. Their state updates follow ageneric conservative algorithm. The conservative update rule determines thegrowth of a virtual time surface. The physics of this growth is reflected inthe utilization (the fraction of working processors) and in the interfacewidth. We show that it is possible to nake an explicit connection between theutilization and the macroscopic structure of the virtual time interface. Weexploit this connection to derive the theoretical probability distribution ofupdates in the system within an approximate model. It follows that thetheoretical lower bound for the computational speed-up is s=(L+1)/4 for L>3.Our approach uses simple statistics to count distinct surface configurationclasses consistent with the model growth rule. It enables one to computeanalytically microscopic properties of an interface, which are unavailable bycontinuum methods.'}
2.84e+03 pattern=  0 task=  1 (u=51003)  {'docs': u'We simulate competitive two-component growth on a one dimensional substrateof $L$ sites. One component is a Poisson-type deposition that generatesKardar-Parisi-Zhang (KPZ) correlations. The other is random deposition (RD). Wederive the universal scaling function of the interface width for this model andshow that the RD admixture acts as a dilatation mechanism to the fundamentaltime and height scales, but leaves the KPZ correlations intact. Thisobservation is generalized to other growth models. It is shown that theflat-substrate initial condition is responsible for the existence of an earlynon-scaling phase in the interface evolution. The length of this initial phaseis a non-universal parameter, but its presence is universal. In application toparallel and distributed computations, the important consequence of the derivedscaling is the existence of the upper bound for the desynchronization in aconservative update algorithm for parallel discrete-event simulations. It isshown that such algorithms are generally scalable in a ring communicationtopology.'}
3.03e+03 pattern=  0 task=  2 (u=51003)  {'docs': u'In a state-update protocol for a system of $L$ asynchronous parallelprocesses that communicate only with nearest neighbors, globaldesynchronization in operation times can be deduced from kinetic roughening ofthe corresponding virtual-time horizon (VTH). The utilization of the parallelprocessing environment can be deduced by analyzing the microscopic structure ofthe VTH. We give an overview of how the methods of non-equilibrium surfacegrowth (physics of complex systems) can be applied to uncover some propertiesof state update algorithms used in distributed parallel discrete-eventsimulations (PDES). In particular, we focus on the asynchronous conservativePDES algorithm in a ring communication topology. The time evolution of its VTHis simulated numerically as asynchronous cellular automaton whose update rulecorresponds to the update rule followed by this algorithm. We give theoreticalestimates of the performance as a function of $L$ and the load per processor,i.e., approximate formulas for the mean speedup and for the desynchronization.It is established that, for a given simulation size, there is a theoreticalupper bound for the desynchronization and a theoretical non-zero lower boundfor the utilization. The new approach to performance studies, outlined in thischapter, is particularly useful in the search for the design of anew-generation of algorithms that would efficiently carry out an autonomous ortunable synchronization.'}
1.99e+03 pattern=  0 task=  0 (u=51007)  {'docs': u'Environments for systematic construction of logic programs are needed in theacademy as well as in the industry. Such environments should support welldefined construction methods and should be able to be extended and interactwith other programming tools like debuggers and compilers. We present a variantof the Deville methodology for logic program development, and the design of atool for supporting the methodology. Our aim is to facilitate the learning oflogic programming and to set the basis of more sophisticated tools for programdevelopment.'}
1.63e+03 pattern=  0 task=  0 (u=51346)  {'docs': u'Although attribute grammars are commonly used for compiler construction,little investigation has been conducted on debugging attribute grammars. Thepaper proposes two types of systematic debugging methods, an algorithmicdebugging and slice-based debugging, both tailored for attribute grammars. Bymeans of query-based interaction with the developer, our debugging methodseffectively narrow the potential bug space in the attribute grammar descriptionand eventually identify the incorrect attribution rule. We have incorporatedthis technology in our visual debugging tool called Aki.'}
1.77e+03 pattern=  0 task=  0 (u=51498)  {'docs': u"We present a novel framework for evaluating recommendation algorithms interms of the `jumps' that they make to connect people to artifacts. Thisapproach emphasizes reachability via an algorithm within the implicit graphstructure underlying a recommender dataset, and serves as a complement toevaluation in terms of predictive accuracy. The framework allows us to considerquestions relating algorithmic parameters to properties of the datasets. Forinstance, given a particular algorithm `jump,' what is the average path lengthfrom a person to an artifact? Or, what choices of minimum ratings and jumpsmaintain a connected graph? We illustrate the approach with a common jumpcalled the `hammock' using movie recommender datasets."}
3e+03 pattern=  0 task=  0 (u=51560)  {'docs': u'FLUX is a programming method for the design of agents that reason logicallyabout their actions and sensor information in the presence of incompleteknowledge. The core of FLUX is a system of Constraint Handling Rules, whichenables agents to maintain an internal model of their environment by which theycontrol their own behavior. The general action representation formalism of thefluent calculus provides the formal semantics for the constraint solver. FLUXexhibits excellent computational behavior due to both a carefully restrictedexpressiveness and the inference paradigm of progression.'}
3.07e+03 pattern=  0 task=  0 (u=51628)  {'docs': u'Complex patterns generated by the time evolution of a one-dimensionaldigitalized coupled map lattice are quantitatively analyzed. A method fordiscerning complexity among the different patterns is implemented. Thequantitative results indicate two zones in parameter space where the dynamicsshows the most complex patterns. These zones are located on the two edges of anabsorbent region where the system displays spatio-temporal intermittency.'}
2.98e+03 pattern=  0 task=  0 (u=52074)  {'docs': u'The Painleve test is very useful to construct not only the Laurent-seriessolutions but also the elliptic and trigonometric ones. Such single-valuedfunctions are solutions of some polynomial first order differential equations.To find the elliptic solutions we transform an initial nonlinear differentialequation in a nonlinear algebraic system in parameters of the Laurent-seriessolutions of the initial equation. The number of unknowns in the obtainednonlinear system does not depend on number of arbitrary coefficients of theused first order equation. In this paper we describe the correspondingalgorithm, which has been realized in REDUCE and Maple.'}
2.58e+03 pattern=  0 task=  0 (u=52078)  {'docs': u'The paper deals with the verification of reachability properties in acommonly used state transition model of communication protocols, which consistsof finite state machines connected by potentially unbounded FIFO channels.Although simple reachability problems are undecidable for general protocolswith unbounded channels, they are decidable for the protocols with therecognizable channel property. The decidability question is open for theprotocols with the rational channel property.'}
3.2e+03 pattern=  0 task=  0 (u=52089)  {'docs': u"A sense of direction is an edge labeling on graphs that follows a globallyconsistent scheme and is known to considerably reduce the complexity of severaldistributed problems. In this paper, we study a particular instance of sense ofdirection, called a chordal sense of direction (CSD). In special, we identifythe class of k-regular graphs that admit a CSD with exactly k labels (a minimalCSD). We prove that connected graphs in this class are Hamiltonian and that theclass is equivalent to that of circulant graphs, presenting an efficient(polynomial-time) way of recognizing it when the graphs' degree k is fixed."}
2.58e+03 pattern=  0 task=  0 (u=52195)  {'docs': u"Let $f:\\hat{C}\\to\\hat{C}$ be a subhyperbolic rational map of degree $d$. Weconstruct a set of coding maps $Cod(f)=\\{\\pi_r:\\Sigma\\to J\\}_r$ of the Juliaset $J$ by geometric coding trees, where the parameter $r$ ranges over mappingsfrom a certain tree to the Riemann sphere. Using the universal covering space$\\phi:\\tilde S\\to S$ for the corresponding orbifold, we lift the inverse of $f$to an iterated function system $I=(g_i)_{i=1,2,...,d}$. For the purpose ofstudying the structure of $Cod(f)$, we generalize Kenyon and Lagarias-Wang'sresults : If the attractor $K$ of $I$ has positive measure, then $K$ tiles$\\phi^{-1}(J)$, and the multiplicity of $\\pi_r$ is well-defined. Moreover, wesee that the equivalence relation induced by $\\pi_r$ is described by a finitedirected graph, and give a necessary and sufficient condition for two codingmaps $\\pi_r$ and $\\pi_{r'}$ to be equal."}
2.47e+03 pattern=  0 task=  0 (u=53124)  {'docs': u'Signals consisting of a sequence of pulses show that inherent origin of the1/f noise is a Brownian fluctuation of the average interevent time betweensubsequent pulses of the pulse sequence. In this paper we generalize the modelof interevent time to reproduce a variety of self-affine time series exhibitingpower spectral density S(f) scaling as a power of the frequency f. Furthermore,we analyze the relation between the power-law correlations and the origin ofthe power-law probability distribution of the signal intensity. We introduce astochastic multiplicative model for the time intervals between point events andanalyze the statistical properties of the signal analytically and numerically.Such model system exhibits power-law spectral density S(f)~1/f**beta forvarious values of beta, including beta=1/2, 1 and 3/2. Explicit expressions forthe power spectra in the low frequency limit and for the distribution densityof the interevent time are obtained. The counting statistics of the events isanalyzed analytically and numerically, as well. The specific interest of ouranalysis is related with the financial markets, where long-range correlationsof price fluctuations largely depend on the number of transactions. We analyzethe spectral density and counting statistics of the number of transactions. Themodel reproduces spectral properties of the real markets and explains themechanism of power-law distribution of trading activity. The study providesevidence that the statistical properties of the financial markets are enclosedin the statistics of the time interval between trades. A multiplicative pointprocess serves as a consistent model generating this statistics.'}
3.13e+03 pattern=  0 task=  1 (u=53124)  {'docs': u'We introduce the stochastic multiplicative point process modelling tradingactivity of financial markets. Such a model system exhibits power-law spectraldensity S(f) ~ 1/f**beta, scaled as power of frequency for various values ofbeta between 0.5 and 2. Furthermore, we analyze the relation between thepower-law autocorrelations and the origin of the power-law probabilitydistribution of the trading activity. The model reproduces the spectralproperties of trading activity and explains the mechanism of power-lawdistribution in real markets.'}
2.36e+03 pattern=  0 task=  0 (u=53444)  {'docs': u'We provide an algebraic framework to compute smallest enclosing and smallestcircumscribing cylinders of simplices in Euclidean space $\\E^n$. Explicitly,the computation of a smallest enclosing cylinder in $\\mathbb{E}^3$ is reducedto the computation of a smallest circumscribing cylinder. We improve existingpolynomial formulations to compute the locally extreme circumscribing cylindersin $\\E^3$ and exhibit subclasses of simplices where the algebraic degrees canbe further reduced. Moreover, we generalize these efficient formulations to the$n$-dimensional case and provide bounds on the number of local extrema. Usingelementary invariant theory, we prove structural results on the directionvectors of any locally extreme circumscribing cylinder for regular simplices.'}
3.08e+03 pattern=  0 task=  0 (u=53582)  {'docs': u"Fewnomial theory began with explicit bounds -- solely in terms of the numberof variables and monomial terms -- on the number of real roots of systems ofpolynomial equations. Here we take the next logical step of investigating thecorresponding existence problem: Let FEAS_R denote the problem of decidingwhether a given system of multivariate polynomial equations with integercoefficients has a real root or not. We describe a phase-transition for when mis large enough to make FEAS_R be NP-hard, when restricted to inputs consistingof a single n-variate polynomial with exactly m monomial terms: polynomial-timefor m<=n+2 (for any fixed n) and NP-hardness for m<=n+n^{epsilon} (for nvarying and any fixed epsilon>0). Because of important connections betweenFEAS_R and A-discriminants, we then study some new families of A-discriminantswhose signs can be decided within polynomial-time. (A-discriminants contain allknown resultants as special cases, and the latter objects are central inalgorithmic algebraic geometry.) Baker's Theorem from diophantine approximationarises as a key tool. Along the way, we also derive new quantitative bounds onthe real zero sets of n-variate (n+2)-nomials."}
2.45e+03 pattern=  0 task=  0 (u=53597)  {'docs': u'This paper describes a novel approach to unsupervised learning that has beendeveloped within a framework of "information compression by multiple alignment,unification and search" (ICMAUS), designed to integrate learning with other AIfunctions such as parsing and production of language, fuzzy patternrecognition, probabilistic and exact forms of reasoning, and others.'}
2.44e+03 pattern=  0 task=  0 (u=53599)  {'docs': u"We show that the smoothed complexity of the logarithm of Renegar's conditionnumber is O(log (n/sigma))."}
2.7e+03 pattern=  0 task=  0 (u=53609)  {'docs': u'In this paper we introduce a dynamic programming algorithm to perform lineartext segmentation by global minimization of a segmentation cost function whichconsists of: (a) within-segment word similarity and (b) prior information aboutsegment length. The evaluation of the segmentation accuracy of the algorithm ona text collection consisting of Greek texts showed that the algorithm achieveshigh segmentation accuracy and appears to be very innovating and promissing.'}
3.09e+03 pattern=  0 task=  0 (u=54107)  {'docs': u'Geographic routing with greedy relaying strategies have been widely studiedas a routing scheme in sensor networks. These schemes assume that the nodeshave perfect information about the location of the destination. When thedistance between the source and destination is normalized to unity, theasymptotic routing delays in these schemes are $\\Theta(\\frac{1}{M(n)}),$ whereM(n) is the maximum distance traveled in a single hop (transmission range of aradio). In this paper, we consider routing scenarios where nodes have locationerrors (imprecise GPS), or where only coarse geographic information about thedestination is available, and only a fraction of the nodes have routinginformation. We show that even with such imprecise or limiteddestination-location information, the routing delays are$\\Theta(\\frac{1}{M(n)})$. We also consider the throughput-capacity of networkswith progressive routing strategies that take packets closer to the destinationin every step, but not necessarily along a straight-line. We show that thethroughput-capacity with progressive routing is order-wise the same as themaximum achievable throughput-capacity.'}
2.92e+03 pattern=  0 task=  0 (u=54279)  {'docs': u'The summation formula within pascalian triangle resulting in the fibonaccisequence is extended to the $q$-binomial coefficients $q$-gaussian triangles.'}
2.93e+03 pattern=  0 task=  1 (u=54279)  {'docs': u'Roman logarithmic binomial formula analogue has been found . It is presentedhere also for the case of fibonomial coefficients which recently have beengiven a combinatorial interpretation by the present author.'}
1.5e+03 pattern=  0 task=  0 (u=54424)  {'docs': u'Constraint Handling Rules (CHR) have provided a realistic solution to anover-arching problem in many fields that deal with constraint logicprogramming: how to combine recursive functions or relations with constraintswhile avoiding non-termination problems. This paper focuses on some otherbenefits that CHR, specifically their implementation in SICStus Prolog, haveprovided to computational linguists working on grammar design tools. CHR rulesare applied by means of a subsumption check and this check is made only whentheir variables are instantiated or bound. The former functionality is at bestdifficult to simulate using more primitive coroutining statements such asSICStus when/2, and the latter simply did not exist in any form before CHR.  For the sake of providing a case study in how these can be applied to grammardevelopment, we consider the Attribute Logic Engine (ALE), a Prologpreprocessor for logic programming with typed feature structures, and itsextension to a complete grammar development system for Head-driven PhraseStructure Grammar (HPSG), a popular constraint-based linguistic theory thatuses typed feature structures. In this context, CHR can be used not only toextend the constraint language of feature structure descriptions to includerelations in a declarative way, but also to provide support for constraintswith complex antecedents and constraints on the co-occurrence of feature valuesthat are necessary to interpret the type system of HPSG properly.'}
1.5e+03 pattern=  0 task=  0 (u=54425)  {'docs': u'Computing practice today depends on visual output to drive almost all userinteraction. Other senses, such as audition, may be totally neglected, or usedtangentially, or used in highly restricted specialized ways. We have excellentaudio rendering through D-A conversion, but we lack rich general facilities formodeling and manipulating sound comparable in quality and flexibility tographics. We need co-ordinated research in several disciplines to improve theuse of sound as an interactive information channel.  Incremental and separate improvements in synthesis, analysis, speechprocessing, audiology, acoustics, music, etc. will not alone produce theradical progress that we seek in sonic practice. We also need to create a newcentral topic of study in digital audio research. The new topic will assimilatethe contributions of different disciplines on a common foundation. The keycentral concept that we lack is sound as a general-purpose information channel.We must investigate the structure of this information channel, which is drivenby the co-operative development of auditory perception and physical soundproduction. Particular audible encodings, such as speech and music, illuminatesonic information by example, but they are no more sufficient for acharacterization than typography is sufficient for a characterization of visualinformation.'}
2.3e+03 pattern=  0 task=  0 (u=54703)  {'docs': u'The well-known $O(n^{1-1/d})$ behaviour of the optimal tour length for TSP ind-dimensional Cartesian space causes breaches of the triangle inequality. Otherpractical inadequacies of this model are discussed, including its use as basisfor approximation of the TSP optimal tour length or bounds derivations, which Iattempt to remedy.'}
2.62e+03 pattern=  0 task=  0 (u=55213)  {'docs': u"Interactions are patterns between several attributes in data that cannot beinferred from any subset of these attributes. While mutual information is awell-established approach to evaluating the interactions between twoattributes, we surveyed its generalizations as to quantify interactions betweenseveral attributes. We have chosen McGill's interaction information, which hasbeen independently rediscovered a number of times under various names invarious disciplines, because of its many intuitively appealing properties. Weapply interaction information to visually present the most importantinteractions of the data. Visualization of interactions has provided insightinto the structure of data on a number of domains, identifying redundantattributes and opportunities for constructing new features, discoveringunexpected regularities in data, and have helped during construction ofpredictive models; we illustrate the methods on numerous examples. A machinelearning method that disregards interactions may get caught in two traps:myopia is caused by learning algorithms assuming independence in spite ofinteractions, whereas fragmentation arises from assuming an interaction inspite of independence."}
2.51e+03 pattern=  0 task=  0 (u=55382)  {'docs': u'A large number of different model checking approaches has been proposedduring the last decade. The different approaches are applicable to differentmodel types including untimed, timed, probabilistic and stochastic models. Thispaper presents a new framework for model checking techniques which includessome of the known approaches, but enlarges the class of models for which modelchecking can be applied to the general class of weighted automata. The approachallows an easy adaption of model checking to models which have not beenconsidered yet for this purpose. Examples for those new model types for whichmodel checking can be applied are max/plus or min/plus automata which are wellestablished models to describe different forms of dynamic systems andoptimization problems. In this context, model checking can be used to verifytemporal or quantitative properties of a system. The paper first presentsbriefly our class of weighted automata, as a very general model type. ThenValued Computational Tree Logic (CTL$) is introduced as a natural extension ofthe well known branching time logic CTL. Afterwards, algorithms to check aweighted automaton according to a CTL$ formula are presented. As a last result,a bisimulation is presented for weighted automata and for CTL$.'}
2.65e+03 pattern=  0 task=  0 (u=55384)  {'docs': u'We introduce indexing of tables referencing complex structures such asdigraphs and spatial objects, appearing in genetics and other data intensiveanalysis. The indexing is achieved by extracting dimension schemas from thereferenced structures. The schemas and their dimensionality are determined byproper coloring algorithms and the duality between all such schemas and allsuch possible proper colorings is established. This duality, in turn, providesus with an extensive library of solutions when addressing indexing questions.It is illustrated how to use the schemas, in connection with additionalrelational database technologies, to optimize queries conditioned on thestructural information being referenced. Comparisons using bitmap indexing inthe Oracle 9.2i database, on the one hand, and multidimensional clustering inDB2 8.1.2, on the other hand, are used to illustrate the applicability of theindexing to different technology settings. Finally, we illustrate how theindexing can be used to extract low dimensional schemas from a binary intervaltree in order to resolve efficiently interval and stabbing queries.'}
2.16e+03 pattern=  0 task=  0 (u=55526)  {'docs': u"We present a nondeterministic model of computation based on reversing edgedirections in weighted directed graphs with minimum in-flow constraints onvertices. Deciding whether this simple graph model can be manipulated in orderto reverse the direction of a particular edge is shown to be PSPACE-complete bya reduction from Quantified Boolean Formulas. We prove this result in a varietyof special cases including planar graphs and highly restricted vertexconfigurations, some of which correspond to a kind of passive constraint logic.Our framework is inspired by (and indeed a generalization of) the ``GeneralizedRush Hour Logic'' developed by Flake and Baum.  We illustrate the importance of our model of computation by giving simplereductions to show that several motion-planning problems are PSPACE-hard. Ourmain result along these lines is that classic unrestricted sliding-blockpuzzles are PSPACE-hard, even if the pieces are restricted to be all dominoes(1x2 blocks) and the goal is simply to move a particular piece. No priorcomplexity results were known about these puzzles. This result can be seen as astrengthening of the existing result that the restricted Rush Hour puzzles arePSPACE-complete, of which we also give a simpler proof. Finally, we strengthenthe existing result that the pushing-blocks puzzle Sokoban is PSPACE-complete,by showing that it is PSPACE-complete even if no barriers are allowed."}
2.24e+03 pattern=  0 task=  0 (u=55667)  {'docs': u'We investigate individual packet delay in a model of data networks withtable-free, partial table and full table routing. We present analyticalestimation for the average packet delay in a network with small partial routingtable. Dependence of the delay on the size of the network and on the size ofthe partial routing table is examined numerically. Consequences for networkscalability are discussed.'}
3.24e+03 pattern=  0 task=  0 (u=55756)  {'docs': u'In this paper we compare the performance characteristics of our selectionbased learning algorithm for Web crawlers with the characteristics of thereinforcement learning algorithm. The task of the crawlers is to find newinformation on the Web. The selection algorithm, called weblog update, modifiesthe starting URL lists of our crawlers based on the found URLs containing newinformation. The reinforcement learning algorithm modifies the URL orderings ofthe crawlers based on the received reinforcements for submitted documents. Weperformed simulations based on data collected from the Web. The collectedportion of the Web is typical and exhibits scale-free small world (SFSW)structure. We have found that on this SFSW, the weblog update algorithmperforms better than the reinforcement learning algorithm. It finds the newinformation faster than the reinforcement learning algorithm and has better newinformation/all submitted documents ratio. We believe that the advantages ofthe selection algorithm over reinforcement learning algorithm is due to thesmall world property of the Web.'}
2.76e+03 pattern=  0 task=  0 (u=55912)  {'docs': u"Websites of a particular class form increasingly complex networks, and newtools are needed to map and understand them. A way of visualizing this complexnetwork is by mapping it. A map highlights which members of the community havesimilar interests, and reveals the underlying social network. In this paper, wewill map a network of websites using Kohonen's self-organizing map (SOM), aneural-net like method generally used for clustering and visualization ofcomplex data sets. The set of websites considered has been the Blogalia webloghosting site (based at http://www.blogalia.com/), a thriving community ofaround 200 members, created in January 2002. In this paper we show how SOMdiscovers interesting community features, its relation with othercommunity-discovering algorithms, and the way it highlights the set ofcommunities formed over the network."}
3e+03 pattern=  0 task=  0 (u=56031)  {'docs': u"The synchronisation of Tree Parity Machines (TPMs), has proven to provide avaluable alternative concept for secure symmetric key exchange. Yet, from acryptographer's point of view, authentication is at least as important as asecure exchange of keys. Adding an authentication via hashing e.g. isstraightforward but with no relation to Neural Cryptography. We consequentlyformulate an authenticated key exchange within this concept. Anotheralternative, integrating a Zero-Knowledge protocol into the synchronisation, isalso presented. A Man-In-The-Middle attack and even all currently knownattacks, that are based on using identically structured TPMs andsynchronisation as well, can so be averted. This in turn has practicalconsequences on using the trajectory in weight space. Both suggestions have theadvantage of not affecting the previously observed physics of this interactingsystem at all."}
2.87e+03 pattern=  0 task=  0 (u=56079)  {'docs': u'This paper explores the use of genetic algorithms for the design of networks,where the demands on the network fluctuate in time. For varying networkconstraints, we find the best network using the standard genetic algorithmoperators such as inversion, mutation and crossover. We also examine how thechoice of genetic algorithm operators affects the quality of the best networkfound. Such networks typically contain redundancy in servers, where severalservers perform the same task and pleiotropy, where servers perform multipletasks. We explore this trade-off between pleiotropy versus redundancy on thecost versus reliability as a measure of the quality of the network.'}
2.87e+03 pattern=  0 task=  1 (u=56079)  {'docs': u'Evolutionary computation algorithms are increasingly being used to solveoptimization problems as they have many advantages over traditionaloptimization algorithms. In this paper we use evolutionary computation to studythe trade-off between pleiotropy and redundancy in a client-server basednetwork. Pleiotropy is a term used to describe components that perform multipletasks, while redundancy refers to multiple components performing one same task.Pleiotropy reduces cost but lacks robustness, while redundancy increasesnetwork reliability but is more costly, as together, pleiotropy and redundancybuild flexibility and robustness into systems. Therefore it is desirable tohave a network that contains a balance between pleiotropy and redundancy. Weexplore how factors such as link failure probability, repair rates, and thesize of the network influence the design choices that we explore using geneticalgorithms.'}
2.86e+03 pattern=  0 task=  0 (u=56083)  {'docs': u'We present an extensive analysis of long-term statistics of the queries towebsites using logs collected on several web caches in Russian academicnetworks and on US IRCache caches. We check the sensitivity of the statisticsto several parameters: (1) duration of data collection, (2) geographicallocation of the cache server collecting data, and (3) the year of datacollection. We propose a two-parameter modification of the Zipf law andinterpret the parameters. We find that the rank distribution of websites isstable when approximated by the modified Zipf law. We suggest that websitepopularity may be a universal property of Internet.'}
1.81e+03 pattern=  0 task=  0 (u=56101)  {'docs': u'Unlike traditional reinforcement learning (RL), market-based RL is inprinciple applicable to worlds described by partially observable MarkovDecision Processes (POMDPs), where an agent needs to learn short-term memoriesof relevant previous events in order to execute optimal actions. Most previouswork, however, has focused on reactive settings (MDPs) instead of POMDPs. Herewe reimplement a recent approach to market-based RL and for the first timeevaluate it in a toy POMDP setting.'}
2e+03 pattern=  0 task=  1 (u=56101)  {'docs': u"We introduce a learning method called ``gradient-based reinforcementplanning'' (GREP). Unlike traditional DP methods that improve their policybackwards in time, GREP is a gradient-based method that plans ahead andimproves its policy before it actually acts in the environment. We deriveformulas for the exact policy gradient that maximizes the expected futurereward and confirm our ideas with numerical experiments."}
1.26e+03 pattern=  0 task=  0 (u=56179)  {'docs': u'We survey recent developments in the study of (worst-case) one-way functionshaving strong algebraic and security properties. According to [RS93], this lineof research was initiated in 1984 by Rivest and Sherman who designed two-partysecret-key agreement protocols that use strongly noninvertible, total,associative one-way functions as their key building blocks. If commutativity isadded as an ingredient, these protocols can be used by more than two parties,as noted by Rabi and Sherman [RS93] who also developed digital signatureprotocols that are based on such enhanced one-way functions.  Until recently, it was an open question whether one-way functions having thealgebraic and security properties that these protocols require could be createdfrom any given one-way function. Recently, Hemaspaandra and Rothe [HR99]resolved this open issue in the affirmative, by showing that one-way functionsexist if and only if strong, total, commutative, associative one-way functionsexist.  We discuss this result, and the work of Rabi, Rivest, and Sherman, and recentwork of Homan [Hom99] that makes progress on related issues.'}
2.75e+03 pattern=  0 task=  0 (u=56227)  {'docs': u'Even when a system is proven to be correct with respect to a specification,there is still a question of how complete the specification is, and whether itreally covers all the behaviors of the system. Coverage metrics attempt tocheck which parts of a system are actually relevant for the verificationprocess to succeed. Recent work on coverage in model checking suggests severalcoverage metrics and algorithms for finding parts of the system that are notcovered by the specification. The work has already proven to be effective inpractice, detecting design errors that escape early verification efforts inindustrial settings. In this paper, we relate a formal definition of causalitygiven by Halpern and Pearl [2001] to coverage. We show that it givessignificant insight into unresolved issues regarding the definition of coverageand leads to potentially useful extensions of coverage. In particular, weintroduce the notion of responsibility, which assigns to components of a systema quantitative measure of their relevance to the satisfaction of thespecification.'}
2.35e+03 pattern=  0 task=  0 (u=56299)  {'docs': u'We investigate the notion of cyclicity for convolutional codes as it has beenintroduced by Piret and Roos in the seventies. Codes of this type are describedas submodules of the module of all vector polynomials in one variable with someadditional generalized cyclic structure but also as specific left ideals in askew polynomial ring. Extending a result of Piret, we show in a purelyalgebraic setting that these ideals are always principal. This leads to thenotion of a generator polynomial just like for cyclic block codes. Similarly acontrol polynomial can be introduced by considering the right annihilatorideal. An algorithmic procedure is developed which produces unique reducedgenerator and control polynomials. We also show how basic code properties and aminimal generator matrix can be read off from these objects. A close linkbetween polynomial and vector description of the codes is provided by certaingeneralized circulant matrices.'}
1.63e+03 pattern=  0 task=  0 (u=56431)  {'docs': u"We characterize those intersection-type theories which yield completeintersection-type assignment systems for lambda-calculi, with respect to thethree canonical set-theoretical semantics for intersection-types: the inferencesemantics, the simple semantics and the F-semantics. These semantics arise bytaking as interpretation of types subsets of applicative structures, asinterpretation of the intersection constructor set-theoretic inclusion, and bytaking the interpretation of the arrow constructor a' la Scott, with respect toeither any possible functionality set, or the largest one, or the least one.  These results strengthen and generalize significantly all earlier results inthe literature, to our knowledge, in at least three respects. First of all theinference semantics had not been considered before. Secondly, thecharacterizations are all given just in terms of simple closure conditions onthe preorder relation on the types, rather than on the typing judgmentsthemselves. The task of checking the condition is made therefore considerablymore tractable. Lastly, we do not restrict attention just to lambda-models, butto arbitrary applicative structures which admit an interpretation function.Thus we allow also for the treatment of models of restricted lambda-calculi.Nevertheless the characterizations we give can be tailored just to the case oflambda-models."}
3.27e+03 pattern=  0 task=  0 (u=56554)  {'docs': u'A trusted quantum relay is introduced to enable quantum key distributionlinks to form the basic legs in a quantum key distribution network. The idea isbased on the well-known intercept/resend eavesdropping. The same scheme can beused to make quantum key distribution between several parties. No entanglementis required.'}
1.03e+03 pattern=  0 task=  0 (u=56673)  {'docs': u"We present a new approach to the simulation and analysis of immune systembehavior. The simulations that can be done with our software package calledSIMMUNE are based on immunological data that describe the behavior of immunesystem agents (cells, molecules) on a microscopial (i.e. agent-agentinteraction) scale by defining cellular stimulus-response mechanisms. Since thebehavior of the agents in SIMMUNE can be very flexibly configured, itsapplication is not limited to immune system simulations. We outline theprinciples of SIMMUNE's multiscale analysis of emergent structure within thesimulated immune system that allow the identification of immunological contextsusing minimal a priori assumptions about the higher level organization of theimmune system."}
1.02e+03 pattern=  0 task=  0 (u=56675)  {'docs': u'We present an open architecture for just-in-time code generation and dynamiccode optimization that is flexible, customizable, and extensible. Whileprevious research has primarily investigated functional aspects of such asystem, architectural aspects have so far remained unexplored. In this paper,we argue that these properties are important to generate optimal code for avariety of hardware architectures and different processor generations withinprocessor families. These properties are also important to make system-levelcode generation useful in practice.'}
2.42e+03 pattern=  0 task=  0 (u=56752)  {'docs': u'XML is of great importance in information storage and retrieval because ofits recent emergence as a standard for data representation and interchange onthe Internet. However XML provides little semantic content and as a resultseveral papers have addressed the topic of how to improve the semanticexpressiveness of XML. Among the most important of these approaches has beenthat of defining integrity constraints in XML. In a companion paper we definedstrong functional dependencies in XML(XFDs). We also presented a set of axiomsfor reasoning about the implication of XFDs and showed that the axiom system issound for arbitrary XFDs. In this paper we prove that the axioms are alsocomplete for unary XFDs (XFDs with a single path on the l.h.s.). The secondcontribution of the paper is to prove that the implication problem for unaryXFDs is decidable and to provide a linear time algorithm for it.'}
3.07e+03 pattern=  0 task=  0 (u=56788)  {'docs': u'We generalize the definition of a counter and counter reversal complexity andinvestigate the power of generalized deterministic counter automata in terms oflanguage recognition.'}
2.5e+03 pattern=  0 task=  0 (u=56792)  {'docs': u'Standard quantitative models of the stock market predict a log-normaldistribution for stock returns (Bachelier 1900, Osborne 1959), but it isrecognised (Fama 1965) that empirical data, in comparison with a Gaussian,exhibit leptokurtosis (it has more probability mass in its tails and centre)and fat tails (probabilities of extreme events are underestimated). Differentattempts to explain this departure from normality have coexisted. Inparticular, since one of the strong assumptions of the Gaussian model concernsthe volatility, considered finite and constant, the new models were built on anon finite (Mandelbrot 1963) or non constant (Cox, Ingersoll and Ross 1985)volatility. We investigate in this thesis a very recent model (Dragulescu etal. 2002) based on a Brownian motion process for the returns, and a stochasticmean-reverting process for the volatility. In this model, the forwardKolmogorov equation that governs the time evolution of returns is solvedanalytically. We test this new theory against different stock indexes (DowJones Industrial Average, Standard and Poor s and Footsie), over differentperiods (from 20 to 105 years). Our aim is to compare this model with theclassical Gaussian and with a simple Neural Network, used as a benchmark. Weperform the usual statistical tests on the kurtosis and tails of the expecteddistributions, paying particular attention to the outliers. As claimed by theauthors, the new model outperforms the Gaussian for any time lag, but isartificially too complex for medium and low frequencies, where the Gaussian ispreferable. Moreover this model is still rejected for high frequencies, at a0.05 level of significance, due to the kurtosis, incorrectly handled.'}
2.71e+03 pattern=  0 task=  0 (u=56967)  {'docs': u"Argumentation has proved a useful tool in defining formal semantics forassumption-based reasoning by viewing a proof as a process in which proponentsand opponents attack each others arguments by undercuts (attack to anargument's premise) and rebuts (attack to an argument's conclusion). In thispaper, we formulate a variety of notions of attack for extended logic programsfrom combinations of undercuts and rebuts and define a general hierarchy ofargumentation semantics parameterised by the notions of attack chosen byproponent and opponent. We prove the equivalence and subset relationshipsbetween the semantics and examine some essential properties concerningconsistency and the coherence principle, which relates default negation andexplicit negation. Most significantly, we place existing semantics put forwardin the literature in our hierarchy and identify a particular argumentationsemantics for which we prove equivalence to the paraconsistent well-foundedsemantics with explicit negation, WFSX$_p$. Finally, we present a general prooftheory, based on dialogue trees, and show that it is sound and complete withrespect to the argumentation semantics."}
2.71e+03 pattern=  0 task=  0 (u=56976)  {'docs': u"In this paper we outline an approach of applying model-based diagnosis to thefield of automatic software debugging of hardware designs. We present ourvalue-level model for debugging VHDL-RTL designs and show how to localize theerroneous component responsible for an observed misbehavior. Furthermore, wediscuss an extension of our model that supports the debugging of sequentialcircuits, not only at a given point in time, but also allows for consideringthe temporal behavior of VHDL-RTL designs. The introduced model is capable ofhandling state inherently present in every sequential circuit. The principalapplicability of the new model is outlined briefly and we use industrial-sizedreal world examples from the ISCAS'85 benchmark suite to discuss thescalability of our approach."}
1.31e+03 pattern=  0 task=  0 (u=57166)  {'docs': u'The multiplicative Newton-like method developed by the author et al. isextended to the situation where the dynamics is restricted to the orthogonalgroup. A general framework is constructed without specifying the cost function.Though the restriction to the orthogonal groups makes the problem somewhatcomplicated, an explicit expression for the amount of individual jumps isobtained. This algorithm is exactly second-order-convergent. The globalinstability inherent in the Newton method is remedied by aLevenberg-Marquardt-type variation. The method thus constructed can readily beapplied to the independent component analysis. Its remarkable performance isillustrated by a numerical simulation.'}
2.73e+03 pattern=  0 task=  0 (u=57339)  {'docs': u'We define XPathLog as a Datalog-style extension of XPath. XPathLog provides aclear, declarative language for querying and manipulating XML whoseperspectives are especially in XML data integration. In our characterization,the formal semantics is defined wrt. an edge-labeled graph-based model whichcovers the XML data model. We give a complete, logic-based characterization ofXML data and the main language concept for XML, XPath. XPath-Logic extends theXPath language with variable bindings and embeds it into first-order logic.XPathLog is then the Horn fragment of XPath-Logic, providing a Datalog-style,rule-based language for querying and manipulating XML data. The model-theoreticsemantics of XPath-Logic serves as the base of XPathLog as a logic-programminglanguage, whereas also an equivalent answer-set semantics for evaluatingXPathLog queries is given. In contrast to other approaches, the XPath syntaxand semantics is also used for a declarative specification how the databaseshould be updated: when used in rule heads, XPath filters are interpreted asspecifications of elements and properties which should be added to thedatabase.'}
2.65e+03 pattern=  0 task=  0 (u=57362)  {'docs': u'We propose a family of very efficient hierarchical indexing schemes forungapped, score matrix-based similarity search in large datasets of short (4-12amino acid) protein fragments. This type of similarity search has importance inboth providing a building block to more complex algorithms and for possible usein direct biological investigations where datasets are of the order of 60million objects. Our scheme is based on the internal geometry of the amino acidalphabet and performs exceptionally well, for example outputting 100 nearestneighbours to any possible fragment of length 10 after scanning on average lessthan one per cent of the entire dataset.'}
3.22e+03 pattern=  0 task=  0 (u=57530)  {'docs': u'An improved inference method for densely connected systems is presented. Theapproach is based on passing condensed messages between variables, representingmacroscopic averages of microscopic messages. We extend previous work thatshowed promising results in cases where the solution space is contiguous tocases where fragmentation occurs. We apply the method to the signal detectionproblem of Code Division Multiple Access (CDMA) for demonstrating itspotential. A highly efficient practical algorithm is also derived on the basisof insight gained from the analysis.'}
1.73e+03 pattern=  0 task=  0 (u=57568)  {'docs': u'It has been designed,built and executed a code for the Fast Fourier Transform(FFT),compiled and executed in a cluster of 2^n computers under the operatingsystem MacOS and using the routines MacMPI. As practical application,the codehas been used to obtain the transformed from an astronomic imagen,to execute afilter on its and with a transformed inverse to recover the image with thevariates given by the filter.The computers arrangement are installed in theObservatorio Astronomico National in Colombia under the name OAN Cluster and inthis has been executed several applications.'}
3.17e+03 pattern=  0 task=  0 (u=57582)  {'docs': u'We consider the problem of maintaining a dynamic set of integers andanswering queries of the form: report a point (equivalently, all points) in agiven interval. Range searching is a natural and fundamental variant of integersearch, and can be solved using predecessor search. However, for a RAM withw-bit words, we show how to perform updates in O(lg w) time and answer queriesin O(lglg w) time. The update time is identical to the van Emde Boas structure,but the query time is exponentially faster. Existing lower bounds show thatachieving our query time for predecessor search requires doubly-exponentiallyslower updates. We present some arguments supporting the conjecture that oursolution is optimal.  Our solution is based on a new and interesting recursion idea which is "moreextreme" that the van Emde Boas recursion. Whereas van Emde Boas uses a simplerecursion (repeated halving) on each path in a trie, we use a nontrivial, vanEmde Boas-like recursion on every such path. Despite this, our algorithm isquite clean when seen from the right angle. To achieve linear space for ourdata structure, we solve a problem which is of independent interest. We developthe first scheme for dynamic perfect hashing requiring sublinear space. Thisgives a dynamic Bloomier filter (an approximate storage scheme for sparsevectors) which uses low space. We strengthen previous lower bounds to show thatthese results are optimal.'}
2.84e+03 pattern=  0 task=  0 (u=57649)  {'docs': u'Many functional logic languages are based on narrowing, a unification-basedgoal-solving mechanism which subsumes the reduction mechanism of functionallanguages and the resolution principle of logic languages. Needed narrowing isan optimal evaluation strategy which constitutes the basis of modern(narrowing-based) lazy functional logic languages. In this work, we present thefundamentals of partial evaluation in such languages. We provide correctnessresults for partial evaluation based on needed narrowing and show that the niceproperties of this strategy are essential for the specialization process. Inparticular, the structure of the original program is preserved by partialevaluation and, thus, the same evaluation strategy can be applied for theexecution of specialized programs. This is in contrast to other partialevaluation schemes for lazy functional logic programs which may change theprogram structure in a negative way. Recent proposals for the partialevaluation of declarative multi-paradigm programs use (some form of) needednarrowing to perform computations at partial evaluation time. Therefore, ourresults constitute the basis for the correctness of such partial evaluators.'}
2.84e+03 pattern=  0 task=  0 (u=57651)  {'docs': u'Many dependability techniques expect certain behaviors from the underlyingsubsystems and fail in chaotic ways if these expectations are not met. Underexpected circumstances, however, software tends to work quite well. This papersuggests that, instead of fixing elusive bugs or rewriting software, we improvethe predictability of conditions faced by our programs. This approach might bea cheaper and faster way to improve dependability of software. Afteridentifying some of the common triggers of unpredictability, the paperdescribes three engineering principles that hold promise in combatingunpredictability, suggests a way to benchmark predictability, and outlines abrief research agenda.'}
2.84e+03 pattern=  0 task=  0 (u=57652)  {'docs': u'Similarity searching finds application in a wide variety of domains includingmultilingual databases, computational biology, pattern recognition and textretrieval. Similarity is measured in terms of a distance function, editdistance, in general metric spaces, which is expensive to compute. Indexingtechniques can be used reduce the number of distance computations. We presentan analysis of various existing similarity indexing structures for the same.The performance obtained using the index structures studied was found to beunsatisfactory . We propose an indexing technique that combines the features ofclustering with M tree(MTB) and the results indicate that this gives betterperformance.'}
3.29e+03 pattern=  0 task=  0 (u=57836)  {'docs': u'We consider various issues which arise as soon as one tries to practicallyimplement simple networks of quantum relays for QKD. In particular we discussauthentication and routing which are essential ingredients of any QKD network.This paper aims to address some gaps between quantum and networking aspects ofQKD networks usually reserved to specialist in physics and computer sciencerespectively.'}
2.59e+03 pattern=  0 task=  0 (u=57853)  {'docs': u"We introduce a model of computation based on quaternions, which is inspiredon the quantum computing model. Pure states are vectors of a suitable linearspace over the quaternions. Other aspects of the theory are the same as inquantum computing: superposition and linearity of the state space, unitarity ofthe transformations, and projective measurements. However, one notableexception is the fact that quaternionic circuits do not have a uniquely definedbehaviour, unless a total ordering of evaluation of the gates is defined. Givensuch an ordering a unique unitary operator can be associated with thequaternionic circuit and a proper semantics of computation can be associatedwith it.  The main result of this paper consists in showing that this model is no morepowerful than quantum computing, as long as such an ordering of gates can bedefined. More concretely we show, that for all quaternionic computation using nquaterbits, the behaviour of the circuit for each possible gate ordering can besimulated with n+1 qubits, and this with little or no overhead in circuit size.The proof of this result is inspired of a new simplified and improved proof ofthe equivalence of a similar model based on real amplitudes to quantumcomputing, which states that any quantum computation using n qubits can besimulated with n+1 rebits, and in this with no circuit size overhead.  Beyond this potential computational equivalence, however, we propose thismodel as a simpler framework in which to discuss the possibility of aquaternionic quantum mechanics or information theory. In particular, it alreadyallows us to illustrate that the introduction of quaternions might violate someof the ``natural'' properties that we have come to expect from physical models."}
1.97e+03 pattern=  0 task=  0 (u=57881)  {'docs': u'In this paper we address the complexity of solving linear programmingproblems with a set of differential equations that converge to a fixed pointthat represents the optimal solution. Assuming a probabilistic model, where theinputs are i.i.d. Gaussian variables, we compute the distribution of theconvergence rate to the attracting fixed point. Using the framework of RandomMatrix Theory, we derive a simple expression for this distribution in theasymptotic limit of large problem size. In this limit, we find that thedistribution of the convergence rate is a scaling function, namely it is afunction of one variable that is a combination of three parameters: the numberof variables, the number of constraints and the convergence rate, rather than afunction of these parameters separately. We also estimate numerically thedistribution of computation times, namely the time required to reach a vicinityof the attracting fixed point, and find that it is also a scaling function.Using the problem size dependence of the distribution functions, we derive highprobability bounds on the convergence rates and on the computation times.'}
  863 pattern=  1 task=  0 (u=57909)  {'docs': u"The Abstract Syntax Description Language (ASDL) is a language for specifyingthe tree data structures often found in compiler intermediate representations.The ASDL generator reads an ASDL specification and generates code to construct,read, and write instances of the trees specified. Using ASDL permits a compilerto be decomposed into semi-independent components that communicate by readingand writing trees. Each component can be written in a different language,because the ASDL generator can emit code in several languages, and the fileswritten by ASDL-generated code are machine- and language-independent. ASDL ispart of the National Compiler Infrastructure project, which seeks to reducedramatically the overhead of computer systems research by making it much easierto build high-quality compilers. This paper describes dividing lcc, a widelyused retargetable C compiler, into two components that communicate via treesdefined in ASDL. As the first use of ASDL in a `real' compiler, this experiencereveals much about the effort required to retrofit an existing compiler to useASDL, the overheads involved, and the strengths and weaknesses of ASDL itselfand, secondarily, of lcc."}
3.25e+03 pattern=  0 task=  0 (u=58002)  {'docs': u"This paper describes a comprehensive prototype of large-scale fault adaptiveembedded software developed for the proposed Fermilab BTeV high energy physicsexperiment. Lightweight self-optimizing agents embedded within Level 1 of theprototype are responsible for proactive and reactive monitoring and mitigationbased on specified layers of competence. The agents are self-protecting,detecting cascading failures using a distributed approach. Adaptive,reconfigurable, and mobile objects for reliablility are designed to beself-configuring to adapt automatically to dynamically changing environments.These objects provide a self-healing layer with the ability to discover,diagnose, and react to discontinuities in real-time processing. A genericmodeling environment was developed to facilitate design and implementation ofhardware resource specifications, application data flow, and failure mitigationstrategies. Level 1 of the planned BTeV trigger system alone will consist of2500 DSPs, so the number of components and intractable fault scenarios involvedmake it impossible to design an `expert system' that applies traditionalcentralized mitigative strategies based on rules capturing every possiblesystem state. Instead, a distributed reactive approach is implemented using thetools and methodologies developed by the Real-Time Embedded Systems group."}
2.41e+03 pattern=  0 task=  0 (u=58169)  {'docs': u'We consider combinatorial optimization problems defined over randomensembles, and study how solution cost increases when the optimal solutionundergoes a small perturbation delta. For the minimum spanning tree, theincrease in cost scales as delta^2; for the mean-field and Euclidean minimummatching and traveling salesman problems in dimension d>=2, the increase scalesas delta^3; this is observed in Monte Carlo simulations in d=2,3,4 and intheoretical analysis of a mean-field model. We speculate that the scalingexponent could serve to classify combinatorial optimization problems into asmall number of distinct categories, similar to universality classes instatistical physics.'}
2.56e+03 pattern=  0 task=  0 (u=58576)  {'docs': u'Dataset storage, exchange, and access play a critical role in scientificapplications. For such purposes netCDF serves as a portable and efficient fileformat and programming interface, which is popular in numerous scientificapplication domains. However, the original interface does not provide anefficient mechanism for parallel data storage and access. In this work, wepresent a new parallel interface for writing and reading netCDF datasets. Thisinterface is derived with minimum changes from the serial netCDF interface butdefines semantics for parallel access and is tailored for high performance. Theunderlying parallel I/O is achieved through MPI-IO, allowing for dramaticperformance gains through the use of collective I/O optimizations. We comparethe implementation strategies with HDF5 and analyze both. Our tests indicateprogramming convenience and significant I/O performance improvement with thisparallel netCDF interface.'}
3.03e+03 pattern=  0 task=  0 (u=58921)  {'docs': u"We present results from the first geological field tests of the `CyborgAstrobiologist', which is a wearable computer and video camcorder system thatwe are using to test and train a computer-vision system towards having some ofthe autonomous decision-making capabilities of a field-geologist. The CyborgAstrobiologist platform has thus far been used for testing and development ofthese algorithms and systems: robotic acquisition of quasi-mosaics of images,real-time image segmentation, and real-time determination of interesting pointsin the image mosaics. The hardware and software systems function reliably, andthe computer-vision algorithms are adequate for the first field tests. Inaddition to the proof-of-concept aspect of these field tests, the main resultof these field tests is the enumeration of those issues that we can improve inthe future, including: dealing with structural shadow and microtexture, andalso, controlling the camera's zoom lens in an intelligent manner. Nonetheless,despite these and other technical inadequacies, this Cyborg Astrobiologistsystem, consisting of a camera-equipped wearable-computer and itscomputer-vision algorithms, has demonstrated its ability of finding genuinelyinteresting points in real-time in the geological scenery, and then gatheringmore information about these interest points in an automated manner."}
3.08e+03 pattern=  0 task=  0 (u=59365)  {'docs': u"The most advanced implementation of adaptive constraint processing withConstraint Handling Rules (CHR) allows the application of intelligent searchstrategies to solve Constraint Satisfaction Problems (CSP). This presentationcompares an improved version of conflict-directed backjumping and two variantsof dynamic backtracking with respect to chronological backtracking on some ofthe AIM instances which are a benchmark set of random 3-SAT problems. A CHRimplementation of a Boolean constraint solver combined with these differentsearch strategies in Java is thus being compared with a CHR implementation ofthe same Boolean constraint solver combined with chronological backtracking inSICStus Prolog. This comparison shows that the addition of ``intelligence'' tothe search process may reduce the number of search steps dramatically.Furthermore, the runtime of their Java implementations is in most cases fasterthan the implementations of chronological backtracking. More specifically,conflict-directed backjumping is even faster than the SICStus Prologimplementation of chronological backtracking, although our Java implementationof CHR lacks the optimisations made in the SICStus Prolog system. To appear inTheory and Practice of Logic Programming (TPLP)."}
3.08e+03 pattern=  0 task=  0 (u=59366)  {'docs': u"We examine the structure of families of distortion balls from the perspectiveof Kolmogorov complexity. Special attention is paid to the canonicalrate-distortion function of a source word which returns the minimal Kolmogorovcomplexity of all distortion balls containing that word subject to a bound ontheir cardinality. This canonical rate-distortion function is related to themore standard algorithmic rate-distortion function for the given distortionmeasure. Examples are given of list distortion, Hamming distortion, andEuclidean distortion. The algorithmic rate-distortion function can behavedifferently from Shannon's rate-distortion function. To this end, we show thatthe canonical rate-distortion function can and does assume a wide class ofshapes (unlike Shannon's); we relate low algorithmic mutual information to lowKolmogorov complexity (and consequently suggest that certain aspects of themutual information formulation of Shannon's rate-distortion function behavedifferently than would an analogous formulation using algorithmic mutualinformation); we explore the notion that low Kolmogorov complexity distortionballs containing a given word capture the interesting properties of that word(which is hard to formalize in Shannon's theory) and this suggests an approachto denoising; and, finally, we show that the different behavior of therate-distortion curves of individual source words to some extent disappearsafter averaging over the source words."}
3.08e+03 pattern=  0 task=  0 (u=59367)  {'docs': u'Modern Ethernet switches support many advanced features beyond route learningand packet forwarding such as VLAN tagging, IGMP snooping, rate limiting, andstatus monitoring, which can be controlled through a programmatic interface.Traditionally, these features are mostly used to statically configure anetwork. This paper proposes to apply them as dynamic control mechanisms tomaximize physical network link resources, to minimize failure recovery time, toenforce QoS requirements, and to support link-layer multicast withoutbroadcasting. With these advanced programmable control mechanisms, standardEthernet switches can be used as effective building blocks formetropolitan-area Ethernet networks (MEN), storage-area networks (SAN), andcomputation cluster interconnects. We demonstrate the usefulness of this newlevel of control over Ethernet switches with a MEN architecture that featuresmulti-fold throughput gains and sub-second failure recovery time.'}
2.87e+03 pattern=  0 task=  0 (u=59776)  {'docs': u"This paper investigates the different effects of chaotic switching onParrondo's games, as compared to random and periodic switching. The rate ofwinning of Parrondo's games with chaotic switching depends on coefficient(s)defining the chaotic generator, initial conditions of the chaotic sequence andthe proportion of Game A played. Maximum rate of winning can be obtained withall the above mentioned factors properly set, and this occurs when chaoticswitching approaches periodic behavior."}
2.96e+03 pattern=  0 task=  0 (u=59918)  {'docs': u'We propose a new two-dimensional meshing algorithm called PINW able togenerate meshes that accurately approximate the distance between any two domainpoints by paths composed only of cell edges. This technique is based on anextension of pinwheel tilings proposed by Radin and Conway. We prove that thealgorithm produces triangles of bounded aspect ratio. This kind of mesh wouldbe useful in cohesive interface finite element modeling when the crackpropagation pathis an outcome of a simulation process.'}
2.78e+03 pattern=  0 task=  0 (u=59949)  {'docs': u'We analyze the evolution of the local simulation times (LST) in ParallelDiscrete Event Simulations. The new ingredients introduced are i) we associatethe LST with the nodes and not with the processing elements, and 2) we proposeto minimize the exchange of information between different processing elementsby freezing the LST on the boundaries between processing elements for some timeof processing and then releasing them by a wide-stream memory exchange betweenprocessing elements. Highlights of our approach are i) it keeps the highestlevel of processor time utilization during the algorithm evolution, ii) ittakes a reasonable time for the memory exchange excluding the time-consumingand complicated process of message exchange between processors, and iii) thecommunication between processors is decoupled from the calculations performedon a processor. The effectiveness of our algorithm grows with the number ofnodes (or threads). This algorithm should be applicable for any parallelsimulation with short-range interactions, including parallel or gridsimulations of partial differential equations.'}
1.84e+03 pattern=  0 task=  0 (u=59988)  {'docs': u'We describe a model of a communication network that allows us to pricecomplex network services as financial derivative contracts based on the spotprice of the capacity in individual routers. We prove a theorem of a Girsanovtransform that is useful for pricing linear derivatives on underlying assets,which can be used to price many complex network services, and it is used toprice an option that gives access to one of several virtual channels betweentwo network nodes, during a specified future time interval. We give thecontinuous time hedging strategy, for which the option price is independent ofthe service providers attitude towards risk. The option price contains thedensity function of a sum of lognormal variables, which has to be evaluatednumerically.'}
3.22e+03 pattern=  0 task=  0 (u=60544)  {'docs': u'This paper represents an approach to creating global knowledge systems, usingnew philosophy and infrastructure of global distributed semantic network (frameknowledge representation system) based on the space-time database construction.The main idea of the space-time database environment introduced in the paper isto bind a document (an information frame, a knowledge) to a special kind ofentity, that we call permanent entity, -- an object without history andevolution, described by a "point" in the generalized, informational space-time(not an evolving object in the real space having history). For documents(information) it means that document content is unchangeable, and documents areabsolutely persistent. This approach leads to new knowledge representation andretreival techniques. We discuss the way of applying the concept to a globaldistributed scientific library and scientific workspace. Some practical aspectsof the work are elaborated by the open IT project athttp://sourceforge.net/projects/gil/.'}
2.21e+03 pattern=  0 task=  0 (u=60659)  {'docs': u'The problem of searchability in decentralized complex networks is of greatimportance in computer science, economy and sociology. We present a formalismthat is able to cope simultaneously with the problem of search and thecongestion effects that arise when parallel searches are performed, and obtainexpressions for the average search cost--written in terms of the searchalgorithm and the topological properties of the network--both in presence andabscence of congestion. This formalism is used to obtain optimal networkstructures for a system using a local search algorithm. It is found that onlytwo classes of networks can be optimal: star-like configurations, when thenumber of parallel searches is small, and homogeneous-isotropic configurations,when the number of parallel searches is large.'}
3.16e+03 pattern=  0 task=  0 (u=60824)  {'docs': u'We show that the model of quantum computation based on density matrices andsuperoperators can be decomposed in a pure classical (functional) part and aneffectful part modeling probabilities and measurement. The effectful part canbe modeled using a generalization of monads called arrows. We express theresulting executable model of quantum computing in the programming languageHaskell using its special syntax for arrow computations. The embedding inHaskell is however not perfect: a faithful model of quantum computing requirestype capabilities which are not directly expressible in Haskell.'}
2.67e+03 pattern=  0 task=  0 (u=60861)  {'docs': u'In this paper, we propose a mathematical framework for automated buglocalization. This framework can be briefly summarized as follows. A programexecution can be represented as a rooted acyclic directed graph. We define anexecution snapshot by a cut-set on the graph. A program state can be regardedas a conjunction of labels on edges in a cut-set. Then we argue that adebugging task is a pruning process of the execution graph by using cut-sets. Apruning algorithm, i.e., a debugging task, is also presented.'}
3.11e+03 pattern=  0 task=  0 (u=60923)  {'docs': u'This report presents Jartege, a tool which allows random generation of unittests for Java classes specified in JML. JML (Java Modeling Language) is aspecification language for Java which allows one to write invariants forclasses, and pre- and postconditions for operations. As in the JML-JUnit tool,we use JML specifications on the one hand to eliminate irrelevant test cases,and on the other hand as a test oracle. Jartege randomly generates test cases,which consist of a sequence of constructor and method calls for the classesunder test. The random aspect of the tool can be parameterized by associatingweights to classes and operations, and by controlling the number of instanceswhich are created for each class under test. The practical use of Jartege isillustrated by a small case study.'}
3.11e+03 pattern=  0 task=  0 (u=60930)  {'docs': u'The paper gives a brief review of the expectation-maximization algorithm(Dempster 1977) in the comprehensible framework of discrete mathematics. InSection 2, two prominent estimation methods, the relative-frequency estimationand the maximum-likelihood estimation are presented. Section 3 is dedicated tothe expectation-maximization algorithm and a simpler variant, the generalizedexpectation-maximization algorithm. In Section 4, two loaded dice are rolled. Amore interesting example is presented in Section 5: The estimation ofprobabilistic context-free grammars.'}
3.1e+03 pattern=  0 task=  0 (u=60997)  {'docs': u'In this paper a conditional logic is defined and studied. This conditionallogic, Deterministic Bayesian Logic, is constructed as a deterministiccounterpart to the (probabilistic) Bayesian conditional. The logic isunrestricted, so that any logical operations are allowed. This logic is shownto be non-trivial and is not reduced to classical propositions. The Bayesianconditional of DBL implies a definition of logical independence. Interestingresults are derived about the interactions between the logical independence andthe proofs. A model is constructed for the logic. Completeness results areproved. It is shown that any unconditioned probability can be extended to thewhole logic DBL. The Bayesian conditional is then recovered from theprobabilistic DBL. At last, it is shown why DBL is compliant with Lewistriviality.'}
3.1e+03 pattern=  0 task=  0 (u=60998)  {'docs': u"This research analyzes complex networks in open-source software at theinter-package level, where package dependencies often span across projects andbetween development groups. We review complex networks identified at ``lower''levels of abstraction, and then formulate a description of interacting softwarecomponents at the package level, a relatively ``high'' level of abstraction. Bymining open-source software repositories from two sources, we empirically showthat the coupling of modules at this granularity creates a small-world andscale-free network in both instances."}
3.09e+03 pattern=  0 task=  0 (u=61159)  {'docs': u'Internet is growing at a fast pace. The link speeds are surging toward 40Gbps with the emergence of faster link technologies. New applications arecoming up which require intelligent processing at the intermediate routers.Switches and routers are becoming the bottlenecks in fast communication. On onehand faster links deliver more packets every second and on the other handintelligent processing consumes more CPU cycles at the router. The conflictinggoals of providing faster but computationally expensive processing call for newapproaches in designing routers.  This survey takes a look at the core functionalities, like packetclassification, buffer memory management, switch scheduling and output linkscheduling performed by a router in its data path processing and discusses thealgorithms that aim to reduce the performance bound for these operations. Animportant requirement for the routers is to provide Quality of Serviceguarantees. We propose an algorithm to guarantee QoS in Input Queued Routers.The hardware solution to speed up router operation was Application SpecificIntegrated Circuits (ASICs). But the inherent inflexibility of the method is ademerit as network standards and application requirements are constantlyevolving, which seek a faster turnaround time to keep up with the changes. Thepromise of Network Processors (NP) is the flexibility of general-purposeprocessors together with the speed of ASICs. We will study the architecturalchoices for the design of Network Processors and focus on some of thecommercially available NPs. There is a plethora of NP vendors in the market.The discussion on the NP benchmarks sets the normalizing platform to evaluatethese NPs.'}
1.15e+03 pattern=  0 task=  0 (u=61500)  {'docs': u'One way of suggesting that an NP problem may not be NP-complete is to showthat it is in the class UP. We suggest an analogous new approach---weaker instrength of evidence but more broadly applicable---to suggesting thatconcrete~NP problems are not NP-complete. In particular we introduce the classEP, the subclass of NP consisting of those languages accepted by NP machinesthat when they accept always have a number of accepting paths that is a powerof two. Since if any NP-complete set is in EP then all NP sets are in EP, itfollows---with whatever degree of strength one believes that EP differs fromNP---that membership in EP can be viewed as evidence that a problem is notNP-complete.  We show that the negation equivalence problem for OBDDs (ordered binarydecision diagrams) and the interchange equivalence problem for 2-dags are inEP. We also show that for boolean negation the equivalence problem is inEP^{NP}, thus tightening the existing NP^{NP} upper bound. We show that FewP,bounded ambiguity polynomial time, is contained in EP, a result that is notknown to follow from the previous SPP upper bound. For the three problems andclasses just mentioned with regard to EP, no proof of membership/containment inUP is known, and for the problem just mentioned with regard to EP^{NP}, noproof of membership in UP^{NP} is known. Thus, EP is indeed a tool that givesevidence against NP-completeness in natural cases where UP cannot currently beapplied.'}
2.41e+03 pattern=  0 task=  0 (u=61522)  {'docs': u"Optimization of decision problems in stochastic environments is usuallyconcerned with maximizing the probability of achieving the goal and minimizingthe expected episode length. For interacting agents in time-criticalapplications, learning of the possibility of scheduling of subtasks (events) orthe full task is an additional relevant issue. Besides, there exist highlystochastic problems where the actual trajectories show great variety fromepisode to episode, but completing the task takes almost the same amount oftime. The identification of sub-problems of this nature may promote e.g.,planning, scheduling and segmenting Markov decision processes. In this work,formulae for the average duration as well as the standard deviation of theduration of events are derived. The emerging Bellman-type equation is a simpleextension of Sobel's work (1982). Methods of dynamic programming as well asmethods of reinforcement learning can be applied for our extension. Computerdemonstration on a toy problem serve to highlight the principle."}
2.41e+03 pattern=  0 task=  0 (u=61523)  {'docs': u'Flavor (Formal Language for Audio-Visual Object Representation) has beencreated as a language for describing coded multimedia bitstreams in a formalway so that the code for reading and writing bitstreams can be automaticallygenerated. It is an extension of C++ and Java, in which the typing systemincorporates bitstream representation semantics. This allows describing in asingle place both the in-memory representation of data as well as theirbitstream-level (compressed) representation. Flavor also comes with atranslator that automatically generates standard C++ or Java code from theFlavor source code so that direct access to compressed multimedia informationby application developers can be achieved with essentially zero programming.Flavor has gone through many enhancements and this paper fully describes thelatest version of the language and the translator. The software has been madeinto an open source project as of Version 4.1, and the latest downloadableFlavor package is available at http://flavor.sourceforge.net.'}
2.5e+03 pattern=  0 task=  0 (u=61785)  {'docs': u'Volatility, fitting with first order Landau expansion, stationarity, andcausality of the Taiwan stock market (TAIEX) are investigated based on dailyrecords. Instead of consensuses that consider stock market index change as arandom time series we propose the market change as a dual time series consistsof the index and the corresponding volume. Therefore, causalities between thesetwo time series are investigated.'}
2.67e+03 pattern=  0 task=  0 (u=62146)  {'docs': u"We study self-referential sentences of the type related to the Liar paradox.In particular, we consider the problem of assigning consistent fuzzy truthvalues to collections of self-referential sentences. We show that the problemcan be reduced to the solution of a system of nonlinear equations. Furthermore,we prove that, under mild conditions, such a system always has a solution (i.e.a consistent truth value assignment) and that, for a particular implementationof logical ``and'', ``or'' and ``negation'', the ``mid-point'' solution isalways consistent. Next we turn to computational issues and present severaltruth-value assignment algorithms; we argue that these algorithms can beunderstood as generalized sequential reasoning. In an Appendix we present alarge number of examples of self-referential collections (including the Liarand the Strengthened Liar), we formulate the corresponding truth valueequations and solve them analytically and/ or numerically."}
2.67e+03 pattern=  0 task=  0 (u=62147)  {'docs': u'The main goal of Fiddle, a distributed debugging engine, is to provide aflexible platform for developing debugging tools. Fiddle provides a layered setof interfaces with a minimal set of debugging functionalities, for theinspection and control of distributed and multi-threaded applications.  This paper illustrates how Fiddle is used to support integrated testing anddebugging. The approach described is based on a tool, called Deipa, thatinterprets sequences of commands read from an input file, generated by anindependent testing tool. Deipa acts as a Fiddle client, in order to enforcespecific execution paths in a distributed PVM program. Other Fiddle clients maybe used along with Deipa for the fine debugging at process level. Fiddle andDeipa functionalities and architectures are described, and a working exampleshows a step-by-step application of these tools.'}
2.23e+03 pattern=  0 task=  0 (u=62330)  {'docs': u'In this paper we present a simple source code configuration tool. ExLibrisoperates on libraries and can be used to extract from local libraries all coderelevant to a particular project. Our approach is not designed to addressproblems arising in code production lines, but rather, to support the needs ofindividual or small teams of researchers who wish to communicate their Prologprograms. In the process, we also wish to accommodate and encourage the writingof reusable code. Moreover, we support and propose ways of dealing with issuesarising in the development of code that can be run on a variety of like-mindedProlog systems. With consideration to these aims we have made the followingdecisions: (i) support file-based source development, (ii) require minimalprogram transformation, (iii) target simplicity of usage, and (iv) introduceminimum number of new primitives.'}
2.23e+03 pattern=  0 task=  0 (u=62360)  {'docs': u'We study an abstract optimization problem arising from biomolecular sequenceanalysis. For a sequence A of pairs (a_i,w_i) for i = 1,..,n and w_i>0, asegment A(i,j) is a consecutive subsequence of A starting with index i andending with index j. The width of A(i,j) is w(i,j) = sum_{i <= k <= j} w_k, andthe density is (sum_{i<= k <= j} a_k)/ w(i,j). The maximum-density segmentproblem takes A and two values L and U as input and asks for a segment of Awith the largest possible density among those of width at least L and at mostU. When U is unbounded, we provide a relatively simple, O(n)-time algorithm,improving upon the O(n \\log L)-time algorithm by Lin, Jiang and Chao. When bothL and U are specified, there are no previous nontrivial results. We solve theproblem in O(n) time if w_i=1 for all i, and more generally inO(n+n\\log(U-L+1)) time when w_i>=1 for all i.'}
3.28e+03 pattern=  0 task=  0 (u=62385)  {'docs': u'Semistructured databases require tailor-made concurrency control mechanismssince traditional solutions for the relational model have been shown to beinadequate. Such mechanisms need to take full advantage of the hierarchicalstructure of semistructured data, for instance allowing concurrent updates ofsubtrees of, or even individual elements in, XML documents. We present anapproach for concurrency control which is document-independent in the sensethat two schedules of semistructured transactions are considered equivalent ifthey are equivalent on all possible documents. We prove that it is decidable inpolynomial time whether two given schedules in this framework are equivalent.This also solves the view serializability for semistructured schedulespolynomially in the size of the schedule and exponentially in the number oftransactions.'}
2.58e+03 pattern=  0 task=  0 (u=62447)  {'docs': u'In this work we firstly review some results in Classical Information Theory.Next, we try to generalize these results by using the Tsallis entropy. Wepresent a preliminary result and discuss our aims in this field.'}
2.58e+03 pattern=  0 task=  0 (u=62448)  {'docs': u'Configuring consists in simulating the realization of a complex product froma catalog of component parts, using known relations between types, and pickingvalues for object attributes. This highly combinatorial problem in the field ofconstraint programming has been addressed with a variety of approaches sincethe foundation system R1(McDermott82). An inherent difficulty in solvingconfiguration problems is the existence of many isomorphisms amonginterpretations. We describe a formalism independent approach to improve thedetection of isomorphisms by configurators, which does not require to adapt theproblem model. To achieve this, we exploit the properties of a characteristicsubset of configuration problems, called the structural sub-problem, whichcanonical solutions can be produced or tested at a limited cost. In this paperwe present an algorithm for testing the canonicity of configurations, that canbe added as a symmetry breaking constraint to any configurator. The cost andefficiency of this canonicity test are given.'}
2.21e+03 pattern=  0 task=  0 (u=62532)  {'docs': u'Speech recognition has of late become a practical technology for real worldapplications. Aiming at speech-driven text retrieval, which facilitatesretrieving information with spoken queries, we propose a method to integratespeech recognition and retrieval methods. Since users speak contents related toa target collection, we adapt statistical language models used for speechrecognition based on the target collection, so as to improve both therecognition and retrieval accuracy. Experiments using existing test collectionscombined with dictated queries showed the effectiveness of our method.'}
2.21e+03 pattern=  0 task=  0 (u=62533)  {'docs': u'We report experimental results associated with speech-driven text retrieval,which facilitates retrieving information in multiple domains with spokenqueries. Since users speak contents related to a target collection, we producelanguage models used for speech recognition based on the target collection, soas to improve both the recognition and retrieval accuracy. Experiments usingexisting test collections combined with dictated queries showed theeffectiveness of our method.'}
2.32e+03 pattern=  0 task=  0 (u=62767)  {'docs': u'Quantum random walks on graphs have been shown to display many interestingproperties, including exponentially fast hitting times when compared with theirclassical counterparts. However, it is still unclear how to use these novelproperties to gain an algorithmic speed-up over classical algorithms. In thispaper, we present a quantum search algorithm based on the quantum random walkarchitecture that provides such a speed-up. It will be shown that thisalgorithm performs an oracle search on a database of $N$ items with$O(\\sqrt{N})$ calls to the oracle, yielding a speed-up similar to other quantumsearch algorithms. It appears that the quantum random walk formulation hasconsiderable flexibility, presenting interesting opportunities for developmentof other, possibly novel quantum algorithms.'}
3.13e+03 pattern=  0 task=  0 (u=62782)  {'docs': u"Subject of this paper is an implementation of a well-known Motzkin-Burgeralgorithm, which solves the problem of finding the full set of solutions of asystem of linear homogeneous inequalities. There exist a number ofimplementations of this algorithm, but there was no one in Maple, to the bestof the author's knowledge."}
2.95e+03 pattern=  0 task=  0 (u=62865)  {'docs': u'The semijoin algebra is the variant of the relational algebra obtained byreplacing the join operator by the semijoin operator. We discuss someinteresting connections between the semijoin algebra and the guarded fragmentof first-order logic. We also provide an Ehrenfeucht-Fraisse game,characterizing the discerning power of the semijoin algebra. This game gives amethod for showing that certain queries are not expressible in the semijoinalgebra.'}
1.65e+03 pattern=  0 task=  0 (u=62984)  {'docs': u"We have implemented Kima, an automated error correction system for concurrentlogic programs. Kima corrects near-misses such as wrong variable occurrences inthe absence of explicit declarations of program properties. Strongmoding/typing and constraint-based analysis are turning to play fundamentalroles in debugging concurrent logic programs as well as in establishing theconsistency of communication protocols and data types. Mode/type analysis ofModed Flat GHC is a constraint satisfaction problem with many simple mode/typeconstraints, and can be solved efficiently. We proposed a simple and efficienttechnique which, given a non-well-moded/typed program, diagnoses the``reasons'' of inconsistency by finding minimal inconsistent subsets ofmode/type constraints. Since each constraint keeps track of the symboloccurrence in the program, a minimal subset also tells possible sources ofprogram errors. Kima realizes automated correction by replacing symboloccurrences around the possible sources and recalculating modes and types ofthe rewritten programs systematically. As long as bugs are near-misses, Kimaproposes a rather small number of alternatives that include an intendedprogram."}
2.34e+03 pattern=  0 task=  0 (u=62992)  {'docs': u'Query equivalence is investigated for disjunctive aggregate queries withnegated subgoals, constants and comparisons. A full characterization ofequivalence is given for the aggregation functions count, max, sum, prod,toptwo and parity. A related problem is that of determining, for a givennatural number N, whether two given queries are equivalent over all databaseswith at most N constants. We call this problem bounded equivalence. A completecharacterization of decidability of bounded equivalence is given. Inparticular, it is shown that this problem is decidable for all the aboveaggregation functions as well as for count distinct and average. Forquasilinear queries (i.e., queries where predicates that occur positively arenot repeated) it is shown that equivalence can be decided in polynomial timefor the aggregation functions count, max, sum, parity, prod, toptwo andaverage. A similar result holds for count distinct provided that a fewadditional conditions hold. The results are couched in terms of abstractcharacteristics of aggregation functions, and new proof techniques are used.Finally, the results above also imply that equivalence, under bag-setsemantics, is decidable for non-aggregate queries with negation.'}
2.93e+03 pattern=  0 task=  0 (u=63043)  {'docs': u'We consider the problem of providing service guarantees in a high-speedpacket switch. As basic requirements, the switch should be scalable to highspeeds per port, a large number of ports and a large number of traffic flowswith independent guarantees. Existing scalable solutions are based on VirtualOutput Queuing, which is computationally complex when required to provideservice guarantees for a large number of flows.  We present a novel architecture for packet switching that provides supportfor such service guarantees. A cost-effective fabric with small externalspeedup is combined with a feedback mechanism that enables the fabric to bevirtually lossless, thus avoiding packet drops indiscriminate of flows. Throughanalysis and simulation, we show that this architecture provides accuratesupport for service guarantees, has low computational complexity and isscalable to very high port speeds.'}
2.82e+03 pattern=  0 task=  0 (u=63164)  {'docs': u'Self-similarity in the network traffic has been studied from several aspects:both at the user side and at the network side there are many sources of thelong range dependence. Recently some dynamical origins are also identified: theTCP adaptive congestion avoidance algorithm itself can produce chaotic and longrange dependent throughput behavior, if the loss rate is very high. In thispaper we show that there is a close connection between the static and dynamicorigins of self-similarity: parallel TCPs can generate the self-similaritythemselves, they can introduce heavily fluctuations into the background trafficand produce high effective loss rate causing a long range dependent TCP flow,however, the dropped packet ratio is low.'}
1.09e+03 pattern=  0 task=  0 (u=63399)  {'docs': u'Bestvina and Handel have found an effective algorithm that determines whethera given homeomorphism of an orientable, possibly punctured surface ispseudo-Anosov. We present a software package in Java that realizes thisalgorithm for surfaces with one puncture. Moreover, the package allows the userto define homeomorphisms in terms of Dehn twists, and in the pseudo-Anosov caseit generates images of train tracks in the sense of Bestvina-Handel.'}
1.7e+03 pattern=  0 task=  0 (u=63683)  {'docs': u'In logic programming, dynamic scheduling refers to a situation where theselection of the atom in each resolution (computation) step is determined atruntime, as opposed to a fixed selection rule such as the left-to-right one ofProlog. This has applications e.g. in parallel programming. A mechanism tocontrol dynamic scheduling is provided in existing languages in the form ofdelay declarations.  Input-consuming derivations were introduced to describe dynamic schedulingwhile abstracting from the technical details. In this paper, we first formalisethe relationship between delay declarations and input-consuming derivations,showing in many cases a one-to-one correspondence. Then, we define amodel-theoretic semantics for input-consuming derivations of simply-modedprograms. Finally, for this class of programs, we provide a necessary andsufficient criterion for termination.'}
1.7e+03 pattern=  0 task=  1 (u=63683)  {'docs': u'We study the properties of input-consuming derivations of moded logicprograms. Input-consuming derivations can be used to model the behavior oflogic programs using dynamic scheduling and employing constructs such as delaydeclarations.  We consider the class of nicely-moded programs and queries. We show that forthese programs a weak version of the well-known switching lemma holds also forinput-consuming derivations. Furthermore, we show that, under suitableconditions, there exists an algebraic characterization of termination ofinput-consuming derivations.'}
2.69e+03 pattern=  0 task=  0 (u=63904)  {'docs': u'A simple mathematical definition of the 4-port model for pure Prolog isgiven. The model combines the intuition of ports with a compact representationof execution state. Forward and backward derivation steps are possible. Themodel satisfies a modularity claim, making it suitable for formal reasoning.'}
2.69e+03 pattern=  0 task=  0 (u=63907)  {'docs': u'Let $\\orig{A}$ be any matrix and let $A$ be a slight random perturbation of$\\orig{A}$. We prove that it is unlikely that $A$ has large condition number.Using this result, we prove it is unlikely that $A$ has large growth factorunder Gaussian elimination without pivoting. By combining these results, webound the smoothed precision needed by Gaussian elimination without pivoting.Our results improve the average-case analysis of Gaussian elimination withoutpivoting performed by Yeung and Chan (SIAM J. Matrix Anal. Appl., 1997).'}
2.73e+03 pattern=  0 task=  0 (u=63966)  {'docs': u'The increase in the amount of data on the Internet has led to the developmentof a new generation of applications based on selective informationdissemination where, data is distributed only to interested clients. Suchapplications require a new middleware architecture that can efficiently matchuser interests with available information. Middleware that can satisfy thisrequirement include event-based architectures such as publish-subscribesystems. In this demonstration paper we address the problem of semanticmatching. We investigate how current publish/subscribe systems can be extendedwith semantic capabilities. Our main contribution is the development andvalidation (through demonstration) of a semantic pub/sub system prototypeS-ToPSS (Semantic Toronto Publish/Subscribe System).'}
2.73e+03 pattern=  0 task=  1 (u=63966)  {'docs': u'TCP is the De facto standard for connection oriented transport layerprotocol, while UDP is the De facto standard for transport layer protocol,which is used with real time traffic for audio and video. Although there havebeen many attempts to measure and analyze the performance of the TCP protocolin wireless networks, very few research was done on the UDP or the interactionbetween TCP and UDP traffic over the wireless link. In this paper, we tudy theperformance of TCP and UDP over IEEE802.11 ad hoc network. We used twotopologies, a string and a mesh topology. Our work indicates that IEEE802.11 asa ad-hoc network is not very suitable for bulk transfer using TCP. It alsoindicates that it is much better for real-time audio. Although one has to becareful here since real-time audio does require much less bandwidth than thewireless link bandwidth. Careful and detailed studies are needed to furtherclarify that issue.'}
2.73e+03 pattern=  0 task=  0 (u=63968)  {'docs': u'In recent years, the amount of information on the Internet has increasedexponentially developing great interest in selective information disseminationsystems. The publish/subscribe paradigm is particularly suited for designingsystems for routing information and requests according to their contentthroughout wide-area network of brokers. Current publish/subscribe systems uselimited syntax-based content routing but since publishers and subscribers areanonymous and decoupled in time, space and location, often over wide-areanetwork boundary, they do not necessarily speak the same language.Consequently, adding semantics to current publish/subscribe systems isimportant. In this paper we identify and examine the issues in developingsemantic-based content routing for publish/subscribe broker networks.'}
3.21e+03 pattern=  0 task=  0 (u=64149)  {'docs': u'An information agent is viewed as a deductive database consisting of 3 parts:an observation database containing the facts the agent has observed or sensedfrom its surrounding environment, an input database containing the informationthe agent has obtained from other agents, and an intensional database which isa set of rules for computing derived information from the information stored inthe observation and input databases. Stabilization of a system of informationagents represents a capability of the agents to eventually get correctinformation about their surrounding despite unpredictable environment changesand the incapability of many agents to sense such changes causing them to havetemporary incorrect information. We argue that the stabilization of a system ofcooperative information agents could be understood as the convergence of thebehavior of the whole system toward the behavior of a "superagent", who has thesensing and computing capabilities of all agents combined. We show thatunfortunately, stabilization is not guaranteed in general, even if the agentsare fully cooperative and do not hide any information from each other. We givesufficient conditions for stabilization and discuss the consequences of ourresults.'}
3.19e+03 pattern=  0 task=  0 (u=64216)  {'docs': u"We investigate the usage of rule dependency graphs and their colorings forcharacterizing and computing answer sets of logic programs. This approachprovides us with insights into the interplay between rules when inducing answersets. We start with different characterizations of answer sets in terms oftotally colored dependency graphs that differ in graph-theoretical aspects. Wethen develop a series of operational characterizations of answer sets in termsof operators on partial colorings. In analogy to the notion of a derivation inproof theory, our operational characterizations are expressed as(non-deterministically formed) sequences of colorings, turning an uncoloredgraph into a totally colored one. In this way, we obtain an operationalframework in which different combinations of operators result in differentformal properties. Among others, we identify the basic strategy employed by thenoMoRe system and justify its algorithmic approach. Furthermore, we distinguishoperations corresponding to Fitting's operator as well as to well-foundedsemantics. (To appear in Theory and Practice of Logic Programming (TPLP))"}
3.09e+03 pattern=  0 task=  0 (u=64401)  {'docs': u"We present a novel framework, called balanced overlay networks (BON), thatprovides scalable, decentralized load balancing for distributed computing usinglarge-scale pools of heterogeneous computers. Fundamentally, BON encodes theinformation about each node's available computational resources in thestructure of the links connecting the nodes in the network. This distributedencoding is self-organized, with each node managing its in-degree and localconnectivity via random-walk sampling. Assignment of incoming jobs to nodeswith the most free resources is also accomplished by sampling the nodes viashort random walks. Extensive simulations show that the resulting highlydynamic and self-organized graph structure can efficiently balancecomputational load throughout large-scale networks. These simulations cover awide spectrum of cases, including significant heterogeneity in availablecomputing resources and high burstiness in incoming load. We provide analyticalresults that prove BON's scalability for truly large-scale networks: inparticular we show that under certain ideal conditions, the network structureconverges to Erdos-Renyi (ER) random graphs; our simulation results, however,show that the algorithm does much better, and the structures seem to approachthe ideal case of d-regular random graphs. We also make a connection betweenhighly-loaded BONs and the well-known ball-bin randomized load balancingframework."}